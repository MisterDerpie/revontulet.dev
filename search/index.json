[{"content":"This post has been lying around as an idea for a couple of months now. A list of bullet points on my computer is from somewhere mid-June, and I originally intended to title it \u0026ldquo;The Underappreciation of Fast Feedback Loops\u0026rdquo;. However, calling out a cost sounds catchier and is more accurate, because we do not only underappreciate them, but they cost us. First and foremost, nerves (because who likes to wait for results?), but moreover do slow feedback loops actively reduce delivery speed of teams.\nIn Software Engineering, we easily set targets around metrics and processes which have a constant behavior, regardless of whether we have 50, 300 or 50,000 lines of code. At the same time, we do not pay close attention to those metrics that change as complexity grows, and only realize their impact when the pain is obvious.\nWhy do we accelerate cars to high speed before we turn on the engine? We don\u0026rsquo;t do this. Reading this headline, you might be wondering \u0026ldquo;What on earth are you talking about?\u0026rdquo; Worry not, the post is still about software, we\u0026rsquo;re getting there.\nA time consuming design It\u0026rsquo;s a sunny day in a utopian dystopian world, and you bring your car in for inspection. At the end of the inspection, you see it being towed out of the workshop, straight onto the highway. The towing car accelerates to a speed of 100 km/h (62 mp/h), and then the key in your car is turned and the ignition kicks in. From that point on, the engine is running. Great, they\u0026rsquo;ve verified that the engine works!\nWhy would you do this?! Coming from our real world, you naturally ask \u0026ldquo;Why do you need to accelerate to a high speed, if you want to know whether the engine turns on?\u0026rdquo; Fair question. \u0026ldquo;Can\u0026rsquo;t you just turn the key with the car standing still?\u0026rdquo; Absolutely. \u0026ldquo;Why does it need to be on the highway and at a high speed to verify that the engine starts?\u0026rdquo; Right, why?\nWe keep building our own limitations You might get an idea now where I am heading to. What if, in this fictional world, the car was built in such a way that it must accelerate to a high speed after an inspection, before the engine can be turned on. It is very obvious that this is a an additional cost and a waste of time, as inspections take much longer.\nLet me ask you this: How often have you heard the sentence \u0026ldquo;We need to deploy this to {somewhere} to verify whether it works\u0026rdquo;? Do you think that deploying your application somewhere is really necessary, to see if your change behaves as expected? Now is the time to ask yourself: Why don\u0026rsquo;t we test this locally? And I am quite sure, you have heard all kinds of excuses around complexity and dependencies. You won\u0026rsquo;t need a highway and a car being towed to high speed to turn on the engine, and your application should not need a fully-fledged environment to verify its behavior.\nThe Development Feedback Loop During the development of a feature, a simplified feedback loop until done looks as follows:\nChange Something Verify - If it does not yet behave as expected, back to 1 Done. Once the Verify step actually yields your expected results, be that automated or manual, you move on and consider it done.\nTheoretical Ideal It is important to understand the implication of where the feedback can be gathered, and how much time each approach takes. In an ideal world, it should look like above.\nNormally, being able to gather feedback locally is the fastest. An engineer only needs to rebuild their feature, maybe have a set of dependencies running on the side, and thus is entirely independent on other systems to verify correctness. If the feedback turns out to not be what they expect, a rebuild happens seemingly fast.\nSecond in line is a staging (also testing) environment. Usually, within this environment all dependencies are present. However, it is not that simple as just recompiling the feature and testing it, given this is not on our machine anymore. To test changes in the staging environment, that change must somehow get there. The actual steps involved depend on the environment, yet due to its nature of needing more steps, the time until feedback can be collected increases.\nLast but not least, what if it can only be verified in the actual production environment that hits real traffic? Regardless of whether it\u0026rsquo;s dangerous to test there or not, it tends to take the most time to get the change out there. Often, one also has to be precautious and apply additional measures, for example the use Feature Flags, to make sure not to impair existing flows.\nIf you\u0026rsquo;re manufacturing an engine, after its assembly, you do not want (nor need) to build a car around it to verify that the engine works.\nFeedback Loops in Practice From working in several teams across various companies, and talking with peers in the field, the reality actually more often than not looks like this.\nIn the first graph, the time to gather feedback on the local machine is infinite. This should depict the absence of a local environment, so there is no way to verify changes locally. The second graph indicates that gathering feedback locally is more time consuming than gathering it in staging.\nTeams tend to either not have a setup to have a (reliable) local feedback loop, or if they do, it is slower (maybe more fragile) than deploying their application to a different environment. This is often excused with having too many dependencies to do this. Whilst there are certainly dependencies that we may not easily replicate locally, we need to keep in mind that at the beginning of every software project, the amount of dependencies is zero. Our projects do not get in a hard-to-locally-verify state out of nowhere, and the fact of \u0026ldquo;too many dependencies to resolve\u0026rdquo; comes from not having resolved one-by-one at the time of their introduction.\nA Self-Inflicted Problem One may wonder now, if testing locally involves the least hoops to jump, how do projects end up in a state where testing in a different environment ends up being faster? This is the software equivalent of the Boiling Frog.\nThe boiling frog is an apologue describing a frog being slowly boiled alive. The premise is that if a frog is put suddenly into boiling water, it will jump out, but if the frog is put in tepid water which is then brought to a boil slowly, it will not perceive the danger and will be cooked to death.\nâ€• Wikipedia When we join a new team, we immediately notice the things that do not work very well. Especially if we come from teams where we took those things for granted. Right here, that is the degradation of the feedback loop. With every dependency pulled in, and every feature that needs to get shipped faster, it is very likely that the feedback time deteriorates a bit. Since we are part of the process, we do not notice it. Much like your relatives told you when growing up \u0026ldquo;You have grown quite a lot since last time we saw you!\u0026rdquo;, although you don\u0026rsquo;t actively notice any difference day-to-day. Most often, teams do not put the effort in keeping local verification easy enough when they add new features. This debt accumulates, and will be paid at some point, but only noticed once it becomes too painful and too much of a drag to keep shipping fast.\nWhere are the targets? If you have seen \u0026ldquo;The Office\u0026rdquo;, Michael Scott screaming \u0026ldquo;Where are the turtles?!\u0026rdquo; might be a scene stuck to your head. Given the boiling frog situation of the feedback loop, I come to wonder: Where are the targets?\nWe tend to track many things in software projects, probably a metric all of us are familiar with is Code Coverage. That is, how many statements of your code are executed during an automated test run. On GitHub, you most likely have encountered at least one repository throughout your life that used CodeCov. For code coverage, we don\u0026rsquo;t lightheartedly accept changes that will lead to a decrease below a certain threshold.\nWhen it comes to delivery speed, Velocity (Wikipedia) and Lead Time (Wikipedia) are terms that pop up. Velocity describes how much time tasks of similar complexity take, whilst lead time denotes the total time taken from requesting a feature to its eventual delivery. At some point, when the feedback loop is so unbearably slow, the team\u0026rsquo;s velocity decreases. And given that the lead time is directly depending on velocity, so will the team end up consuming more time on delivering results.\nWhilst it is accepted for teams to have a target on Code Coverage, and not accept changes that breach a certain threshold, we do not really set up a target of \u0026ldquo;How much time something should take to verify\u0026rdquo;. If we had that target, it would be expected that engineers spend time during their feature development on keeping the feedback loop short and healthy. Sure, this would have a constant impact on the team\u0026rsquo;s velocity for any task, however, it avoids teams ever getting into a situation that is so bad that it is hard to recover. Think about a code base with 100,000 LoC, without any tests. It is unlikely that anyone will ever sufficiently retrofit any sort of automated testing into that code base, without bringing the team\u0026rsquo;s feature delivery to a grinding halt \u0026hellip; for months to come. Strangely enough, it is widely accepted across the industry to take slightly more time by adding a set of automated tests for each task.\nThe Hidden Cost of Slow Feedback Loops Delivering with n-1 Engineers To support my point and underline the headline, let\u0026rsquo;s look at numbers. Whilst I have claimed that slower feedback loops are counterproductive, it still remains to show what the hidden cost is. Note that this is a constructed scenario, and on a one-team level this may not reflect reality well enough. However, taken a large enough organization, the results converge towards the outlined example.\nAssuming we are having a team of 8 engineers and a feedback loop where locally testing is the fastest, that is, the ideal situation. Over the course of the year, the team adds new features but neglects to keep local verifiability fast. Therfore, they start deploying their changes to a staging environment for any verification. Compared to a year ago, an engineer now needs to spend 10 extra minutes per verification attempt. Say each engineer needs to do that 6 times per day. Let\u0026rsquo;s do the math:\n$$ \\begin{aligned} \u0026= 8 \\,\\text{Engineers} \\times 10 \\,\\frac{\\text{Minutes}}{\\text{Run}} \\times 6 \\,\\frac{\\text{Runs}}{\\text{Engineer/Day}} \\\\ \u0026= 480 \\,\\frac{\\text{Minutes}}{\\text{Day}} \\\\ \u0026= 8 \\,\\frac{\\text{Hours}}{\\text{Day}} \\\\ \u0026= 5 \\,\\frac{\\text{Days}}{\\text{Week}} \\end{aligned} $$The absence of a fast feedback loop results in the team losing the capacity of a full time engineer. That team manages to get the work of 7 done with the capacity of 8. Usually not the \u0026ldquo;Buy 2 get 3\u0026rdquo; kind of deal you would like to have.\nCumulative Debt One may make an argument against that, that if we were to constantly add time for keeping feedback loops fast, we end up with the same velocity. On the surface, that looks to be a zero-sum game.\nIn practice, there is a turning point in which the constant effort put into keeping them fast will be overtaken by the slowdown of never addressing them in the first place. With new projects, this trend takes a long time before it sets in. However, once slow enough, it overtakes the linear growth, and from there on will be net-negative until resolved. Given time usually being accounted for feature delivery already, the time where this gets addressed might be far in the future.\nBottom Line To finish this article, I\u0026rsquo;d like to suggest teams to invest more time into fast feedback loops. As an anecdote, I once joined a product where colleagues needed to deploy software to a staging environment, just to verify that their new GET endpoint, reading a record from a database and returning it, works as expected. After questioning this and being told \u0026ldquo;But it\u0026rsquo;s too hard, there are too many dependencies to do this locally\u0026rdquo;, I spent some time to build the local feedback loop, which was eventually adopted by the whole team. For the sake of numbers, the actual time it took to verify was ~20-30 minutes per attempt before the change made, whilst the local setup took precisely the compile time and issuing a request, so less than a minute.\nIf you find yourself in the situation where seemingly simple tasks like this vary a deployment, stop adding yet another feature and address local verifiability. It\u0026rsquo;s worth it.\n","date":"2025-08-17T10:00:00Z","permalink":"https://revontulet.dev/p/2025-hidden-cost-slow-feedback-loops/","title":"The Hidden Cost of Slow Feedback Loops"},{"content":" The Matrix AI is everywhere. It is all around us. Even now, in this very room. You can see it when you look out your window something up on the internet or when you turn on your television washing machine. You can feel it when you go to work do anything nowadays\u0026hellip; when you go to church a search engine\u0026hellip; when you pay fill out your taxes. It is the world that has been pulled over your eyes to blind you from the truth meaningful content.\nâ€• The Matrix, Almost The Matrix Today\u0026rsquo;s AI world reminds me a lot of The Matrix. For about two years, we have been living in a world where AI has become the new Nocode, the new Bitcoin, the new Web3, the new Metaverse, basically everything. It is hardly possible to not encounter AI on, in, over, under, everywhere around us.\nIf I would like to, I could connect my washing machine to the internet. There is even an AI stamp! Whyever that needs AI, my laundry was clean 5 years ago, and my washing machine is still offline, and shall always be.\nWhy this post? Whilst being on a bus ride across Germany, I was thinking about the latest news in tech, my work, and to no one\u0026rsquo;s surprise: AI. Over the course of the past months, we all have been reading more (or less) on AI. Sales pitches have been made that it is the past, the present, the future, drawing a resemblance to the Matrix: \u0026ldquo;It is all around us.\u0026rdquo; Predicting the future is impossible (even with AI, sorry to break it like this to you), but if you just predict every possible outcome, by definition one scenario turns out to be true. We see wild claims and heavy doubts, and have the full spectrum of predictions.\n\u0026ldquo;AI-first\u0026rdquo; movements are popping up everywhere, but let\u0026rsquo;s just have a look on where we are some of it is, and where some of us are now.\nSuccess Stories? Whilst nobody can tell what AI-first exactly is, it seems to be AI doing work that humans will not be needed for anymore. Sometimes for better, sometimes for worse. Two examples where the \u0026ldquo;all-in\u0026rdquo; resulted in at least \u0026ldquo;some-out\u0026rdquo;\nKlarna:\n2024 - Klarna\u0026rsquo;s AI assistant is doing the job of 700 workers (archive.is) 2025 - Feb - 2/3 of customer service is AI assistants at Klarna (archive.is) 2025 - May - Klarna rehires humans to ensure customers can always talk to humans (archive.is) Duolingo:\n2025 - April - Duolingo will replace contract workers with AI (archive.is) 2025 - May - Duolingo Faces Backlash Over AI Strategy, Pivots to Retract Its Statement (archive.is) At the same time, we hear great success stories from the really big tech companies.\n2024 - Amazon - Amazon CEO Andy Jassy Says Company\u0026rsquo;s AI Assistant Has Saved $260M And 4.5K Developer-Years Of Work (archive.is) 2025 - Google - AI Powers 25% of Googleâ€™s Code (archive.is) Amazon and Google are boasting about their success stories, without providing much data. But luckily, we can find that data ourselves, for instance, on Open Source projects\u0026hellip; or can we? The blog \u0026ldquo;Pivot to AI\u0026rdquo; published the article If AI is so good at coding â€¦ where are the open source contributions?, and the result should make all of us wonder\u0026hellip;\nOftentimes, companies get away making these bold claims without ever proving them. We don\u0026rsquo;t see the methodology Amazon used to come up with 4,500 developer years. Was that even work that was necessary in the first place? Say you have a staple of dirty dishes and a trashbin having only an empty milk carton. You could empty the bin, but that\u0026rsquo;s clearly not needed and the dishes won\u0026rsquo;t be done either. What kind of Code is Google referring to, and how much longer would it take a human to produce it? It should be easy to reproduce their results on FOSS.\nAs with any new technology, it is just sufficient to make a bold claim. Those asking about evidence are met with \u0026ldquo;It\u0026rsquo;s true, but I will not make the effort to prove it to you because you won\u0026rsquo;t believe me anyway.\u0026rdquo; Consequently, no need to give proof, right?\nSide-Note: I am aware that there are LLM benchmarks. Coming from \u0026ldquo;It solved a well-defined undergrad test\u0026rdquo; to AGI or above claims is imho too big of a gap, that has yet to be filled. An article about one of GitHub\u0026rsquo;s actual benchmarks is Does GitHub Copilot Improve Code Quality? Here\u0026rsquo;s How We Lie With Statistics (archive.is) from Jadarma\u0026rsquo;s blog\u0026hellip; which draws a grim picture on the methodology.\nTo our luck, there\u0026rsquo;s been a public experiment. Recently (May 2025), .NET Developers tried out Copilot on their GitHub repository, see PR github/dotnet/runtime#115762. Whilst there are a lot of snarky comments, I think we should respect the attempt. It is important to see where we stand, and see where we can improve, or whether we aren\u0026rsquo;t there (yet?). Have a look at it yourself, it\u0026rsquo;s worth it. In this experiment, AI could unfortunately not live up to the standard of what CEOs are telling shareholders. Instead, the closest resemblance would be a Fiebertraum (German for feverish dreams, basically a nightmare).\nWe are only six months away from AGI! This statement can probably not be attributed to just one single person, as it is mostly used in a mocking way. Ever since ChatGPT, we heard how all kinds of jobs are going to be cut, engineers are put out of work, and prompt-engineering is the future.\nWithout doubt, AI has made it into our toolbelt. Similar to other tools engineers use. For example, opening a documentation to see how to use a technology. Reading a book for a deep dive into inner workings. Optimizing memory layout for making use of Cache Lanes. Opening Google Search to crawl Stackoverflow.\nThe list of tools, technologies and techniques engineers can use are near endless. And AI is yet another one that helps us.\nHowever, looking back at history\nCompilers didn\u0026rsquo;t replace the need for engineers Google didn\u0026rsquo;t make books obsolete No-Code anyone? \u0026hellip; The questionable speedup of AI(-first) Talking with peers in my field, which is distributed systems and backend engineering, we see our CEOs and CTOs moving to \u0026ldquo;AI-first\u0026rdquo; codebases. With AI code editors like Cursor and Windsurf (the latter soon to be Open AI (archive.is)), engineers are able to not just prompt for snippets, but use their whole codebase as the underlying dataset.\nWhen you try Cursor, ChatGPT,\u0026hellip; for the first time, it is very impressive how quickly it achieves a prototype of what you imagine. If you ever attempted to build a GUI by just drawing a rectangle and reacting to the mouse events, extracting coordinates etc., you probably know how much code that needs. Compare that to e.g. HTML, where creating a button in a few lines of code is the way, and executing an action on mouse click is as easy passing a function to a method named like the action itself. Coming from the lower level of things, this will speed up your GUI building 10x, without a doubt. You are now able to achieve even more in a shorter time - but what kind of work? This brings me to the Pareto Principle, which most of us should be familiar with.\nThe Pareto principle (also known as the 80/20 rule, the law of the vital few and the principle of factor sparsity) states that for many outcomes, roughly 80% of consequences come from 20% of causes (the \u0026ldquo;vital few\u0026rdquo;).\nâ€• Joseph M. Juran, Wikipedia Simply put, you spend 20% of your time to achieve 80% of your goals, but the remaining 20% is where you actually spend the vast majority. What has always been hard were the last 20%. To actually bring it over the finish line requires a lot of effort after the \u0026ldquo;quick and dirty\u0026rdquo; part is done - again, this is where we spend most of our time. Frameworks, higher level languages, compilers\u0026hellip; They all have in common to reduce the first 80% of our work, but do not eliminate 80% of our time spent. With LLMs, we seem to witness the same speedup on that relative negligible 20%.\n\u0026ldquo;I am just one prompt away from getting the right results!\u0026rdquo; About two weeks ago, I tried out Cursor myself on a Golang Template file. Said file is already hard for us to maintain and comprehend, so it seemed a fair baseline. Whilst the first suggestion worked (it took Cursor about 30 minutes to think), one small requirement change made it impossible for Cursor to produce a correct result. For a good 12 hours, over the course of 1 1/2 days, I tried to prompt it such that it yields what we needed. Eventually, I noticed that my prompts converged more and more to be almost the code I wanted. After still not getting a working result, I ended up implementing it myself in less than 30 minutes.\nThe more niche a topic is, the worse seem to be the LLM\u0026rsquo;s results. For about 3 months now I am a Neovim user, before I never used neither Vim nor Nvim (adjusting git config to use nano was one of my first steps on a new OS). As other Nvim users know, you end up configuring a lot. Some of these configurations come from plugins, others are native to Vim. Overall, I made the great experience to get myself into \u0026ldquo;I am just one prompt away from getting the right configuration\u0026rdquo;\u0026hellip; eventually, I look at the clock, see that I spent 30 minutes on what seems to be a 2 line configuration, and end up using Google, GitHub and Reddit to find the right answer within seconds. This experience is shared among peers, where AI traps you into the thinking \u0026ldquo;I am only one prompt away\u0026rdquo;, whilst cleary it just does not know the answer. You can try it yourself the next time you are trapped. Prompt the solution back to your favorite coding tool and ask it if this works, or why the solution does/not work. Prepare yourself a tea beforehand, to enjoy the storytelling.\nAnother example stems from Go. Since January I am using Golang, and it seems that Golang adds features that are adopted almost as soon as they are released. One example that found adoption near instant are range loops with integers (gobyexample). We\u0026rsquo;re 9 months past Golang\u0026rsquo;s 1.23 Release, whilst ChatGPT still seem to be unaware of it. Whenever its creating a for loop, you will see the old for i := 0; i \u0026lt; n; i++ syntax. In a PR last week, an AI review bot told me that Go version 1.24 does not exist, although it\u0026rsquo;s been released since February 2025.\nAt work I got to witness a conversation less than two weeks ago, where people were discussing that Cursor on a large-ish codebase does not work well with their rules. It is natural that the bigger the code base the more rules you want to have, to avoid just one more edge-case and live hands-off. Think of it like your code, the more cases you support, the more \u0026ldquo;if\u0026quot;s you end up with, eventually. An engineer pointed out that some rules seem to be ignored as of recent. To their conclusion, the prompt in Cursor is appended to the rules, i.e. the full content of the rules file + the prompt are fed into Cursor when making a request. Every. single. request. Thus, a too large ruleset won\u0026rsquo;t fit into the context window, and your code ends up not looking as expected, given the large amount of rules.\nNo forward-looking statements A forward-looking statement predicts, projects, or uses future events as expectations or possibilities\nâ€• Wikipedia If I recall correctly, the first time I encountered that statement was in the context of shareholders, acquisitions and such. This might be the most accurate statement for the future of A(G)I for coding. It\u0026rsquo;s just perfectly aligned with \u0026ldquo;We can\u0026rsquo;t promise anything\u0026rdquo;, and one should think people won\u0026rsquo;t then, but at the same time any possible kind of claim has been made - excluding those that LLMs make themselves.\nWhilst LLMs and AI these days seem to be truly helpful tools, we should come back to a more nuanced reality. Companies have yet to prove that AI as a mostly autonomous assistant can live up to its promises, but all we see is that it depends. Right now, it depends a lot more towards the side of overpromises, rather than meeting the big expectations.\nMany engineers, myself included, are probably using AI tools regularly in their day-to-day life. Much so like using Google Search, opening a book, reading a documentation or eating breakfast.\nBottom line: We\u0026rsquo;re having quite exciting times ahead of us. It would be great if AI could prove what is proclaimed these days - But I have my doubts. I wonder if we\u0026rsquo;re going to see job roles with descriptions of \u0026ldquo;We need engineers to help us untangle the AI mess we created\u0026rdquo;. If that happens to be the case, I hope that Google Search will be back from the dead.\nWe\u0026rsquo;re just 6 months away from AGI.\nâ€• CEOs these days, whatever date you\u0026#39;re reading this ","date":"2025-05-31T10:00:00Z","permalink":"https://revontulet.dev/p/2025-ai-first/","title":"AI-first - We're just 6 months away from AGI ;-)"},{"content":"This post covers how to tether your iPhone via USB to your RaspberryPi, and share the iPhone\u0026rsquo;s network on eth0. Eventually, the RaspberryPi will be connected to a router via ethernet, making the router use the iPhone\u0026rsquo;s internet connection. A RaspberryPi 4B is used, but this guide should work on other models too.\nOne may wonder why this setup is needed. Routing at least DNS traffic through the Pi with the Pi-hole is a common use-case, but all traffic less so.\nMy connection to the world is through the iPhone\u0026rsquo;s hotspot. Said hotspot turns invisible after a period of time. It is not the classic hidden SSID invisible, but connecting to it once invisible is impossible, only connected devices stay.\nStill, why use a router, when the RaspberryPi comes with a WLAN chip? About a month ago, I attempted a similar setup and had the Pi directly as a WiFi hotspot. Unfortuantely, the throughput was only about 40 Mbit, defeating the purpose of the iPhone\u0026rsquo;s 300 Mbit 5G connection.\nGiven the fact that my RaspberyPi was collecting dust, it was perfectly suited. Moreover does my router at hand not support a USB connection, hence the need.\nInstalling Ubuntu Server To install Ubuntu Server, you may use the RaspberryPi Imager. It can be downloaded from raspberrypi.com/software.\nDuring the installation, it will ask you to provide a custom configuration. Choose yes and provide a username and a password to be able to SSH into it during the next steps. Then install it.\nOnce done, start your RaspberryPi and connect it via ethernet to your router.\nRaspberryPi Internet Setup Find the RaspberryPi\u0026rsquo;s IP Address from your router. Usually, it is listed under connected devices. Then ssh into it, e.g.\n1 ssh name-you-chose@192.168.X.Y SSH probably asks you to trust the fingerprint, and you may select yes. Note: In case you need to factory reset your Pi, and it gets assigned the same IP, you will see an error from ssh. That is due to not matching fingerprints. Remove the entries of that IP from ~/.ssh/known_hosts.\nConnect to the iPhone Install the packages\nusbmuxd libimobiledevice-utils ipeth-utils (this may be optional, I didn\u0026rsquo;t try it without) 1 2 3 4 sudo apt install -y \\ usbmuxd \\ ipheth-utils \\ libimobiledevice-utils Now it is time to connect our iPhone. With Mobile Hotspot enabled, it will show a dialog to Trust/Not trust the RaspberryPi. Trust it, and you may be required to enter your password.\nWe can see that it\u0026rsquo;s connected on the RaspberryPi running\n1 networkctl 1 2 3 4 5 IDX LINK TYPE OPERATIONAL SETUP 1 lo loopback carrier unmanaged 2 eth0 ether routable configured 3 wlan0 wlan off unmanaged 4 enxfae5ce082148 ether off unmanaged Important: Your iPhone will have a different ID, and in the steps from here on you need to replace it in the provided snippets.\nThe iPhone is enx..., and the next step is to have it managed, so it gets an IP assigned and we can route traffic through that interface. At this point, the guide deviates slightly from Arch Wiki\u0026rsquo;s setup using systemd-networkd. Although one may set it up that way, Ubuntu Server ships with netplan, which we take advantage of. That way, we only need to configure netplan.\nThere should already be an entry for eth0, so you may only add the part for the iPhone.\n1 sudo vim /etc/netplan/50-cloud-init.yaml 1 2 3 4 5 6 7 network: version: 2 renderer: networkd ethernets: enxfae5ce082148: // Add this dhcp4: true // Add this // ... keep eth0 unchanged for now Apply the new network config.\n1 netplan apply We can confirm it using the ip command.\n1 ip a 1 2 3 4 5 6 7 8 4: enxfae5ce082148: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether aa:bb:cc:dd:ff:gg brd ff:ff:ff:ff:ff:ff inet 123.12.10.4/28 metric 100 brd 123.12.10.15 scope global dynamic enxfae5ce082148 valid_lft 86357sec preferred_lft 86357sec inet6 1111:1222:333:4444:5555:6666:7777:8888/64 scope global mngtmpaddr noprefixroute valid_lft forever preferred_lft forever inet6 1111::2222:3333:4444:2148/64 scope link proto kernel_ll valid_lft forever preferred_lft forever To test the connection, curl provides the option to set an interface through which to connect.\n1 2 3 curl --interface enxfae5ce082148 \\ https://example.com \\ -w \u0026#39;%{speed_download}\u0026#39; -o /dev/null Great, now you could use the iPhone\u0026rsquo;s USB tethered network to connect to the internet.\nRoute traffic through the iPhone The second part is to tell iptables to route traffic through our iPhone\u0026rsquo;s interface. It is done with adding this entry using the iptables command.\n1 iptables -t nat -A POSTROUTING -o enxfae5ce082148 -j MASQUERADE Note that the newly created entry is not persisted on reboot. To persist it, one solution is to use iptables-persistent. Whilst installing, it asks you to back up existing configurations. Answer \u0026ldquo;Yes\u0026rdquo;.\n1 sudo apt install -y iptables-persistent You will be able to confirm a MASQUERADE entry running\n1 cat /etc/iptables/rules.v4 In case the entry is not present, rerun the steps and the persistence\n1 sudo iptables-save \u0026gt; /etc/iptables/rules.v4 We still need to enable IP forwarding between interfaces. This is achieved adding the following configuration.\n1 sudo vim /etc/sysctl.d/30-ipforward.conf 1 net.ipv4.ip_forward = 1 Last but not least, we must be able to connect on eth0. I did not get into the steps of setting up DHCP on it, so you may do that yourself. After this step, it will be assigned 192.168.42.1.\n1 sudo vim /etc/netplan/50-cloud-init.yaml 1 2 3 4 5 6 7 8 9 10 network: version: 2 renderer: networkd ethernets: enxfae5ce082148: dhcp4: true eth0: dhcp4: false addresses: - 192.168.42.1/24 From here on, for connecting to your RaspberryPi on eth0, you will need to assign the external device (e.g. a Laptop) a correct IP address, the right net mask and the right gateway. Note that the IP address suffix must be a value between 2 and 254.\nGiven our setup, the configuration is\nType Value IP Address 192.168.42.X Gateway 192.168.42.1 Netmask 24 or 255.255.255.0 Reboot the RaspberryPi, and the changes take effect. Keep in mind that you must assign the IP on your external device like above table, otherwise you will not see the RPi.\nThe Pi can be accessed at ssh your-name@192.168.42.1 thereafter.\nUsing the RaspberryPi with a Router The final step of this setup is to use the RaspberryPi as the internet provider for a router. Across most routers, these steps are almost identical, and you may consult your router\u0026rsquo;s documentation on where to find the right setuppage.\nFirst, you need to connect your RaspberryPi to the port of the router that says \u0026ldquo;WAN\u0026rdquo;.\nThen you need to find out where you can set the Conenction Mode/Internet Connection type of your router. Provide the values as below, and your router should be able to connect to the RaspberryPi, and use its network.\nType Value Connection Mode Static IP IP address 192.168.42.77 Subnet Mask 255.255.255.0 Default Gateway 192.168.42.1 Primary DNS server 9.9.9.9 Secondary DNS server 8.8.8.8 The resulting speed with this setup is ~190 Mbit downloading, and 22 Mbit uploading. With a different USB cable, I only achieved 120 Mbit, so the remaining 110 Mbit may be due to the USB cable.\nConnecting to the iPhone directly gives me almost 300 Mbit, so there is definitely a loss of performance somewhere.\n","date":"2025-05-11T08:00:00Z","permalink":"https://revontulet.dev/p/2025-iphone-ethernet-bridge-rpi-4-ubuntu/","title":"iPhone Ethernet bridge on RaspberryPi with Ubuntu Server"},{"content":"Welcome to part one of the three-part series \u0026ldquo;Hello, Sway!\u0026rdquo; The series is split into\nArch \u0026amp; Sway setup using archinstall Sway Configuration Linux QoL Tools Motivation For a while already, there has been no real reason for me to keep Windows. The doubt started about two years ago, in January 2023, when I got myself a Steam Deck. Although the purchase was mostly to emulate console and handheld games, of course the question was \u0026ldquo;But Can It Run Crysis?\u0026rdquo; To my surprise, the games I tried performed incredibly well, almost all of them out of the box. Thanks to ProtonDB, if there was a title that needed some extra configuration, it is usually there.\nBesides gaming, my work is either on a Mac or (used to be) on a Linux machine. When using Linux at work, the default for me was to set up Arch and i3wm (I use Arch btw). Moreover, after being a nano user for quite a while, it was time to move to vim. It\u0026rsquo;s a nice coincidence that Sway uses vim keybinds for moving windows, although one could easily configure that in i3wm.\nGiven the circumstances, let\u0026rsquo;s move to Linux!\nArch \u0026amp; Sway setup using archinstall archinstall is a guided installer to setup Arch. There are many posts on how to create a bootable USB stick with Arch, and how to run archinstall. Thus, instead of providing all the steps here, you may want to follow e.g. this one on debugpoint.com: Installing Arch Linux Using archinstall Automated Script up to the point of installing the Desktop Environment.\nBefore looking into what to select, first comes two common issues, one often faced on laptops and the other may faced when running on an Nvidia card. Scroll past that to Sway, Ly Greeter and Graphics Driver in case you don\u0026rsquo;t encounter any issues.\nIssues During Archinstall Archinstall boots into a Blackscreen on Nvidia It may happen that with Nvidia Graphics Cards, once you select Arch Linux Install from the bootmedium, you only see a blank black screen. In that case, try to add nomodeset.\nSelect the entry on the USB stick you want to boot from, then press e. Navigate your cursor to the end of the line, and append nomodeset. This should resolve the issue. If you\u0026rsquo;re curious what exactly this does, feel free to read this post on ubuntuforums.com: How to set NOMODESET ant other kernel boot options in grub2\nCannot connect to Wifi This issue is common on Laptops. To connect to the WiFi, one may use iwctl (as suggested). Trying to turn on the WiFi could result in operation failed.\nTo fix this, run\n1 rfkill unblock all and then retry.\nFor further reading on why and how, see rfkill caveat (ArchWiki/NetworkConfiguration).\nSway, Graphics Driver and Ly On the desktop environment installation, select the following\nDesktop - Sway This one goes without saying.\nGraphics Driver For the Graphics Driver, it depends on your Graphics Card. As my laptop has an RTX 3070, which is newer than Turing, I chose the \u0026ldquo;Nvidia (open kernel \u0026hellip;)\u0026rdquo;. To find which Nvidia driver you need, please consult ArchWiki/NVIDIA.\nGreeter - Ly My setup uses Ly as the greeter. This greeter takes you to the next level. Check it out on github.com/fairyglade/ly.\nNow, finish up the installation, and boot into your system.\nStart Sway - Nvidia Unsupported GPU As mentioned in the Arch Wiki, to start Sway with Nvidia drivers, one must explicitly state that this is an unspported GPU (ref). Edit the Sway Desktop file\n1 vim /usr/share/wayland-sessions/sway.desktop and change the Exec line to\n1 Exec=sway --unsupported-gpu Logout, select Sway, and start it \u0026hellip; aaand you may see a black screen. At least that happened when I did this, and I don\u0026rsquo;t have an explanation for this, as in, why it is fixed after a reboot. In case that doesn\u0026rsquo;t happen: Great. If it happens though, reboot, and then - at least on my machineâ„¢ - it worked.\nCongratulations. You most likely have an Arch \u0026amp; Sway setup with a fancy login screen now. In the next post, we\u0026rsquo;ll look into some tools to get simple Sway configuration with most important things sorted.\n","date":"2025-03-26T18:00:00Z","permalink":"https://revontulet.dev/p/2025-arch-and-i3-hello-sway-install-arch-and-sway/","title":"Arch and i3 - Hello, Sway! - Install Arch and Sway"},{"content":"Welcome to revontulet.dev! So \u0026hellip; what is this about?\nIntroduction My name is Matthias DÃ¶pmann. I\u0026rsquo;m a Software Engineer, originally from Germany, nowadays located at the Arctic Circle in Finland. There is no specific content for this blog. At the time of writing, I host a couple of articles on MisterDerpie.com, that I intend to migrate over to this blog.\nThere were several reasons I stopped writing on the old blog. The first is, that whilst living in Dublin and working for Amazon/Amazon Web Services, I was fairly occupied by work and rarely found motivation to follow up on these topics besides work. Second, in 2022, the world at Amazon was still remote, so on top of that I spent a lot of time travelling. Moreover, instead of reading computer science/software engineering books, I spent a good chunk studying languages (actual languages, not Programming Languages). These days I\u0026rsquo;m taking courses in Finnish, so I am not sure how much time is left to actually dump content here, but let\u0026rsquo;s see! Ultimately, the non-integration of the old blog with GitHub Actions or the like set the bar higher to publish contentent. Although I dockerized the process of rendering the blog, uploading it to an FTP after the render was still too annoying (\u0026hellip; and yes, whilst I could have automated that, I didn\u0026rsquo;t). Checking out a GitHub repository, write down some thought, and then just hit git commmit and git push is just a lot more convenient.\nGiven the content of my last blog, my work has shifted a bit. Whilst I was working in Java \u0026amp; Kotlin mostly, since 2025, my main language at work is Go (and YAML, loads of YAML).\nAbout Revontulet The reason to call this site \u0026ldquo;Revontulet\u0026rdquo; is that Revontulet (Wikipedia) is the Finnish word for Aurora (Wikipedia). I found that a unique enough name for a website, as well as coupling it close to where I am situated these days.\nThe cover photo is taken in Northern Lapland, Finland, in June 2024. You can find the exact location in the link section at the bottom.\n","date":"2025-03-16T00:00:00Z","image":"https://revontulet.dev/p/2025-hello-world/cover-min_hu_b810e63c2236e410.png","permalink":"https://revontulet.dev/p/2025-hello-world/","title":"Hello World"},{"content":"Using i3 and Ubuntu native is probably best, but you may not get the choice to do so, in case your working machine is running Windows. I looked through several guides how to set up Ubuntu with i3 on WSL2, but it took me several hours to get the setup I looked for because things did not work as expected. Therefore I\u0026rsquo;m writing this guide.\nInstalling Ubuntu on WSL2 There are plenty of tutorials and videos to do so. I will refer to the very one that I used to set up the bare, GUIless minimum, WSL2 Ubuntu GUI by David Bombai. Reproduce the steps until Install Ubuntu GUI (7:25) to get the headless Ubuntu on WSL2 up and running.\nInstalling i3 and VcXsrv Once you set up your Ubuntu, created a user and are prompted with the shell, update the package index.\n1 sudo apt update Next, install lxdm and i3. We also need to provide the information where Xserver sends its signals to. This is done by specifying the DISPLAY environment variable in .bashrc. The echo command adds this.\n1 2 sudo apt install -y lxdm i3 echo \u0026#34;export DISPLAY=\\$(cat /etc/resolv.conf | grep nameserver | awk \u0026#39;{print \\$2; exit;}\u0026#39;):0.0\u0026#34; \u0026gt;\u0026gt; ~/.bashrc After you setup all this, restart Ubuntu (e.g. run exit and then start it again).\nNext, install VcXsrv. Once installed, in your installation directory (default: C:\\Program Files\\VcXsrv) start xlaunch.exe. Launch it with the parameters provided in the screenshots.\nSelect Fullscreen, then click Next.\nSelect the option Start no client and click Next.\nIn Extra Settings, select Disable access control and click Next.\nYou will be presented with the final setup screen and don\u0026rsquo;t need to do anything here. Click Finish and your screen should turn black, because VcXsrv opened a fullscreen window with no clients attached. Press Alt-Tab to tab out the window, and go back to your Ubuntu terminal.\nIn your Ubuntu terminal, run i3 (i.e. type i3 and press enter) and you should start seeing some log (maybe including error messages). When you now tab back into the black fullscreen, after waiting a short time you should be able to select your i3 config. It doesn\u0026rsquo;t matter whether you run the config setup or not, for some reason it won\u0026rsquo;t be properly persisted. Thus, we need to \u0026ldquo;restore\u0026rdquo; the original config. Exit VcXsrv and go back to the Ubuntu console. If i3 hasn\u0026rsquo;t terminated, press CTRL+C to close it.\nRun the contents of this MisterDerpie/i3-default-config-alt-key gist in your Ubuntu shell. This you can do by copying the content and right-click into the Ubuntu window. It will create the proper i3 configuration needed for your next connection.\nRestart Ubuntu and restart VcXsrv (with the previous parameters). When you now run i3 again, go to VcXsrv you should see everything working.\nExtra: Oh My Zsh As I\u0026rsquo;m a very happy user of Oh My Zsh, and you may use it too, installing and making it the default shell would cause the DISPLAY variable to stop being defined. This is because it\u0026rsquo;s part of .bashrc and not .zshrc.\nAssuming you don\u0026rsquo;t have Oh My Zsh installed yet, below script will do the job.\n1 2 sudo apt install -y git zsh sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; Once installed, issue below command to reenable the proper definition of the DISPLAY environment variable.\n1 echo \u0026#34;export DISPLAY=\\$(cat /etc/resolv.conf | grep nameserver | awk \u0026#39;{print $2; exit;}\u0026#39;):0.0\u0026#34; \u0026gt;\u0026gt; ~/.zshrc ","date":"2021-12-04T19:00:00Z","permalink":"https://revontulet.dev/p/2021-ubuntu-gui-wsl2/","title":"Ubuntu with WSL2 and i3 Window Manager GUI"},{"content":"During my summer vacation, I got myself a Lenovo Legion 5 Pro, powered by an AMD Ryzen 7 5800H and an Nvidia RTX3070. Though this post is not about the legion, I have to say it is an awesome laptop. What this post is about is how to make Ubuntu with i3wm run on the Lenovo Legion.\nInstalling Ubuntu Create Bootable USB Media You need a USB stick as well as a Ubuntu ISO image. Plenty of tutorials exist on the web to create a bootable Ubuntu USB stick. Ubuntu.com provides official documentation, coming from a device running Windows, Ubuntu or Mac OS X.\nBoot USB Media Once you created your boot device, boot up the Legion and press F12 to get into the boot menu. From there, select the USB stick.\nNext you are presented with the GRUB interface, where you have to select Ubuntu (safe graphics) and then press RETURN. After a while (length of the while depends on the speed of your USB stick) you should see a window asking whether you want to Try or Install Ubuntu. Select Install Ubuntu.\nThere is only one part in the installer you need to select a certain option to not run into issues. At some point you should be asked whether you want to have a Normal Installation or Minimal Installation. See this image in the official guide. In the bottom of the window, below Other options, you have to select Install third-party software. This is necessary to install the proprietary Nvidia driver. If you don\u0026rsquo;t, you will most likely be presented a black screen after booting into Ubuntu.\nFinish the installation and reboot into Ubuntu.\ni3 Install i3 Once you booted to your freshly installed Ubuntu, it is very straightforward to install i3. Run this command in your Terminal.\n1 sudo apt install i3 That\u0026rsquo;s already it.\nReplace gdm The next step is to replace GNOME Display Manager, abbreviated gdm. We will replace it with LightDM. It is as easy as installing i3.\n1 sudo apt install lightdm At some point during the installation, a window pops up in your terminal, asking you to select your display manager. In the list, use the arrow keys to navigate to lightdm and hit RETURN to confirm.\nIf you ever want to go back to gdm, you can do so by running below command.\n1 sudo dpkg-reconfigure gdm3 Why replace gdm? This is the part where I ran into a problem that may only be with my setup. When having multiple screens connected (1 over HDMI, 1 over Lenovo Dock), I could login to i3 but then everything was frozen. But it was not Linux that froze, as I could tell from the clock continuing to run. Logging into Gnome worked though. Also, logging into Gnome before logging into i3 made it work. But neither do I want to use Gnome nor do I want to take extra steps every time I boot Linux. So I tried back and forth, and the only way I got this to work is to use lightdm over gdm.\nThe fact that I wasted quite some time on this motivated me sharing a short blog post. Using a different display manager might not suit everyone, but probably for most users this does the job.\n","date":"2021-08-22T15:00:00Z","permalink":"https://revontulet.dev/p/2021-lenovo-legion-ubuntu/","title":"Ubuntu and i3 on Lenovo Legion 5 Pro"},{"content":"In my previous post I stated that I am currently reading Kotlin in Action (2017, Manning). In chapter 9, the authors introduce generics in Kotlin. My overall experience with generics in Java is good, but apparently that book taught me something completely new. The concept of Covariance and Contravariance was something unknown to me. This post, though categorized Kotlin, focuses more on the concepts in general, yet will demonstrate them in Kotlin. I assume the reader has a basic understanding of generics and inheritance.\nInheritance for normal Classes Before we get into generics, let us look into inheritance for normal classes. For that, consider the following class hierarchy.\n1 2 3 abstract class ClothingItem(val price: Float) class Shoe(price: Float, val size: Float) : ClothingItem(price) class Jacket(price: Float, val hasZipper: Boolean) : ClothingItem(price) Between Shoe and Jacket there is no relationship in terms of \u0026ldquo;Shoe is-a Jacket\u0026rdquo; or \u0026ldquo;Jacket is-a Shoe\u0026rdquo;. Thus, whenever we expect a shoe, we cannot treat it as a jacket and vice versa. But between ClothingItem and Shoe/Jacket, there is in fact an is-a relationship. Shoes and jackets are both clothing items. Hence we can always use a Shoe or Jacket in code when we expect a ClothingItem.\n1 2 3 fun printPrice(item: ClothingItem) { println(\u0026#34;The item costs ${item.price}.\u0026#34;) } We can call that with either shoe or jacket.\n1 2 3 4 5 // The item costs 5. printPrice(Shoe(5f, 33.5f)) // The item costs 20. printPrice(Jacket(20f, false)) Covariance To explain how inheritance in generics works, and the word \u0026ldquo;covariance\u0026rdquo;, I want to do that by an example.\nDomain Model We create a Basket where we can put exactly 2 of any clothing item. Also we define a function to print the total price of the items in the basket.\n1 2 3 4 5 6 7 class Basket\u0026lt;T : ClothingItem\u0026gt;(val firstItem: T, val secondItem: T) fun printBasketPrice(basket: Basket\u0026lt;ClothingItem\u0026gt;) { val firstPrice = basket.firstItem.price val secondPrice = basket.secondItem.price val total = basket.firstItem.price + basket.secondItem.price println(\u0026#34;The total price of $firstPrice + $secondPrice = $total.\u0026#34;) } Now, assume we only want to buy shoes, so we create a basket only containing shoes. This saves us from accidently putting a jacket into the basket. We then want to print its price, using the previously defined function.\n1 2 val shoeBasket = Basket\u0026lt;Shoe\u0026gt;(Shoe(5.5f, 37.5f)) printPrice(shoeBasket) When we try to run this, it won\u0026rsquo;t compile, because the types are not matching.\nBasket\u0026lt;Shoe\u0026gt; not is-a Basket\u0026lt;ClothingItem\u0026gt; This is strange. We know that Shoe is-a ClothingItem, and wherever we expect the latter, we could actually provide the former. It turns out that Basket\u0026lt;Shoe\u0026gt; is not a Basket\u0026lt;ClothingItem\u0026gt;.\nEven though the inner types are in an is-a relationship with each other, in the generic world this does not apply. Why is that?\nIt is best shown by changing the basket to be able to replace items, i.e. change val to var. Now we add a function that is able to replace the first clothing item by one that is provided. This won\u0026rsquo;t compile, but if it would, the program would crash at the print.\n1 2 3 4 5 6 7 8 9 10 11 class Basket\u0026lt;T : ClothingItem\u0026gt;(var firstItem: T, var secondItem: T) fun replaceFirstItem(basket: Basket\u0026lt;ClothingItem\u0026gt;, item: ClothingItem) { basket.firstItem = item } //... called from some other place val basket = Basket\u0026lt;Shoe\u0026gt;(Shoe(1f, 2f), Shoe(3f, 4f)) replaceFirstItem(basket, Jacket(5f, false)) // The print causes the program to crash // Basket\u0026lt;Shoe\u0026gt; now holds a Jacket, and Jacket has no size property println(\u0026#34;The shoe has size ${basket.firstItem.size}.\u0026#34;) And that is the reason why the is-a relationship for generics is not safe to treat the same way as for normal classes. It is unsafe for a function that expects a more generic type to pass a more specific one.\nBut if we could not do that, this would limit us by a lot. You\u0026rsquo;ve seen the function to print the price of the basket.\n1 2 3 fun printBasketPrice(basket: Basket\u0026lt;ClothingItem\u0026gt;) { // ... } If we would not be able to do that, we would have to create the same function for each type of clothing. That may be possible, but neither keeps your code DRY nor would it be feasible if you had hundreds of items.\nDefinition of Covariance The previous can be achieved with covariance. Formally defined:\n1 2 Given a class with a generic parameter: SomeClass\u0026lt;T\u0026gt;. We say SomeClass\u0026lt;A\u0026gt; is-a SomeClass\u0026lt;B\u0026gt; if A is-a B, and that SomeClass\u0026lt;T\u0026gt; is covariant on T. Informally speaking, the \u0026ldquo;normal object inheritance\u0026rdquo; holds. Thus the covariant basket inheritance diagram looks like the following.\nCovariance in Code Covariance exactly solves our problem. But how do we achieve that in code? And, if we enable that, would we run into the same problem that we could maybe override values with the wrong type?\nIn Kotlin, you define covariance on T using the keyword out.\n1 2 3 4 5 6 // Note, we use val again class Basket\u0026lt;out T : ClothingItem\u0026gt;(val firstItem: T, val secondItem: T) // ... call it val basket = Basket\u0026lt;Shoe\u0026gt;(Shoe(1f, 2f), Shoe(3f, 4f)) println(\u0026#34;The shoe has size ${basket.firstItem.size}.\u0026#34;) The keyword is not coincidantelly named out, it actually has a specific reason. When you mark the generic parameter as covariant, you can only use an instance of T in out positions. This does probably introduce another questionmark, because what are out positions? Informally said, an out position is everything that is returning, but never receiving. An instance of T could be acquired by getting from the basket, because the getter would return the instance. An operation like setting a variable to an instance would not be allowed, because the setter would receive the instance.\nWith that said, with a covariant basket, we could never run into overriding an item with a wrong instance. How should we? The operation to write is a receiving operation, thus not an out operation. This is also the reason we have to define the items as val instead of var. As var provides setters, it wouldn\u0026rsquo;t compile.\nContravariance Covariance is a concept that is quite natural, because it follows the \u0026ldquo;normal flow\u0026rdquo; of inheritance. What is way less natural is the concept of contravariance, because it allows you to walk \u0026ldquo;upwards\u0026rdquo; in the inheritance tree - kind of.\nAdding an Action Object Let\u0026rsquo;s create an abstract base class that supports a single function, provided two objects, and returns nothing.\n1 2 3 abstract class Action\u0026lt;T\u0026gt; { abstract fun performAction(firstItem: T, secondItem: T): Unit } For contravariance we want to work with a basket full of jackets. So we create an action specific to jackets.\n1 2 3 4 5 6 7 8 9 class JacketAction : Action\u0026lt;Jacket\u0026gt;() { override fun performAction(firstItem: Jacket, secondItem: Jacket) { if (firstItem.hasZipper \u0026amp;\u0026amp; secondItem.hasZipper) { println(\u0026#34;Both jackets have zippers!!!\u0026#34;) } else { println(\u0026#34;You have at least one non-zipper jacket.\u0026#34;) } } } Call Action on printing Let\u0026rsquo;s create a method that takes a basket full of jackets and an action for jackets. Also we create a jacket basket and call the method with it and the previously defined jacket action.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 fun printComplimentAndPerformAction(basket: Basket\u0026lt;Jacket\u0026gt;, action: Action\u0026lt;Jacket\u0026gt;) { println(\u0026#34;Nice jackets!\u0026#34;) action.performAction(basket.firstItem, basket.secondItem) } // ... in the caller val jacketBasket: Basket\u0026lt;Jacket\u0026gt; = Basket(Jacket(1f, false), Jacket(2f, false)) val jacketAction = JacketAction() printComplimentAndPerformAction(jacketBasket, jacketAction) /* Output Nice jackets! You have at least one non-zipper jacket. */ More general action Say we want to have an action we can perform on any clothing item. That is a common use-case. You have a less specific type (in our example: Clothing Item) and want to perform an action based on methods already provided by that type. Such action could, for example, be to print the more expensive of the two items. If we had to create that for every single item type, we would repeat code all over the place and this would not scale at all. Not to mention again that there are many clothing items.\n1 2 3 4 5 6 7 8 9 class ClothingActionHighestPrice : Action\u0026lt;ClothingItem\u0026gt;() { override fun performAction(firstItem: ClothingItem, secondItem: ClothingItem) { if (firstItem.price \u0026gt; secondItem.price) { println(\u0026#34;First item is more expensive.\u0026#34;) } else { println(\u0026#34;Second item is more expensive\u0026#34;) } } } We would like to call the printComplimentAndPerformAction method with the ClothingActionHighestPrice. But this will not compile.\n1 2 3 val clothingActionHighestPrice = ClothingActionHighestPrice() // This call won\u0026#39;t compile printComplimentAndPerformAction(jacketBasket, clothingActionHighestPrice) At first glance, this seems to be intuitive. We know that Jacket is-a ClothingItem, so we cannot pass a ClothingItem when we want to have a Jacket. So what we want to do is shown in below diagram.\nDefinition of Contravariance It is possible to achieve previous diagram by contravariance.\n1 2 Given a class with a generic parameter: SomeClass\u0026lt;T\u0026gt;. We say SomeClass\u0026lt;A\u0026gt; is-a SomeClass\u0026lt;B\u0026gt; if B is-a A, and that SomeClass\u0026lt;T\u0026gt; is contravariant on T. Informally speaking, we say that our class with generic parameter A is a child of another class with generic parameter B if B supersedes A. The diagram shows that we want to do exactly that. ClothingItem supersedes Jacket, yet we want the Action\u0026lt;ClothingItem\u0026gt; to be a Action\u0026lt;Jacket\u0026gt;.\nWhen I read about contravariance the first time, I was confused. My confusion came from that I thought we are actually allowing the Action\u0026lt;ClothingItem\u0026gt; to perform methods on Jacket. This is obviously not the case, the clothing item action only sees a clothing item.\nContravariance in Code For covariance there is out, and contravariance is more or less the opposite of covariance. It\u0026rsquo;s not a big surprise that the keyword for contravariance is in. On the abstract base class, the Action, we make T covariant.\n1 2 3 abstract class Action\u0026lt;in T\u0026gt; { abstract fun performAction(firstItem: T, secondItem: T): Unit } As with the out keyword, the in keyword also specifies where the type is allowed to be placed at. You can only place T on receiving, not returning operations. Passing the jackets into the performAction function is receiving, so we can safely do that. Yet we cannot receive T from performAction, because the base type is unknown to that method. It could be anything more specific.\nAssume for contravariance you would allow to also return the type. When the printComplimentAndPerformAction method, defined on jacket, would receive some clothing item, it could also be a shoe. This would not be safe anymore.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 abstract class Action\u0026lt;in T\u0026gt; { // Having T in the return is not allowed abstract fun performAction(firstItem: T, secondItem: T): T } // ... class NotAllowedReturn : Action\u0026lt;ClothingItem\u0026gt;() { override fun performAction(firstItem: ClothingItem, secondItem: ClothingItem): ClothingItem { return Shoe(1f, 1f) } } // ... fun printComplimentAndPerformAction(basket: Basket\u0026lt;Jacket\u0026gt;, action: Action\u0026lt;Jacket\u0026gt;) { println(\u0026#34;Nice jackets!\u0026#34;) val receivedItem = action.performAction(basket.firstItem, basket.secondItem) // If firstItem were var, we would put a shoe in here // Which violates the contract that there are only jackets basket.firstItem = receivedItem } Invariance Now that covariance and contravariance are covered, the last piece is the default mode: Invariance.\n1 2 Given a class with a generic parameter: SomeClass\u0026lt;T\u0026gt;. We say SomeClass\u0026lt;A\u0026gt; is invariant to SomeClass\u0026lt;B\u0026gt; if neither SomeClass\u0026lt;A\u0026gt; is-a SomeClass\u0026lt;B\u0026gt; nor SomeClass\u0026lt;B\u0026gt; is-a SomeClass\u0026lt;A\u0026gt;. In simple words: Whenever an instance of SomeClass\u0026lt;A\u0026gt; is expected, you must provide SomeClass\u0026lt;A\u0026gt;, and cannot provide a SomeClass\u0026lt;T\u0026gt; with a T being higher/lower in the class hierarchy. This is the default mode for any generic class in Kotlin. That is why the first example didn\u0026rsquo;t work and we had to make it covariant.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 abstract class ClothingItem(val price: Float) class Shoe(price: Float, val size: Float) : ClothingItem(price) class Jacket(price: Float, val hasZipper: Boolean) : ClothingItem(price) // by default, T is invariant class Basket\u0026lt;T : ClothingItem\u0026gt;(val firstItem: T, val secondItem: T) // and thus this method only takes a basket of type ClothingItem fun printBasketPrice(basket: Basket\u0026lt;ClothingItem\u0026gt;) { val firstPrice = basket.firstItem.price val secondPrice = basket.secondItem.price val total = basket.firstItem.price + basket.secondItem.price println(\u0026#34;The total price of $firstPrice + $secondPrice = $total.\u0026#34;) } fun main(args: Array\u0026lt;String\u0026gt;) { val shoeBasket = Basket\u0026lt;Shoe\u0026gt;(Shoe(5.5f, 37.5f)) // so this will not compile printBasketPrice(shoeBasket) } Bottom Line Covariance and contravariance enable us to provide compile time guarantee for runtime safety. The most prominent example for covariance is the Collection interface. Though contravariance has fewer use cases, you probably have used that one probably already. The Comparator interface is the most prominent candidate where contravariance is used.\n","date":"2021-07-04T22:00:00Z","permalink":"https://revontulet.dev/p/2021-generics-invariance-covariance-contravariance/","title":"Generics - Invariance, Covariance and Contravariance"},{"content":"The repository with the Kotlin code can be found on github/MisterDerpie/kotlin-data-stubs.\nCurrently I\u0026rsquo;m having the great luck that our team started using Kotlin at work. For that reason, I decided to learn Kotlin properly and started my journey with Kotlin in Action (2017, Manning). This is a bit outdated, considering that Kotlin 1.5 is the current release, but the vast majority of concepts are the same as for 1.0.\nOne concept that fascinates me and I really see a lot of value in is Extensions. Especially for tests I find that concept very useful, because it enables us to provide test related logic to our classes without actually adding this in the real production code. Let\u0026rsquo;s have a look at how we can utilize extensions to create stubs of business classes.\nProblem statement in Java Test Utils to Stub Classes A problem that often arises in Java projects is that you need to provide stubs for using e.g. the same customer in multiple tests. Suppose you have a class Customer.java that looks like this\n1 2 3 4 5 6 7 8 9 @AllArgsConstructor /* Generates constructor with all parameters */ @Getter /* Generates getters for all properties */ public class Customer { private final UUID id; private final String name; private final Integer age; } and you would like to stub the same customer for your tests. How would you do that? Probably you create some class called TestUtils.java, that provides a static method stubCustomer. In larger projects, you most likely have many different stubs in it, from Address over Customer to Shopping Cart. Thus, you start adding all these stubs in the TestUtils class.\n1 2 3 4 5 6 7 8 9 10 11 12 public class TestUtils { public static Customer stubCustomer() { ... } public static Address stubAddress() { ... } ... } I have seen such classes having hundreds of lines of code, with many different, totally unrelated objects. A first attempt to improve that would be to create TestUtilsCustomer.java, TestUtilsAddress.java, \u0026hellip; That would leave us with a better, yet still not the best solution.\nTest Code meets Production Code Wouldn\u0026rsquo;t it be nice to provide stubCustomer directly in the Customer itself? With Java, you would have to include such method in your production code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 @AllArgsConstructor /* Generates constructor with all parameters */ @Getter /* Generates getters for all properties */ public class Customer { private final UUID id; private final String name; private final Integer age; public static Customer stubFullAgeCustomer() { return new Customer(UUID.randomUUID(), \u0026#34;JavaName\u0026#34;, 18) } } Testcode should never, never reside in production code. With Kotlin, we can actually do that, but without production code ever knowing about this stubFullAgeCustomer method.\nSolution in Kotlin Before continue reading, think about why it would be nice to have a preconfigured Customer provided by the Customer object itself. When implementing the Builder Pattern, you configure each property by a callchain of functions with the properties\u0026rsquo; names. It could look like this Customer.name(\u0026quot;Builder\u0026quot;).age(23).uuid(UUID.randomUUID()).build(). So why not getting a fully preset instance as part of this object, too?\nThis can be achieved in Kotlin.\nSetup We define a Customer data class and an AgeCheckService (the latter is purely to have something to test, so that I can show how our extension works).\nCustomer.kt\n1 2 3 4 5 6 7 8 9 10 package dto import java.util.UUID data class Customer( val id: UUID, val name: String, val age: Int) { companion object {} } What enables us to extend the Customer with new \u0026ldquo;static\u0026rdquo; methods for our test is the companion object. As this is a pure data class, we don\u0026rsquo;t add anything else.\nAgeCheckService.kt\n1 2 3 4 5 6 7 8 9 10 11 package service import dto.Customer class AgeCheckService { companion object { fun isCustomerOfFullAge(customer: Customer): Boolean { return customer.age \u0026gt;= 18 } } } This services only functionality is to return true when the customer is of full age, and false when they\u0026rsquo;re minor. As initially stated, this is for the sole purpose of having something to test.\nExtend Customer In our test, we would like to call the following:\n1 2 Customer.stubFullAgeCustomer() Customer.stubMinorAgeCustomer() to retrieve preconfigured customers with some age greater or equal than 18 or less than 18, respectively. This is as easy as doing companion object extensions.\nTo do so, simply place a file in your testpath, and define the functions on Customer.Companion.\n1 2 3 4 5 6 7 8 9 10 11 package dto import java.util.UUID fun Customer.Companion.stubFullAgeCustomer(): Customer { return Customer(UUID.nameUUIDFromBytes(\u0026#34;This is some string for UUID\u0026#34;.toByteArray()), \u0026#34;MisterDerpie\u0026#34;, 24) } fun Customer.Companion.stubMinorAgeCustomer(): Customer { return Customer(UUID.nameUUIDFromBytes(\u0026#34;This is another UUID string\u0026#34;.toByteArray()), \u0026#34;MinorPerson\u0026#34;, 17) } Note that when you do not define a name for the companion object in the class, it will be accessible by Companion. Congratulations, you just added extensions which you can use in your tests.\nTest AgeCheckService using Extension Stub The last step is to import your Extensions into your test. They are not automatically applied globally to your Customer, which is the reason why you explicitly have to import them.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package service import dto.Customer import dto.stubFullAgeCustomer import dto.stubMinorAgeCustomer import org.junit.jupiter.api.Test import org.assertj.core.api.Assertions.assertThat class AgeCheckServiceTest { @Test fun `should return true when customer is of full age`() { val customer = Customer.stubFullAgeCustomer() assertThat(AgeCheckService.isCustomerOfFullAge(customer)).isEqualTo(true) } @Test fun `should return false when customer is of minor age`() { val customer = Customer.stubMinorAgeCustomer() assertThat(AgeCheckService.isCustomerOfFullAge(customer)).isEqualTo(false) } } See the imports at the top. I placed the Customer in a package called dto. The stubFullAgeCustomer and stubMinorAgeCustomer are also placed in a package called dto (but the name doesn\u0026rsquo;t matter, it\u0026rsquo;s just due to the same folder structure in the tests). As stated initially, we need to import them to actually apply the extension to the Customer.\nThe tests are then straightforward: Get the customer and assert that the age service in fact returns true when they\u0026rsquo;re of full age and false when they\u0026rsquo;re of minor age.\n","date":"2021-06-25T22:00:00Z","permalink":"https://revontulet.dev/p/2021-teststubs-with-kotlin-extensions/","title":"Teststubs with Kotlin Extensions"},{"content":"The sourcecode of this post is available on github/MisterDerpie/spring-boot-with-mongodb.\nForeword For a small application to store receipts I wanted to use Spring Boot and NoSQL database MongoDB. As with many basic topics in the spring world, there is a Getting Started guide on spring.io, with the specific title \u0026ldquo;Accessing Data with MongoDB\u0026rdquo;.\nThough this guide may suffice for a really straightforward start, it actually misses out two, from my point of view, essential questions.\nHow to connect to a MongoDB instance? How to integration test MongoDB? Therefore I wrote this post. As I want to use JavaMoney, I also cover Mongo Converters. The list explains what is covered in this post.\nUse MongoDB as a data store in Spring Connect to a MongoDB instance with your provided credentials Integration test with an embedded MongoDB Use JavaMoney with MongoDB Setup MongoDB First we need an instance of MongoDB. I will - as usual - use Docker. Therefore I assume you have docker installed, if not, there are plenty of guides how to do that available online. We use the Docker-Mongo image. To start a MongoDB instance that is running in the background, run below code from the shell.\n1 2 3 4 5 6 docker container run -d \\ --name mongodb \\ -e MONGO_INITDB_ROOT_USERNAME=admin \\ -e MONGO_INITDB_ROOT_PASSWORD=admin \\ -p 27017-27019:27017-27019 \\ mongo:latest Setup Spring Boot Project Create Spring Project This section is about creating the Spring project. After you created the project, don\u0026rsquo;t forget to add the additional dependencies (next section).\nCreate project with Spring Initializr The easiest way to create the project is using Spring Initializr. I will use Gradle as the build automation tool, but it doesn\u0026rsquo;t matter and you can well select Maven.\nWe will include the 2 starters\nLombok Spring Data MongoDB To do so, click on Add Dependencies (top right corner) and search for Spring Data Mongo DB (don\u0026rsquo;t use the reactive one) and Lombok.\nSelect that you want to have a Gradle Project, use Java, fill out the Project Metadata, select Packaging Jar and Java 11. Then click Generate and you should get a .zip containing your project. Unzip this anywhere and open it in the IDE of your choice. I prefer IntelliJ.\nCreate project with Gradle In case you want to create your Gradle project from scratch, you can follow docs.gradle.org - Building Java Applications Sample. Add below dependencies to your dependencies block in the build.gradle file.\n1 2 3 4 5 implementation \u0026#39;org.springframework.boot:spring-boot-starter-data-mongodb\u0026#39; implementation \u0026#39;org.springframework.boot:spring-boot-starter-web\u0026#39; compileOnly \u0026#39;org.projectlombok:lombok\u0026#39; annotationProcessor \u0026#39;org.projectlombok:lombok\u0026#39; testImplementation \u0026#39;org.springframework.boot:spring-boot-starter-test\u0026#39; Additional Dependencies To show how to integration test and how to use JavaMoney with MongoDB, we add two dependencies, namely Flapdoodle Embedded MongoDB and JavaMoney. Add\n1 2 3 testImplementation \u0026#39;org.springframework.boot:spring-boot-starter-test\u0026#39; testImplementation \u0026#39;de.flapdoodle.embed:de.flapdoodle.embed.mongo:3.0.0\u0026#39; implementation \u0026#39;org.javamoney.moneta:moneta-core:1.4.2\u0026#39; to your dependencies in the build.gradle file. It should then look similar to this.\n1 2 3 4 5 6 7 8 dependencies { implementation \u0026#39;org.springframework.boot:spring-boot-starter-data-mongodb\u0026#39; compileOnly \u0026#39;org.projectlombok:lombok\u0026#39; annotationProcessor \u0026#39;org.projectlombok:lombok\u0026#39; testImplementation \u0026#39;org.springframework.boot:spring-boot-starter-test\u0026#39; testImplementation \u0026#39;de.flapdoodle.embed:de.flapdoodle.embed.mongo:3.0.0\u0026#39; implementation \u0026#39;org.javamoney.moneta:moneta-core:1.4.2\u0026#39; } Connect Application to MongoDB This part only covers how to configure your application to be able to connect with MongoDB. I assume you have a MongoDB instance running configured the same way as I did with the docker image. That is, listening on port 27017, with user admin and password admin. Note that you should not use those credentials in a real application of course, but for only running this on your local machine and getting started this is perfectly fine.\nIn the resources folder (where the application.properties resides), create a new properties file called e.g. application-production.properties and put the following content in it.\n1 2 3 4 5 6 spring.data.mongodb.authentication-database=admin spring.data.mongodb.username=admin spring.data.mongodb.password=admin spring.data.mongodb.database=testdatabase spring.data.mongodb.port=27017 spring.data.mongodb.host=localhost This enables Spring\u0026rsquo;s MongoDB repository to\nconnect to a MongoDB instance running on localhost with port 27017 and username admin and password admin and selected database testdatabase and authentication database admin (default in the docker container) Why not the default application.properties? You may wonder why we don\u0026rsquo;t use the default properties file. The reason is that the embedded MongoDB for some reason does not override these values. In case you run your integration tests without providing a profile and/or not providing these values, it would fall back to the default properties file. Thus the tests would fail, as you could not connect to the database.\nFor the sake of completion, before your tests even start you would see this error.\n1 org.springframework.beans.factory.BeanCreationException: Error creating bean with name \u0026#39;embeddedMongoServer\u0026#39; Create Mongo Document in Application The next part is to create a MongoDB document representation class. Simply put, this is the entity you are going to persist in the database. We keep it simple and want to store an Item we purchased. Its values are an id, a name and a price. We will use the id as the primary key.\nCreate a class called Item with the following contents.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import lombok.AllArgsConstructor; import lombok.Data; import org.javamoney.moneta.FastMoney; import org.springframework.data.annotation.Id; import org.springframework.data.mongodb.core.mapping.Document; import java.util.UUID; @Document @AllArgsConstructor @Data public class Item { @Id public UUID id; public FastMoney price; public String name; } Let\u0026rsquo;s explain a bit what\u0026rsquo;s going on here.\n@Document indicates that we want this class to be persistable in MongoDB. In case you are familiar with JPA, this is the equivalent to @Entity.\n@AllArgsConstructor and @Data are Lombok annotations. The former one creates what it states, a constructor with all parameters. With the latter we automatically create getters and setters.\n@Id marks this field as the primary key to use in the database.\nUse MongoRepository to access Document The next step is that we want to store our document in the database. Spring provides a very easy to use interface for that. Just create a class called ItemRepository that extends MongoRepository\u0026lt;T, ID\u0026gt;.\n1 2 3 4 5 6 7 import org.springframework.data.mongodb.repository.MongoRepository; import java.util.UUID; public interface ItemRepository extends MongoRepository\u0026lt;Item, UUID\u0026gt; { } The first generic parameter of MongoRepository indicates what entity we want to persist. As our Item is the entity, we put this there. To identify items, we set the primary key of type UUID. Therefore we put the ID type as the second generic parameter.\nIntegration Test MongoDB We are done, almost. Let us test our MongoRepository whether it works as expected (hint, it doesn\u0026rsquo;t, but we will get to that).\nCreate the test Create a test class ItemRepositoryTest with the following content\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import org.javamoney.moneta.FastMoney; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.autoconfigure.data.mongo.DataMongoTest; import java.util.UUID; import static org.assertj.core.api.Assertions.assertThat; @DataMongoTest public class ItemRepositoryTest { @Autowired ItemRepository itemRepository; @Test public void shouldStoreItem() { Item item = new Item(UUID.randomUUID(), FastMoney.of(1, \u0026#34;EUR\u0026#34;), \u0026#34;Test Item\u0026#34;); itemRepository.save(item); Item storedItem = itemRepository.findById(item.getId()).orElseThrow(); assertThat(storedItem.getId()).isEqualTo(item.getId()); assertThat(storedItem.getPrice()).isEqualTo(item.getPrice()); assertThat(storedItem.getName()).isEqualTo(item.getName()); } } Let us look at what\u0026rsquo;s done here, before we run the test.\n@DataMongoTest starts the Spring Boot Test context. In addition to that it also creates an embedded MongoDB instance.\n@Autowired \u0026ldquo;injects the dependency\u0026rdquo; of ItemRepository into the test class. Simply put, Spring creates an instance of ItemRepository and puts the reference to that instance in itemRepository. So itemRepository is not null, and the creation of it is done by Spring\u0026rsquo;s Dependency Injection container. If you want to to read more about it click on the links for @Autowired and Spring Dependency Injection.\nThe test itself then is pretty simple. We create an Item with a random UUID, value of 1 euro and the name \u0026ldquo;Test Item\u0026rdquo;. Then we store this in the database and try to query the database for the item ID, asserting that the retrieved item is equal to the initial item.\nRun the test When we try to run the test, it will even fail before the first assertion is reached. Looking at the logs, we can see the following error.\n1 2 3 Failed to instantiate org.javamoney.moneta.FastMoney using constructor NO_CONSTRUCTOR with arguments org.springframework.data.mapping.model.MappingInstantiationException: Failed to instantiate org.javamoney.moneta.FastMoney using constructor NO_CONSTRUCTOR with arguments ... The problem is the internal representation of FastMoney. When serialized into a JSON object, it actually does not just look like\n1 { \u0026#34;number\u0026#34;: 1, \u0026#34;currency\u0026#34;: \u0026#34;EUR\u0026#34; } but like\n1 {\u0026#34;currency\u0026#34;:{\u0026#34;context\u0026#34;:{\u0026#34;providerName\u0026#34;:\u0026#34;java.util.Currency\u0026#34;,\u0026#34;empty\u0026#34;:false},\u0026#34;numericCode\u0026#34;:978,\u0026#34;defaultFractionDigits\u0026#34;:2,\u0026#34;currencyCode\u0026#34;:\u0026#34;EUR\u0026#34;},\u0026#34;number\u0026#34;:1.00000,\u0026#34;precision\u0026#34;:1,\u0026#34;factory\u0026#34;:{\u0026#34;defaultMonetaryContext\u0026#34;:{\u0026#34;fixedScale\u0026#34;:true,\u0026#34;maxScale\u0026#34;:5,\u0026#34;amountType\u0026#34;:\u0026#34;org.javamoney.moneta.FastMoney\u0026#34;,\u0026#34;precision\u0026#34;:19,\u0026#34;providerName\u0026#34;:null,\u0026#34;empty\u0026#34;:false},\u0026#34;amountType\u0026#34;:\u0026#34;org.javamoney.moneta.FastMoney\u0026#34;,\u0026#34;maxNumber\u0026#34;:92233720368547.75807,\u0026#34;minNumber\u0026#34;:-92233720368547.75808,\u0026#34;maximalMonetaryContext\u0026#34;:{\u0026#34;fixedScale\u0026#34;:true,\u0026#34;maxScale\u0026#34;:5,\u0026#34;amountType\u0026#34;:\u0026#34;org.javamoney.moneta.FastMoney\u0026#34;,\u0026#34;precision\u0026#34;:19,\u0026#34;providerName\u0026#34;:null,\u0026#34;empty\u0026#34;:false}},\u0026#34;context\u0026#34;:{\u0026#34;fixedScale\u0026#34;:true,\u0026#34;maxScale\u0026#34;:5,\u0026#34;amountType\u0026#34;:\u0026#34;org.javamoney.moneta.FastMoney\u0026#34;,\u0026#34;precision\u0026#34;:19,\u0026#34;providerName\u0026#34;:null,\u0026#34;empty\u0026#34;:false},\u0026#34;zero\u0026#34;:false,\u0026#34;positive\u0026#34;:true,\u0026#34;negative\u0026#34;:false,\u0026#34;negativeOrZero\u0026#34;:false,\u0026#34;positiveOrZero\u0026#34;:true,\u0026#34;scale\u0026#34;:5} But FastMoney does only have a constructor that accepts these very two parameters. The solution is to provide a custom converter, that would store our money representation exactly as what the constructor expects.\nMongoDB Converter Create Converters To successfully save and load our Item from the database, we need to convert the price. This is done by creating a ReadingConverter and a WritingConverter. Create two classes, namely FastMoneyReadConverter and FastMoneyWriteConverter with the following contents.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import org.javamoney.moneta.FastMoney; import org.springframework.core.convert.converter.Converter; import org.springframework.data.convert.ReadingConverter; import java.math.BigDecimal; @ReadingConverter public class FastMoneyReadConverter implements Converter\u0026lt;String, FastMoney\u0026gt; { public FastMoney convert(String input) { String[] storedValue = input.split(\u0026#34;###\u0026#34;); return FastMoney.of(new BigDecimal(storedValue[0]), storedValue[1]); } } 1 2 3 4 5 6 7 8 9 10 11 12 import org.javamoney.moneta.FastMoney; import org.springframework.core.convert.converter.Converter; import org.springframework.data.convert.WritingConverter; @WritingConverter public class FastMoneyWriteConverter implements Converter\u0026lt;FastMoney, String\u0026gt; { public String convert(FastMoney input) { return input.getNumber() + \u0026#34;###\u0026#34; + input.getCurrency(); } } Our converters implement the Converter\u0026lt;Input, Output\u0026gt; interface. The naming states exactly what we want to achieve with them. With a ReadingConverter, we want to convert when reading from the database into the Java Object. A WritingConverter is used when we write into the database from the Java Object.\nWhat the converters do is very straightforward. We take the values from the Money, concatenate them with a placeholder of three hashtags and then save this entire string to the database. On reading from the database, we revert that operation by splitting them at the three hashtags and creating a FastMoney instance of the two obtained values.\nCreate Converter Config We created our converters but need to make the MongoRepository aware of using them when storing/loading documents from MongoDB. For that, we simply create a configuration and provide a list of MongoCustomConversions.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.data.mongodb.core.convert.MongoCustomConversions; import java.util.List; @Configuration public class ConverterConfig { @Bean public MongoCustomConversions mongoCustomConversions() { return new MongoCustomConversions(List.of(new FastMoneyReadConverter(), new FastMoneyWriteConverter())); } } @Configuration loads this class in the Spring context.\n@Bean tells Spring to get an instance of the return type (here: MongoCustomConversions) by calling this method. In the mongoCustomConversion method we pass a list of our converters to the constructor of MongoCustomConversions and return that.\nIntegration Tests Revisited Let\u0026rsquo;s try running our integration tests again. Now that we have the converters in place, we should be good to go, right? No. Unfortunately our tests will fail for the very same reason again.\nWhy is that?\nWhen running with @DataMongoTest, it does not scan for the ConverterConfig. That is why we need to include this in the Spring context in this test. This is easily done by adding @Import(ConverterConfig.class).\nOur test should finally look like this and pass\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import org.javamoney.moneta.FastMoney; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.autoconfigure.data.mongo.DataMongoTest; import org.springframework.context.annotation.Import; import java.util.UUID; import static org.assertj.core.api.Assertions.assertThat; @DataMongoTest @Import(ConverterConfig.class) public class ItemRepositoryTest { @Autowired ItemRepository itemRepository; @Test public void shouldStoreItem() { Item item = new Item(UUID.randomUUID(), FastMoney.of(1, \u0026#34;EUR\u0026#34;), \u0026#34;Test Item\u0026#34;); itemRepository.save(item); assertThat(storedItem.getId()).isEqualTo(item.getId()); assertThat(storedItem.getPrice()).isEqualTo(item.getPrice()); assertThat(storedItem.getName()).isEqualTo(item.getName()); } } Congratulations, you successfully wired Spring with MongoDB.\n","date":"2021-05-26T22:00:00Z","permalink":"https://revontulet.dev/p/2021-spring-boot-with-mongodb/","title":"Spring Boot with MongoDB"},{"content":"Foreword Since many years I am using JDownloader as the solution to download things on all my computers. The problem with it is though that the files are not synchronized between them. Thus I cannot access what I downloaded from any machine different from the original downloader. This problem worsens even with the fact that once I am not at home, I cannot access the harddrives the multimedia is on.\nWhat bugs me about JDownloader though is that there is no CLI application nor a WebUI, unless you want to rely on their external MyJDownloader service. But I wanted to find a way that enables me to run JDownloader on a headless RaspberryPi. The JDownloader should be accessible via browser, yet selfhosted.\nPrerequisites I only tested this on a RaspberryPi 4 with 8 GB of RAM running Ubuntu Server 21.04. It could be that the Raspbery Pi 3 is too weak for this, but feel free to give it a try. Therefore I assume you are running on a Debian based distribution. All not-containerized programs we need to build the multimedia station is Docker and ZeroTier.\nInstallation of Prerequisites To build this setup, we are using\nDocker ZeroTier MisterDerpie/jdownloader-docker Docker-Apache/httpd. We need to install Docker and ZeroTier (the latter needs to be set up) before starting, the other two components will be touched later on.\nDocker Installing Docker on the RaspberryPi is straightforward.\n1 curl https://get.docker.com/ | sh After the installation, you need to be add your user to the docker group.\n1 sudo usermod -aG docker $USER Once this is done, reboot or logout and login. Verify that the installation was successful by running hello-world.\n1 docker container run --rm hello-world ZeroTier This section describes how to setup ZeroTier on the RaspberryPi. This is solely for accessing it from anywhere. In case you don\u0026rsquo;t want to access the Pi from outside the same network which it is running in, you can skip this section. The IP address will then be your Pi\u0026rsquo;s local IP address.\nAs straightforward as it is to install Docker, the same it is for ZeroTier. Yet we need more steps to do it.\nCreate ZeroTier Account Go to ZeroTier.com and click on Sign Up to create an account. It is free and you don\u0026rsquo;t need any premium plan.\nCreate a ZeroTier Network On the same page, go to Log In. After logging in, you should be redirected to my.zerotier.com. Click on Create Network.\nYou should see a table with the new created network.\nClick anywhere on the row to go to the network configuration. The network can be named as you want. Important: Ensure that it is marked as private.\nThis will prevent any other user who may guess your network ID from being able to access it directly. Scroll down to the Advanced section and select any of the auto-assign IP addresses from the range in the lowest row. I selected 192.168.195.*, so this tutorial will continue assuming you\u0026rsquo;re in the same range.\nIf you scroll further to the Members section, you should see that no devices have joined this network yet.\nJoin ZeroTier Network with Raspberry Pi To join a network, we need the network ID. This can be found in the yellow box stating No devices have joined in a way of join \u0026lt;NetworkId\u0026gt;. You can join via zerotier-cli command. My networkid is a84ac5c10a0a436f, so I run the following.\n1 sudo zerotier-cli join a84ac5c10a0a436f When the command ran successful, the console output should be 200 join OK. After some seconds you should be able to see the first member in the Members box.\nTo get it assigned an IP address in the virtual ZeroTier network, we need to check the Auth? box. Unless you check this, the device will not be part of this network and cannot communicate with others. I also recommend you to give it a name, for example RaspberryPi.\nWe are done. ZeroTier is set up and installed on our RaspberryPi. As the yellow box warns us, there should be at least two devices. Of course, the other devices should be the hostmachines we want to access the RPi from.\nJoin ZeroTier Network from another machine Linux In case your host machine is a Debian based Linux machine, you can install ZeroTier the very same as done for the RaspberryPi. The only step you need to do after installation is joining the previously created network.\nWindows On Windows, download the ZeroTier One.msi installer and install it. Start ZeroTier from the program menu. Then you should see the icon in the tray (where the speaker symbol is in the taskbar).\nIn case you don\u0026rsquo;t see it, click on the arrow next to the speaker. Then click on the icon and click Join Network ....\nEnter the network\u0026rsquo;s id and click Join.\nAfter a short time Windows should ask you whether you want your computer to be discoverable. Select Yes.\nFinal Step Once you have joined the network from your host machine, don\u0026rsquo;t forget to go back to the webinterface and authorize your device. Again, I recommend you to also name it. See the screenshots from Join ZeroTier Network with RaspberryPi how to authorize and name it.\nJDownloader-Docker The JDownloader-Docker image enables you to run JDownloader and access the UI via any browser. This image has minor JDownloader preconfiguration. If you are interested in the details, check the repository. To do so, we first need to clone the repo and then build the image.\n1 git clone https://github.com/MisterDerpie/jdownloader-docker Go into the root of the directory. From there, build the image named jd2-base.\n1 docker build -t jd2-base . This process will take some time. Once it\u0026rsquo;s done, congratulations: You are now having a JDownloader base image. We will not extend the image but use it to run our container.\nCreate Download Directory Create a folder in your home directory that is called jdownloader-root.\n1 mkdir ~/jdownloader-root/ Go in that directory and create 2 directories, Downloads and Extracted.\n1 2 mkdir Downloads mkdir Extracted Start JDownloader The webinterface to JDownloader is accessible on port 8080. We can link it to any port on the Pi, yet we will use the same port. Make sure you are in the jdownloader-root directory and the directories Downloads and Extracted exist. Then, let\u0026rsquo;s start our JDownloader container.\n1 2 3 4 5 6 docker container run -d \\ -p 8080:8080 \\ -v \u0026#34;$(pwd)/Downloads\u0026#34;:/Downloads \\ -v \u0026#34;$(pwd)/Extracted\u0026#34;:/Extracted \\ --name jdownloader \\ jd2-base This may also take some time. Once it\u0026rsquo;s done you should see the ID of the container being printed.\nWhat does this do? We start the JDownloader container with the name jdownloader and link it\u0026rsquo;s Downloads and Extracted directory to the ones in ~/jdownloader-root/. As I mentioned, the image is preconfigured, so everything that is downloaded will land in Downloads. In case it\u0026rsquo;s an archive, it will be extracted to Extracted and the archives will be deleted.\nNote: This container is not configured to startup on boot again. You\u0026rsquo;d have to run docker container start jdownloader after a reboot.\nConnect to JDownloader This section only works if you set up ZeroTier. Go to the ZeroTier Webinterface and find out the IP Address of the RaspberryPi.\nAs you can see, the IP of my RaspberryPi is 192.168.195.230. To connect to JDownloader, open \u0026lt;yourip\u0026gt;:8080/vnc.html in your browser. So in my case, I go to 192.168.195.230:8080/vnc.html. Click Connect, and there it is. We see JDownloader running and open in the browser.\nApache Webserver for File Access We are almost good to go. Now JDownloader downloads and extracts files for us. We can also access them by accessing the Pi, but how can we access them from e.g. VLC?\nI present you a very basic and rudimentary way of providing them via Docker-Apache/httpd. You will be able to access the files via browser by simply putting in the Pi\u0026rsquo;s IP address.\nNote: This only works when you store all files in either Downloads or Extracted directories. In case you followed this guide, this applies.\nCreate index.html To be presented with a link to your JDownloader WebUI, as well as the Downloads and Extracted folder selection on the landing page, go to ~/jdownloader-root/. There create a file called index.html and add the following content.\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Server\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;a href=\u0026#34;\u0026#34; id=\u0026#34;jdownloader\u0026#34;\u0026gt;JDownloader\u0026lt;/a\u0026gt;\u0026lt;br/\u0026gt; \u0026lt;a href=\u0026#34;./Downloads/\u0026#34;\u0026gt;Downloads\u0026lt;/a\u0026gt;\u0026lt;br/\u0026gt; \u0026lt;a href=\u0026#34;./Extracted/\u0026#34;\u0026gt;Extracted\u0026lt;/a\u0026gt; \u0026lt;script\u0026gt; document.getElementById(\u0026#39;jdownloader\u0026#39;).href = window.location.protocol + \u0026#34;//\u0026#34; + window.location.hostname + \u0026#34;:8080/vnc.html\u0026#34;; \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; Start the Webserver Go into the ~/jdownloader-root/ directory. From there, start the webserver.\n1 2 3 docker container run -d --name webserver \\ -p 80:80 \\ -v \u0026#34;$(pwd)\u0026#34;:/usr/local/apache2/htdocs/ httpd It starts on port 80 (the default port) containing the entire directory. When you go to your browser and navigate to your Pi\u0026rsquo;s IP address (in my case 192.168.195.230) you will be presented the landing page. There you can select whether you want to browse the Downloads or Extracted directory. Moreover can you also go to JDownloader directly from the landing page.\nClicking on e.g. Downloads gives you an index of the files/folders contained in it.\nBottom Line That\u0026rsquo;s already it ðŸŽ‰. You\u0026rsquo;re now running a fully remote accessible Multimedia Station on your Raspberry Pi. As mentioned, all the containers are not configured to start on the Pi\u0026rsquo;s startup. So once you reboot your RaspberryPi, you would have to run the following.\n1 2 docker container start jdownloader docker container start webserver There are several guides available online on how to make a container start on boot. This can for example be achieved by executing docker run with the --restart=always parameter or making it a service in systemd.\nAdditional Content We have JDownloader downloading and extracting, can access it from anywhere and also access the files via browser. Yet two topics should be visited to make it fully worth. The first is how we get containers \u0026amp; links into JDownloader. Second topic is about how to stream downloaded content in VLC.\nAdd DLCs or Links to JDownloader To add links, you can use the clipboard from the sidebar. Click on the sidebar icon.\nNext, open the cliboard.\nThen paste your links in the clipboard. Make a right-click somewhere in the LinkGrabber field. Select Paste Links and the linkgrabber should start gathering them.\nTo get DLCs to JDownloader, you can simply put them in the Downloads or Extracted directory. Do so by e.g. transfer them to the Pi first (e.g. via ssh) and then open them via File -\u0026gt; Load Linkcontainer. The docker image additionally offers the volume /containers where you can place them in, but I didn\u0026rsquo;t use it in this post. This could be used in addition to running for instance a FileZilla container for that directory.\nStream to VLC Media Player As many people use JDownloader to download multimedia files, it would be good having acces to them in VLC player. Open the webui. From there navigate to the file you want to stream. Make a right click on the file and copy it\u0026rsquo;s link address.\nThen go to VLC and click on File -\u0026gt; Open Network Stream ....\nPaste the link there and click on Play.\nVLC should now start playing your selected video.\n","date":"2021-05-02T17:00:00Z","permalink":"https://revontulet.dev/p/2021-raspberry-pi-four-remote-multimedia-station/","title":"RaspberryPi 4 as a remote Multimedia Station"},{"content":"Foreword This blog is created using Jekyll.\nThe striked through sentence above used to be true for the old version of this blog, hosted on [misterderpie.com]. However, the version at [revontulet.dev] uses GitHub pages and Hugo.\nOriginal Post I really like Jekyll, as I don\u0026rsquo;t like frontend development. One problem is though, that I currently only have Jekyll installed on one single machine. So when I\u0026rsquo;m not at home but want to add a blog post, I would have to install Jekyll on the machine I\u0026rsquo;m travelling with. Luckily there is Docker and the envygeeks/jekyll-docker Jekyll Image. As I\u0026rsquo;m adding blogposts very rarely, I do not need to have a full build and delivery pipeline for it (despite the fact that at the time of writing I wouldn\u0026rsquo;t even be able to create such). A simple container to build the latest version of the site suits my needs.\nThe reason I made a small script for that is the following.\nObviously I\u0026rsquo;m lazy, and want to have an out-of-the-box command for every machine. Creating the container with docker run --rm will always create a new container installing all dependencies. This process can take a while. Persisting the container after first creation enables to rerun it after adding a new post. No need to install dependencies again. The script is nothing special, but I thought it\u0026rsquo;s been a while since I added a post. Maybe you can find some use in it.\nSimply create an executable script in your GitHub Repo and put the following content in it (I named it build.sh):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/bin/bash BUILDER_NAME=jekyll_builder JEKYLL_VERSION=3.8 if [ ! \u0026#34;$(docker ps -a -q -f name=$BUILDER_NAME)\u0026#34; ]; then docker container run \\ --name $BUILDER_NAME \\ --env JEKYLL_ENV=production \\ --volume=\u0026#34;$(pwd)\u0026#34;:/srv/jekyll \\ --entrypoint \u0026#34;/bin/bash\u0026#34; \\ jekyll/jekyll:$JEKYLL_VERSION \\ jekyll build else docker container start -i $BUILDER_NAME fi When then running ./build.sh on first startup it will create a container jekyll_builder. Whenever you run this again after adding a blog post (or removing one), this will build the latest version of your site. The output is in _site/ directory (relative to your repositories root), so the same as without using Docker at all.\nOne downside to this approach is that as soon as you move the repository on your machine, this would not work as expected anymore. To make it work again you could simply remove the container docker container rm jekyll_builder.\n","date":"2021-04-24T20:00:00Z","permalink":"https://revontulet.dev/p/2021-create-blog-using-jekyll-and-docker/","title":"Create a blog using Jekyll and Docker"},{"content":"In my previous post, I showed how to enable the Raspberry Pi 4 on Ubuntu Server to read out DHT11/DHT22 sensor data. But this is a very hacky solution. Moreover is the Raspberry Pi cluttered with some stuff, that we may need to configure differently for another setup or remove as a whole. It\u0026rsquo;s a nice coincidence that I\u0026rsquo;m currently reading Docker in Action, Second Edition (Manning, 2019), so I wanted to build a docker image to read out the data. There are some images available for this already, but I still wanted to build my own light-weight image.\nEnough foreword, let\u0026rsquo;s jump right in.\nPrerequesites Raspberry Pi 4 with Docker installed (curl https://get.docker.com/ | sh) DHT11/22 connected with your RapsberryPi (find any guide online for wiring) Build Base Image Interactively in a Container This section is to interactively build the base docker image. If you want to get a Dockerfile right away, see the next section.\nStart Container As base we use python:3.7-alpine, as it is a comparatively very small image with only 41 MB size. We start the alpine shell /bin/ash and to attach us to the container we use the -it parameter. The --name dht parameter is to find the container by a predefined name.\n1 docker container run -it --name dht python:3.7-alpine /bin/ash The shell should now look like below. We\u0026rsquo;re now running as a root user in the container.\n1 / # Install Build Dependencies Build Tools To build all dependencies we need to be able to build the python lib RPi.GPIO.\nRun\n1 2 apk update \u0026amp;\u0026amp; apk upgrade apk add g++ python3-dev to install the build tools.\nRPi.GPIO It cost me some headache and searching around how I get this to build, as I always got errors that gcc exited with status code 1. Luckily I stumbled upon a post in https://archlinuxarm.org/forum/viewtopic.php?p=64598#p64598 by user peiyangxie who gave a one liner how to do this. So let\u0026rsquo;s do it, run\n1 CFLAGS=\u0026#34;-fcommon\u0026#34; pip3 install RPi.GPIO in the container. This will install RPi.GPIO.\nadafruit-circuitpython-dht Last but not least we need to install the library adafruit-circuitpython-dht. Do this by running\n1 pip3 install adafruit-circuitpython-dht Clean up As we do not need g++ and python3-dev anymore, we should remove them to shrink container size.\n1 apk del g++ python3-dev Then exit the container and return to the host shell.\nBuild Image from Container It remains to build the image from the container. This is easily done by\n1 docker container commit dht dht-image and then remove the container\n1 docker container rm dht Via Dockerfile To build dht-image automatically, just create a file called Dockerfile and put below input in it.\n1 2 3 4 5 6 7 8 9 FROM python:3.7-alpine RUN apk update \u0026amp;\u0026amp; apk upgrade RUN apk add g++ python3-dev ## See https://archlinuxarm.org/forum/viewtopic.php?p=64598#p64598 RUN CFLAGS=\u0026#34;-fcommon\u0026#34; pip3 install RPi.GPIO RUN pip3 install adafruit-circuitpython-dht RUN apk del g++ python3-dev ENTRYPOINT [\u0026#34;/bin/ash\u0026#34;] Navigate to the place the Dockerfile is located and run\n1 docker image build -t dht-image . to build the image.\nRead DHT with dht-image If you completed the aforementioned section you should have an image called dht-image. Let\u0026rsquo;s verify this.\n1 docker images It should print out a list of images, which contains dht-image (most likely at the top).\nIn the previous post I shared a script dht.py which we will first download into the container and then change the pin/sensor according to our needs. Finally we run the script and should get results from our sensor.\nNote\nDocker is not able to access the GPIO pins unless we give it acces. There is a really good, short and comprehensive way about the best options we have on stackoverflow.com/a/48234752. We will use the second option and add /dev/gpiomem as a device to our container, as it is the easiest and most sensible option for our needs.\nStart Container We start the container with /dev/gpiomem as a device and run /bin/ash in it.\n1 docker container run --device /dev/gpiomem -it dht-image /bin/ash Download dht script Download the dht.py script via\n1 wget https://revontulet.dev/p/2021-raspberry-pi-four-dht/dht.py Adjust dht script to wiring The line we need to change is line 8. If you are familiar with Vi you could easily do that, but in case you are not this is also achievable with a single sed command (Source: stackoverflow.com). Run the following command with\n{X} - 11 or 22 (DHT version) {Y} - Your GPIO Pin 1 sed -i \u0026#39;8s/.*/dhtDevice = adafruit_dht.DHT{X}(board.D{Y})/\u0026#39; dht.py so for instance, I have a DHT11 and it is connected to Pin 4, so I would run\n1 sed -i \u0026#39;8s/.*/dhtDevice = adafruit_dht.DHT11(board.D4)/\u0026#39; dht.py Run dht script This is the easiest step. Just execute it via\n1 python3 dht.py and you should start seeing output.\nI do not know why, but this is failing a lot more often than running directly on the Pi. So you most likely see something like this\n1 2 3 4 5 6 7 8 9 / # python3 dht.py Checksum did not validate. Try again. Checksum did not validate. Try again. Temp: 69.8 F / 21.0 C Humidity: 20% Checksum did not validate. Try again. A full buffer was not returned. Try again. Temp: 69.8 F / 21.0 C Humidity: 20% Checksum did not validate. Try again. Checksum did not validate. Try again. Anyway, here we are, having a working docker image in place that returns (most of the times) the results from the sensor.\n","date":"2021-03-31T22:00:00Z","permalink":"https://revontulet.dev/p/2021-raspberry-pi-four-dht-docker/","title":"Read Temperature \u0026 Humidity with RaspberryPi 4 - Docker Setup"},{"content":"I recently bought a RaspberryPi 4 and installed Ubuntu Server 20.04 64 Bit on it. There are many tutorials available how to read the temperature/humidity via Python, unfortunately none of them worked out of the box. It seems to be that they are\neither for Raspbian OS or for 32 Bit OSes. After some searching I found a way from several sources to make it run. This is a horrible hacky workaround and I\u0026rsquo;d be very happy to see a better way and/or out-of-the-box solution.\nPrerequisites Ubuntu Server 20.04.2 LTS Python 3 (preinstalled in aforementioned Ubuntu image) DHT11/DHT22 with 4 Pins connected to the Pi (follow this guide to see the wiring) dht.py (Source code from learn.adafruit.com) Installation 1. Install Python 3 Pip We will use adafruit-circuitpython-dht to read out the sensor and install it via pip. Run below command from the shell.\n1 sudo apt install python3-pip 2. Install RPi.GPIO via apt-get Usually this is a dependency to adafruit-circuitpython-dht and thus could be installed with it. Unfortunately it does not succeed installing the dependency, so we need to install it ourself. For more information see this thread on askubuntu.com. Run below command from the shell.\n1 sudo apt-get install RPi.GPIO 3. Install adafruit-circuitpython-dht 1 sudo pip3 install adafruit-circuitpython-dht 4. Install libgiod This step is a bit ugly. Basically the default installation did not succeed for me, and I ended up finding that this solution on GitHub worked yet slightly modified.\n4.1. Install and build libgpiod 1 2 3 4 sudo apt install libgpiod-dev git build-essential git clone https://github.com/adafruit/libgpiod_pulsein.git cd libgpiod_pulsein/src make You should see a new folder named libgpiod_pulsein. Run the script dht.py (linked in the prerequisites) and you should see some similar error:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 Traceback (most recent call last): File \u0026#34;run.py\u0026#34;, line 11, in \u0026lt;module\u0026gt; dhtDevice = adafruit_dht.DHT11(board.D16) File \u0026#34;/usr/local/lib/python3.8/dist-packages/adafruit_dht.py\u0026#34;, line 265, in __init__ super().__init__(True, pin, 18000, use_pulseio) File \u0026#34;/usr/local/lib/python3.8/dist-packages/adafruit_dht.py\u0026#34;, line 56, in __init__ self.pulse_in = PulseIn(self._pin, 81, True) File \u0026#34;/usr/local/lib/python3.8/dist-packages/adafruit_blinka/microcontroller/bcm283x/pulseio/PulseIn.py\u0026#34;, line 67, in __init__ self._process = subprocess.Popen(cmd) File \u0026#34;/usr/lib/python3.8/subprocess.py\u0026#34;, line 854, in __init__ self._execute_child(args, executable, preexec_fn, close_fds, File \u0026#34;/usr/lib/python3.8/subprocess.py\u0026#34;, line 1702, in _execute_child raise child_exception_type(errno_num, err_msg, err_filename) FileNotFoundError: [Errno 2] No such file or directory: \u0026#39;/usr/local/lib/python3.8/dist-packages/adafruit_blinka/microcontroller/bcm283x/pulseio/libgpiod_pulsein\u0026#39; Of particular interest for us is the last line. The file exists, but Python cannot see/access it. Unfortunately granting Python any rights does not solve this problem either. So we need to replace it by the previous build and then grant python correct rights.\nThe path might be different on your computer, so don\u0026rsquo;t blindly copy \u0026amp; paste the next steps but in case it differs use path from your output.\n4.2. Replace libgpiod with build Go into the directory libgpiod_pulsein/src/ from step 4.1. and run the following command from there (important: Replace the path if it differs).\n1 sudo cp libgpiod_pulsein /usr/local/lib/python3.8/dist-packages/adafruit_blinka/microcontroller/bcm283x/pulseio/libgpiod_pulsein 4.3. Set suid for Python If you run dht.py again, it will show you the same error as before. We need to allow any user to run python with the same rights as the owner, which is root. It is done via setuid. This solution was posted on armbian.com. Please note: It is very dangerous to grant these rights to Python. So you should rather copy your Python Binary and change only the copy. See the linked solution for doing so.\nAs I have Python 3.8 on my machine, my Python Binary is found in /usr/bin/python3.8. To enable setuid, run the following command:\n1 sudo chmod 4775 /usr/bin/python3.8 Read Sensor Data Everything set up, so we are ready to read the data. You need to alter the script dht.py in line 8, depending on your GPIO Pin and Sensor:\n1 2 3 4 ## VE - DHT version, e.g. 11 or 22 ## PIN - GPIO Pin the board is connected to, e.g. D4 = GPIO Pin 4 ## VE PIN dhtDevice = adafruit_dht.DHT22(board.D4) It should now start without any errors and print output similar (different values) to that.\n1 2 3 4 5 âžœ ~ python dht.py Temp: 66.2 F / 19.0 C Humidity: 32% Temp: 68.0 F / 20.0 C Humidity: 33% Temp: 68.0 F / 20.0 C Humidity: 33% Temp: 68.0 F / 20.0 C Humidity: 33% ","date":"2021-03-21T11:00:00Z","permalink":"https://revontulet.dev/p/2021-raspberry-pi-four-dht/","title":"Read Temperature \u0026 Humidity with RaspberryPi 4"}]