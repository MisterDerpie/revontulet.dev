[{"content":"To install Arch Linux, the most common and easiest way is booting it up from an installation media. This may not be an option, and instead we want to install it straight onto a drive. The Arch Wiki provides a guide to Install Arch Linux from existing Linux, but it took a while to get all pieces right. Consider this post to aim for a more noob-friendly and handholding setup on how to install Arch from Arch. For readability, only hyperlinks not referring to wiki.archlinux.org are marked with their source in brackets.\nMotivation The first month of 2026 is already over, how time flies! But that also means one thing: Time for new projects! For starters, I needed Arch Linux as a headless distro for a computer that should become a server. Naturally, said machine does not have any peripherals, except a LAN cable that connects it to the router. Therefore, the need to install Arch from Arch was there. I\u0026rsquo;ve installed Arch plenty of times from a bootable media, and when archinstall came around that was even easier. Yet, for the first time, I had to install and configure it non-interactive.\nInstalling Arch Important: All commands here are executed as the superuser. Either run them with sudo or first run su, and then execute them.\nEach section starts with a bullet point, referring to the page in the official Arch Wiki guide, that may be consulted for additional readup.\nSemi-Automated Script The steps from the next parts are all semi-automated and can be downloaded from this post\ninstall-requirements.sh chroot-install.sh. You may want to adjust the parts in chroot-install.sh for your timezone and keyboard layout. Start the install script as superuser (NOT as sudo)\n1 2 3 chmod a+x ./install-requirements.sh chmod a+x ./chroot-install.sh su Once you entered the password, start the installer by\n1 ./install-requirements.sh Prerequisites A host system that runs Arch, a target drive that we install Arch on, and two packages. The first is the arch-install-scripts, which contains pacstrap. Second is dosfstools, used to create FAT32 partitions.\n1 pacman -Sy arch-install-scripts dosfstools We\u0026rsquo;re assuming that the target machine supports UEFI, not BIOS.\nPartitioning and Formatting Partitioning Partition the disks Before we\u0026rsquo;re able to meaningfully interact with our drive, we need to partition it. That is, create separated logical spaces on the underlying drive for where to place data. Essentially, at least two partitions are required.\nThe boot partition, which contains information for the machine to load on start, of type fat32 The system partition, where the whole OS lives, of type ext4 For years I also used to create a Swap partition. It seems that nowadays Swap is really not a thing anymore.\nTo partition, we\u0026rsquo;re using fdisk.\n1 fdisk -l Search for the correct drive in the output (last mention: make sure you ran as superuser, otherwise you see access errors).\n1 2 3 ... Disk /dev/sda: 931.51 GiB, 1000204886016 bytes, 1953525168 sectors Disk model: SN7100 1TB In my case, that is /dev/sda, as I can tell from the disk model. Next, we use fdisk to partiton the disk. Replace the X with the letter of your drive, in my case, that would be a.\n1 fdisk /dev/sdX For brevity, the commands we execute in order are listed in bullet points.\ng - Create a new GPT partition table n - Create a new partition Partition number can be left to default (just hit return) First sector can be left to default (just hit return) Last sector should be set to +1G, which sets \u0026ldquo;Last Sector = First Sector + 1 GB\u0026rdquo; In case fdisk asks about a signature, select \u0026ldquo;Yes\u0026rdquo; to remove it. We once again create a new partition, this time leaving the last sector empty.\nn - Create a new partition Partition number can be left to default (just hit \u0026ldquo;return\u0026rdquo;) First sector can be left to default (just hit \u0026ldquo;return\u0026rdquo;) Last sector leave it empty, the partition will then be the remaining size of the drive. Last but not least, we need to write the partition layout onto the drive. Note: This step is (more or less) irreversible, so double check you got the right drive..\nw - Write the created partition layout and exit fdisk. Formatting Format the Partitions Partitions itself are merely a unit of space separation, and don\u0026rsquo;t provide any additional information about data stored on them. The abstraction to do so are filesystems. For the boot partition, that must be FAT32, whereas for the system partition, we choose ext4.\nMy drive was at /dev/sda, thus my boot partition now is /dev/sda1 and the system partition is /dev/sda2. Replace the X with the letter of your drive, in my case that would be a.\nNote: In case the fat formatting command is not found, run pacman -S dosfstools to install it.\nmkfs.fat -F 32 /dev/sdX1, creates a FAT32 file system on the boot partition mkfs.ext4 /dev/sdX2, creates an ext4 file system on the boot partition We can verify that this works by running fdisk -l again.\n1 2 3 4 5 6 7 8 9 10 11 Disk /dev/sda: 931.51 GiB, 1000204886016 bytes, 1953525168 sectors Disk model: SN7100 1TB Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 33553920 bytes Disklabel type: gpt Disk identifier: ... Device Start End Sectors Size Type /dev/sda1 2048 2099199 2097152 1G Linux filesystem /dev/sda2 2099200 1953523711 1951424512 930.5G Linux filesystem Installing the Kernel and Arch Linux Mount the File Systems Install Essential Packages Mounting the File Systems Before we can proceed, we first need to mount our previously created boot and system partition. Otherwise, there would be no way to write data to it, as the disk is physically there, but the filesystem is not \u0026ldquo;connected\u0026rdquo;.\nWe are using the /mnt/boot directory for the boot partition, and /mnt for the system. Replace the X with the letter of your drive, in my case that would be a.\n1 2 mount /dev/sdX2 /mnt mount --mkdir /dev/sdX1 /mnt/boot Installing Arch and the Linux Kernel Essentially, three packages are needed to get a more or less running system.\nbase - This could simply be called \u0026ldquo;Arch Linux\u0026rdquo;, as in the core programs, configurations and libraries that make up Arch linux - A piece of notorious email threads, the Linux Kernel linux-firmware - Linux Firmware contains the firmware for hardware like the GPU 1 pacstrap -K /mnt base linux linux-firmware Congratulations. At this point, on your partition sdX2, you have a fully functioning Arch Linux. If you were to plug your drive into another machine now, it would not boot up though \u0026hellip; To get closer to a boot, install the Microcode for your processor.\npacstrap -K /mnt intel-ucode, if the target computer has an Intel CPU pacstrap -K /mnt amd-ucode, if the target computer has an AMD CPU Make Arch go Boot Configure the System Creating the fstab Note: In this chapter, we must be truly superuser. Running sudo won\u0026rsquo;t do, so please run su now, if you did not yet.\nThe first thing we need to do is to create an fstab file. Simply put, the fstab file defines how our system is supposed to mount (read: connect) the partitions on our drive.\n1 genfstab -U /mnt \u0026gt;\u0026gt; /mnt/etc/fstab We can inspect the contents of the file by running cat /mnt/etc/fstab - it should contain our entries.\nMinimal System Configuration We need to perform a chroot into the Arch drive. Chroot is, in simple terms, to make the root believe that its entire world is only the new Arch installation, and the host system does not exist.\n1 arch-chroot /mnt Vim, Emacs or Nano are not installed by default in Arch. Having a text editor makes the life a lot easier, and this guide needs one, so install it via pacman. As a Vim user, I install Vim, and the next commands are provided for Vim.\n1 pacman -S vim Setting up time and locale is following precisely the steps from 3.3 Time and 3.4. Localization, so I will not copy them here.\nNext, we set a hostname for the machine. This will be the name shown e.g. on the router.\n1 vim /etc/hostname Last but not least, we should set a root password.\n1 passwd Installing the Booatloader Boot Loader systemd-boot Important: This needs to be in the arch-chroot environment.\nAt this point, the Arch Wiki is our friend to understand what the boot loader is and how it works. In most simple terms, the first software started after/by the UEFI/BIOS is the boot loader. The boot loader then takes care of starting Linux.\nWe are using systemd-boot for no specific reason, except some (very little) familiarity from using it on my machine. It also comes as part of the base package, so we have it at our disposal already. For a server, the EFI boot stub might be a better choice.\nTo set up systemd-boot, run\n1 bootctl install Here we are diverging from the steps in Installing the UEFI Boot Manager. Per my understanding, in /boot/loader/entries, it should contain entries after this command. However, this folder is empty for me. If it is not for you, congratulations, and you can likely skip the setup of the Loader Conf and Entry creation. In case it is, time to artisanal craft the entries. Credits to Use systemd-boot instead of grub in Arch Linux (tsunderechen.io), who\u0026rsquo;s blog post we\u0026rsquo;re following.\nCreating the Loader Conf 1 vim /boot/loader/loader.conf 1 2 default arch # match the filename of the entry timeout 5 # the timeout before loading the entry Creating the Boot Entry Creating the boot entry requires to know the UUID of the partition to load/start Linux from/on. In our case, that would be /dev/sdX2, where X is the letter and 2 is the system partition. We can obtain the partition UUID running\n1 2 3 4 blkid ... /dev/sdX1: UUID=\u0026#34;B1E2-1651\u0026#34; BLOCK_SIZE=\u0026#34;512\u0026#34; TYPE=\u0026#34;vfat\u0026#34; PARTUUID=\u0026#34;137c774b-a0a8-4853-99fc-a40914e49792\u0026#34; /dev/sdX2: UUID=\u0026#34;5d1a6140-013b-4b3d-8ebd-6175ddb5c12f\u0026#34; BLOCK_SIZE=\u0026#34;4096\u0026#34; TYPE=\u0026#34;ext4\u0026#34; PARTUUID=\u0026#34;4104fcf9-e1df-4b7f-b19b-c5b0dd6efef3\u0026#34; After locating our partition, we create an entry arch.conf with our partition UUID.\n1 vim /boot/loader/entries/arch.conf 1 2 3 4 5 title Arch Linux linux /vmlinuz-linux initrd /intel-ucode.img # IMPORTANT: If you use amd, this must be amd-ucode.img initrd /initramfs-linux.img options root=PARTUUID=\u0026lt;PUT-PARTUUID-HERE\u0026gt; rw In my case, the options line should look like this:\n1 options root=PARTUUID=4104fcf9-e1df-4b7f-b19b-c5b0dd6efef3 rw That\u0026rsquo;s it! If we were to plug our drive into another computer now, it would boot!\nMake Arch go Network A server that cannot connect to a network is fairly useless. The last step of this guide is to set up the network connection.\nImportant: This needs to be in the arch-chroot environment.\nNetwork Setup Network Configuration We\u0026rsquo;ve read systemd a couple of times now, and to no one\u0026rsquo;s surprise, the network can also be managed with systemd. Arch Linux ships with systemd-networkd and systemd-resolved. systemd-networkd takes care of getting our network connection over IP sorted, whereas systemd-resolved handles DNS. This guide only covers the Ethernet connection, so please consult the page for systemd-networkd if you require wireless.\nA set of preconfigurations is provided in Arch. To enable the ethernet connection, we can create a symlink, as systemd-networkd adheres to the configs we provide in /etc/systemd/network/.\n1 ln -s /usr/lib/systemd/network/89-ethernet.network.example /etc/systemd/network/89-ethernet.network Configuration done, last but not least we need to start the services on boot. To do so, we enable systemd units using systemctl.\n1 2 systemctl enable systemd-networkd.service systemctl enable systemd-resolved.service \u0026hellip; as this is for a server, it is advised to configure the Wired adatper using a static IP. We can do so by creating a configuration 20-wired.network. In my case, the router\u0026rsquo;s subnetwork is based off of the 24-bit prefix 192.168.8.XYZ. If the current host is in the same network as the server will be, using ip route can tell what the gateway and prefix is. Otherwise, it needs to be configured for the (potentially different) target network.\n1 vim /etc/systemd/network/20-wired.network 1 2 3 4 5 6 7 8 9 [Match] Kind=!* Type=ether [Network] Address=192.168.8.100/24 Gateway=192.168.8.1 DNS=9.9.9.9 DNS=8.8.8.8 Enabling SSH OpenSSH Users and Groups A server unable to connect to serves little to no value. That\u0026rsquo;s where SSH comes to the rescue. Within our chroot, install OpenSSH, and enable the SSH Daemon.\n1 2 pacman -S openssh systemctl enable sshd.service That\u0026rsquo;s already it.\nImportant: This is by no means a secure setup. Do not make your server accessible from the internet without actually knowing what you are doing. This guide covers no security concerns and is solely meant for use in an internal network.\nFinal step: Unless we allow login as root, we need to create a user.\n1 2 useradd -m my-user-name passwd my-user-name Closing Remarks Installing Arch gets easier over time, and is always an interesting exercise with learnings. This was probably the first time I actually read the articles in the Arch Wiki. My sincere recommendation to read them, they offer a great introduction into understanding how Linux works.\n","date":"2026-01-30T10:00:00Z","permalink":"https://revontulet.dev/p/2026-installing-arch-from-arch-onto-an-external-drive/","title":"Installing Arch from Arch onto an External Drive"},{"content":"Every year No Shave November and Movember are held to raise funds and spread awareness for Men\u0026rsquo;s Health, particularly cancer research and treatment. They are more or less the same, but not exactly. Movember vs No Shave November (tue.nl) provides a more detailed view of their differences. The idea is simple: Do not spend your money on a barber for the whole of November, instead donate it to a good cause.\nYou can find more information about either of them at\nmovember.com no-shave.org. Motivation for this Post Year after year, I let my beard grow for the whole November, and at best take one selfie at the end of it, to compare with the previous year. This year though, I took one every day, and wanted to create a GIF showing the progress of my measly beard growth. As the selfies are not taken on the exact same spot nor with the same angle, the face is slightly off on every picture, and furthermore has a different size. To create a GIF where the face is aligned and scaled, I could probably use GIMP, Photoshop or the likes. However, why putting in the effort to manually align 30 pictures, when machines could do the job? After all, we\u0026rsquo;re not working in the software industry to make our lives harder, but easier.\nThe Idea Its a rather simple idea (and there are probably better ideas) on how we can align the pictures. Our face has several reference points. The eyes, the nose, the mouth and ultimately, if we find the outline of our face, we know what to scale.\nAs it turns out, the outline is not all too great, because depending on hair and lighting, this can easily be too skewed. Also, using the mouth risks that if there is a smile in one, but a rather grim look in another picture, the size is incomparable. Maybe the nose could work, however, the eyes turn out to be the perfect reference point(s).\nFirst and foremost, our eyes don\u0026rsquo;t change (too much). The distance between them is always the same in our face. Moreover do they have a perfect center, the pupil, which at worst change light due to lighting, yet that is irrelevant to our use case. As long as you look straight, its position is quite static. Another rather great benefit is that we have a pair of eyes, hence two points to rely on.\nIdea on Scaling Faces to the Same Size Take a look at the picture above. Two different pictures may have the distances x and y between the eyes, respectively. Given that the camera is not static, it is sometimes closer, sometimes farther away, and therefore the distance is not the same in each picture. Scaling the pictures such that the resulting distance is, say 150px, the faces should be more or less the same size in each picture.\nIdea on Aligning Faces A second problem to solve is to align the eyes. Two pictures may have the eyes in a different position, so at some point, we need to move the eyes to be overlapping. As you will see in the implementation, instead of translating the image to move it around, we can achieve this by cropping it. This is a lot easier and can save us some headache.\nImplementation Using Python and OpenCV Given its stance in computer vision, and ease of programming, we use Python. First we have a look into face detection and will see some pitfalls encountered along the way. Then we dive into scaling the picture, and afterwards align it.\nOn a side note: There is a difference between face detection and face recognition. The former, and what we do in the post, is simply to find the position of/presence of faces in pictures, whilst the latter is about identifying the same face across multiple pictures.\nDetecting the Face When you use Google to find out how to detect faces in images, the first hits are OpenCV and the Haar Cascade classifier. That may or may not be the best choice, but certainly the easiest available, so we settle with that. As this post is rather focused on the actual application, you may want to have a read up on how they work in Haar Cascades Explained (medium.com) by Aditya Mittal.\nAssuming we have Python installed, we need to install OpenCV 2. I use Arch, btw, so Python and its packages are managed through pacman, rather than pip.\n1 2 # Arch users sudo pacman -S python-opencv If you\u0026rsquo;re on a system using pip, install the package opencv-python (pip)\n1 2 # pip users pip install opencv-python The first thing we need is our classifier. With the OpenCV repository comes a set of public classifiers, which you can download and use. See GitHub/OpenCV/haarcascades.\nAlthough we want to detect the eyes, we should detect the face first. The explanation for that is rather simple. Our face is a large structure inside a selfie, but the selfie may have many objects that could be interpreted as eyes. We get to see the implication of that in a bit, quite literally. However, it is unlikely that a selfie has multiple objects that look face-alike. So once we detected it, we can reduce the search space for eyes to be only within the face.\nTo detect the face, we are using Haarcascade Frontalface Default (GitHub). Download the file and place it to where your code is located.\nFirst Attempt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import cv2 # Load the face classifier face_cascade = cv2.CascadeClassifier(\u0026#39;haarcascade_frontalface_default.xml\u0026#39;) # Load the image image = cv2.imread(\u0026#34;/path/to/image.png\u0026#34;) # Turn image into grayscale gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # Detect faces faces = face_cascade.detectMultiScale(gray, 1.3, 5) # Print the found face print(faces) First we load the classifier, then we load the image. Those parts should be clear. We then convert the image into grayscale, which is simply to reduce the bit per pixel. Whilst a colored image has Red, Green and Blue represented with 8 bit respectively (=\u0026gt; 24 bit/pixel), a grayscale image may only store 8 bit per pixel, indicating how \u0026ldquo;bright\u0026rdquo; it is. For further explanation, see When to Use Grayscale as a Preprocessing Step (roboflow.com).\nAnother point is that the training data for OpenCV\u0026rsquo;s classifiers may has been in grayscale too. Unfortunately, I could not find the dataset they used, but this guide on Cascade Classifier (opencv.org) uses grayscale images in their examples.\nLast but not least, we run the detection using our Cascade classifier, providing the image, the scale factor and the min neighbors. I am unfortunately in no position to explain these, so for curious readers, please read up on them at e.g. detectMultiScale (opencv.org), How does the parameter scaleFactor in detectMultiScale affect face detection (opencv.org) and OpenCV detectMultiScale() minNeighbors parameter (stackoverflow.com).\nYou may have wondered why the is called faces, and not just a single face. The Cascade Classifier - ideally - detects all faces in a picture. Looking at a selfie with only one face and a wall in the background, it unfortunately finds multiple faces though.\n1 2 3 [[ 610 1155 1465 1465] [ 51 2599 151 151] [ 101 1984 166 166]] Scale Matters Before we go into detail why that is the case, let\u0026rsquo;s add some more code to visualize the found face and draw a rectangle around it.\n1 2 3 4 5 6 7 8 9 for face in faces: # Face is stored as the rectangle [x, y, width, height] x = face[0] y = face[1] w = face[2] h = face[3] # Draw rectangle around it cv2.rectangle(image, (x,y), (x+w, y+h), (0,255,0),2) What\u0026rsquo;s that? There is clearly only one face in the picture, however, we seem to detect three. The simple answer is that the model has been trained on much, much smaller images. My original selfie is 2736x3648 pixels, but oftentimes the training data is (way) less than 500x500 pixels. Therefore, within an image that large, it is not all too surprising to detect multiple faces.\nSecond Attempt There is an easy fix for that: We scale the image down before we search for a face. This should avoid a lot of false-positives, as with smaller images, the amount of details in my towel should vanish, such that only the face will be recognizable as such. Going with 1/10th of the original size suffices to be left with only one face.\n1 image = cv2.resize(image, (0, 0), fx=0.1, fy=0.1) Note: In the final code, you will see that I use 1/3rd of the original picture and only consider the first result of the face detection. At this input size (~1000x1000) it still is not small enough to be precise. However, the multi face detection seems to always return the biggest area first on my machine, and therefore this sufficed my needs. The correct approach would be to scale down to find only one face, and then calculate the respective x, y, width and height in the original image, which is to simply multiply the found values by 1/{scaling factor}.\nDetecting the Eyes Now that we found our face, we continue with detecting the eyes. The code ia almost identical, except now we have to use the classifier for eye detection. If you happen to wear glasses on your pictures, use the haarcascade_frontalface_default.xml (GitHub). Otherwise, go with haarcascade_eye.xml (GitHub).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # Load eye cascade eye_cascade = cv2.CascadeClassifier(\u0026#39;haarcascade_eye_tree_eyeglasses.xml\u0026#39;) # Use x, y, width and height from face as search area face_img = image[y:y+h, x:x+w] # Detect eyes eyes = eye_cascade.detectMultiScale(face_img) print(eyes) # Draw rectangle around the eyes for eye in eyes: eye_x = eye[0] eye_y = eye[1] eye_width = eye[2] eye_height = eye[3] cv2.rectangle( image, (x+eye_x,y+eye_y), (x+eye_x+eye_width, y+eye_y+eye_height), (0,255,0), thickness=5 ) Given we already know the face\u0026rsquo;s area, we need not scan the whole image. Hence we crop it to the original area of the eyes (the rectangle from the previous picture). Again we end up with an array of x, y, width and height.\n1 2 [[ 88 134 85 85] [252 122 103 103]] We need to take into account that these values are relative to the rectangle of the face. That is, x=88 for the first eye is based 88 pixel of the face\u0026rsquo;s x, not off of the image\u0026rsquo;s root. Remember, we cropped our image to reduce the search space, but the classifier does not know that this is part of a bigger image. Thus, to get our real coordinates, we need to offset the face\u0026rsquo;s x and y. The result looks satisfying.\nNote that these rectangles are slightly different in size (roughly 20px, as you can see on the returned array). For our use case, that is perfectly fine, as we\u0026rsquo;re interested in the eyes\u0026rsquo; center only. Even if we were to scale that rectangle 10x, the center point would only change by a margin of error.\nScaling the Face Let\u0026rsquo;s quickly recap this image from the start of the post.\nOur task is to ensure that the distance between the eyes on all output pictures is the same. So if we were to align them one above another, the left and right eye in each picture should perfectly overlap.\nFinding the Center Points This part is fairly simple. We have our rectangle, and its center point is precisely x + 1/2 its width, and y + 1/2 its height. For better visualization, from here on we stop drawing the rectangles around the face and eyes.\n1 2 3 center_x = x + eye_x + (eye_width / 2).astype(int) center_y = y + eye_y + (eye_height / 2).astype(int) cv2.circle(image, center=(center_x, center_y), radius=3, color=(0,0,255), thickness=25) Scaling to a Fixed Distance Although the eyes are not aligned perfectly on the vertical axis, for simplicity we can ignore that, and only focus on scaling along the horizontal axis. To do so, the distance between the two eyes is determined by subtracting the right x from the left x. Note that we use the absolute, as we do not care (for now) about which eye is left or right.\n1 2 3 4 5 6 7 8 # Create a list to append the eye\u0026#39;s center x eyes_center_x = [] for eye in eyes: # ... previous code for detection eyes_center_x.append(center_x) eye_distance = abs(eyes_center_x[0] - eyes_center_x[1]) With my input image (note that it was already scaled that down by 1/3rd), I get 173px to be the distance in this picture. From experimenting around a bit, it turns out that an ideal distance for the output picture is 150px.\nThe math to scale is fairly simple. We\u0026rsquo;re looking for a scaling factor sf such that 173px * sf = 150px. Applying two plus two is four minus one that\u0026rsquo;s three quick maths (knowyourmeme.com), we get\n1 2 # sf = output_distance/actual_eye_distance sf = 150/eye_distance The last missing step is to resize the image by that scaling factor.\n1 image = cv2.resize(image, (0, 0), fx=sf, fy=sf) Aligning the Face Our face should be aligned based on the left eye. For scaling, we did not care which is the left or right eye, for alignment this is important though. Luckily, its fairly easy, if we just find the eye with the leftmost x. Therefore, we introduce a function that given a pair of eyes returns the center of the left eye.\n1 2 3 4 5 6 7 def left_eye_x_y(eyes): if eyes[0][0] \u0026lt; eyes[1][0]: return [(eyes[0][0]+(eyes[0][2]/2)).astype(int), (eyes[0][1]+(eyes[0][3]/2)).astype(int)] else: return [(eyes[1][0]+(eyes[1][2]/2)).astype(int), (eyes[1][1]+(eyes[1][3]/2)).astype(int)] left_eye = left_eye_x_y(eyes) Important: The position of the eyes is from the unscaled image. So is the face\u0026rsquo;s position too. Before we can proceed, we first need to also \u0026ldquo;scale\u0026rdquo; our coordinates for the left eye. This is achieved by multiplying our coordinates with sf.\n1 2 3 4 left_eye = ( ((x+left_eye[0])*sf).astype(int), ((y+left_eye[1])*sf).astype(int), ) To align the face, there are at least to ways. The first is to translate the image, which means to move it along the axes. Image translation with OpenCV is unfortunately not a oneliner of image.move(x, y), see Image Translation (opencv.org). Lucky for us, we can be cheap and just cut off the image at a specific offset. This is equal to moving our input image to a specific coordinate. In OpenCV, images are stored as an array of the form image[height, width]. Let\u0026rsquo;s start by cropping the image to the left eye\u0026rsquo;s center.\n1 image = image[left_eye[1]:, left_eye[0]:] That looks really promising! Taking to pictures, after this step they would be perfectly aligned! However, we obviously would not like to cut off our face, so we need to get a bit more from the left and the top. These values are easiest found by trial \u0026amp; error. For my purposes, using an offset of 300px vertically and 150px horizontally sufficed.\n1 image = image[left_eye[1]-300:, left_eye[0]-150:] We don\u0026rsquo;t need to have a look at the whole shoulder or the largest part of the neck. So let\u0026rsquo;s also crop the image to the bottom and to the right. As before, the values are best determined by trial \u0026amp; error, until it looks good. For my case, 630px in height and 475px in width deemed successful.\n1 2 3 4 image = image[ left_eye[1]-300:(left_eye[1]-300)+630, left_eye[0]-150:(left_eye[0]-150)+475, ] Take a moment to breathe. We\u0026rsquo;re done with all it takes to detect the face, the eyes, scale and align on the left eye, resulting in a nicely cut out image. On to our next task, stitching images together \u0026hellip; and stop drawing the red circle in the eye.\nThe code up until here is available in align_and_cutout.py.\nGenerating the GIF This section can be read independent of the previous section. We will however extend the previous code and add to read the input images, produce the aligned output, and then stitch these output images together.\nTo generate the GIFs, we use ImageIO (pypi.org). As with OpenCV, as an Arch user (btw) I use pacman to install it, but if you\u0026rsquo;re using pip to manage your packages, use pip.\n1 2 # Arch users sudo pacman -S python-imageio 1 2 # pip users pip install opencv-python Processing Input Images For simplicity, we assume that there are two directories. One is input/ and one is output/. All images in input/ are named in a sortable order by their respective date, e.g. 20251101, 20251102, \u0026hellip;\n1 2 3 4 5 6 7 8 9 import os input_path = \u0026#34;input/\u0026#34; output_path = \u0026#34;output/\u0026#34; files = os.listdir(input_path) files.sort() input_files = [(filename, input_path + filename, output_path + filename) for filename in files] Next on, we need to iterate through all files and convert them using our previous code.\n1 2 3 4 5 6 7 8 for filename, fin, fout in input_files: # previously: image = cv2.imread(\u0026#34;image.png\u0026#34;) image = cv2.imread(fin) # . . . find face, eyes, scale, position, crop # previously: cv2.imwrite(\u0026#34;output.png\u0026#34;, image) cv2.imwrite(fout, image) Creating a GIF using ImageIO This is by far the easiest part of this post. We read all frames, append them to a list that the GIF should be generated from \u0026hellip; and generate it.\n1 2 3 4 5 6 7 8 9 10 11 12 import imageio.v2 as imageio frames = [] # Our main loop from the previous step for filename, fin, fout in input_files: # Reading, Processing, ..., and saving cv2.imwrite(fout, image) frames.append(imageio.imread(fout)) imageio.mimsave(\u0026#34;output.gif\u0026#34;, frames, fps=5) The final code can be found in final_result.py\nResult and Closing Remarks Of course, here is a result of 30 days not shaving:\nThe irony of providing this as an MP4 cannot be overlooked. GIFs are way too chunky and would auto load, so I had to embed this as a video \u0026hellip; However, the final result of this code is a GIF, trust me bro\nThis marks the end of my longest post, to date (2025). Given I have zero stance in computer vision or machine learning, it\u0026rsquo;s been a refreshing excursion into a different field.\nOnce again, I\u0026rsquo;d like to point out that more information around the Movember \u0026amp; No-Shave November can be found on\nmovember.com and no-shave.org. ","date":"2025-11-30T08:00:00Z","permalink":"https://revontulet.dev/p/2025-no-shave-november-and-face-detection/","title":"No Shave November Through the Eyes of Face Detection"},{"content":"Since January 2023, I\u0026rsquo;m the owner of a Steam Deck. Until April 2025, my main computer still had Windows running, but I found that I hardly use it for anything but the browser and games. Given the Steam Deck experience, I was confident to ditch Windows and continue playing on Linux from here on. Especially as pretty much anything I play is on Steam. However, in my setup with an Nvidia RTX 3070 and Sway (thus Wayland), heavy ghosting occurred on games like Hogwarts Legacy, and one I started most recently, Arc: Raiders. Overall, these problems seem to happen with games that are built in Unreal Engine, as e.g. RDR2 or GTA V work flawlessly.\nThe TL;DR of this article is pretty much the only thing that helped me resolving it. Given that I did countless searches to find out how that would be resolvable under Sway, this article is more to prevent others wasting hours of their life to try to fix it, if they are content to just install a Desktop Environment alongside Sway, just for gaming.\nIn case anyone ever finds the correct way to get rid of ghosting in Unreal Engine under Sway, please reach out! I would be more than happy to update this piece, and get rid of a DE.\nWhat is Ghosting? This is by no means a formal definition. A lot of people know ghosting from their computer screens, especially some lower-budget IPS screens suffer from it. Simply put, one does not only see the most recent rendered image, but sees images from previous frames too. During a game, when you move, it could look like this (Source: DLSS 2.3: Has Nvidia Fixed Ghosting Issues in Games? (YouTube)):\nInstead of seeing just one car, multiple last positions are still drawn.\nNvidia DLSS Nvidia Deep Learning Super Sampling (DLSS (Wikipedia)) is used in games to achieve two things:\nFPS goes brrrrrt (much like more RGB = more FPS) by rendering at a lower resolution, and then upscaling, with - the claim of - almost no loss of detail, anti-aliasing (Wikipedia), often abbreviated AA. However, the internet is full of people encountering ghosting with DLSS.\nImportant Note: Initially, I thought that DLSS would be my issue. But I want to be very clear that even with AA off in these games (or as off as it could be), the ghosting still occurred.\nSway, Wayland Sway and Nvidia Sway is a window manager under Linux, that is often stated as the successor for i3. Whilst i3 still uses X11, Sway uses Wayland, which itself is considered the successor of X11. In most simple terms, Sway new, i3 old, Wayland new, X11 old. I gave a quick guide on how to install Sway in Arch and i3 - Hello, Sway!.\nNvidia and Sway don\u0026rsquo;t go along all too well. So unwell, that Sway forces you to run it with the a flag \u0026ndash;unsupported-gpu (Arch Wiki). Being a daily user, it\u0026rsquo;s easy to notice small glitches and hiccups here and there, which one would expect when providing this flag. As it does not bother(ed) me (until now), there was never a need to do something about it.\nSway and Multiple Monitors My host device is a laptop, put up on a desk and then connected via a 10m (almost 33 foot) cable to my TV. The laptop screen is not used at all. To turn off the laptop screen after launching, and only render on the TV, I use shikane (GitHub).\n1 .config/shikane/config.toml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [[profile]] name = \u0026#34;tv_setup\u0026#34; [[profile.output]] enable = true search = [\u0026#34;m=LG TV SSCR2\u0026#34;, \u0026#34;s=0x01010101\u0026#34;, \u0026#34;v=LG Electronics\u0026#34;] mode = \u0026#34;2560x1440@119.998Hz\u0026#34; position = \u0026#34;0,0\u0026#34; scale = 1.0 transform = \u0026#34;normal\u0026#34; adaptive_sync = false [[profile.output]] enable = false search = [\u0026#34;m=0x1600\u0026#34;, \u0026#34;s=\u0026#34;, \u0026#34;v=California Institute of Technology\u0026#34;] mode = \u0026#34;2560x1600@165.019Hz\u0026#34; position = \u0026#34;0,0\u0026#34; scale = 1.0 transform = \u0026#34;normal\u0026#34; adaptive_sync = false 1 .config/sway/config 1 2 3 ... exec shikane ... The TV\u0026rsquo;s resolution is 4k, but the RTX 3070 on Lenovo\u0026rsquo;s Legion supports only 2k@120Hz, and anyway, on a 4k screen you can\u0026rsquo;t really see anything without scaling.\n1 swaymsg -t get_outputs 1 2 3 4 5 6 Output DP-1 \u0026#39;LG Electronics LG TV SSCR2 0x01010101\u0026#39; (focused) ... Allow tearing: no Available modes: 3840x2160 @ 60.000 Hz 4096x2160 @ 59.940 Hz My guess is that this has to do with Sway\u0026rsquo;s compositor and tearing being off. It\u0026rsquo;s a wild guess, but somehow Sway and the graphics renderer must compete about who gets to paint the image when. Since DLSS was turned off, and it stil suffered heavy ghosting, this cannot just come from Nvidia. However, given I\u0026rsquo;m using an unsupported GPU, whatcha gonna do?\nThe Fix I tried all kind of things and guides I could find online. Nothing helped, nothing worked, nothing wanted to work. Eventually, the thought struck me to just try out another desktop environment and see if that does the job. A list of officially supported DEs in Arch can be found in Desktop Environments (Arch Wiki).\nCaution! When you install a DE, it\u0026rsquo;ll install a shitload of other tools. This probably has an impact on your Sway experience, e.g. the File Manager may change, or some GTK configs are messed with, or . . . the list goes on. You can always sudo pacman -Rns what you installed, but make sure that your configs are backed up before you proceed.\nInstallation TL;DR:\n1 sudo pacman -S plasma-desktop Note: Install plasma first to get all the tooling for e.g. monitor setup. Boot into KDE, then configure everything. Uninstall KDE then (pacman -Rns plasma) and then install plasma-desktop. Enjoy a preconfigured KDE Plasma, but with a lot less overhead.\nI used KDE Plasma, as it is the same DE that the Steam Deck uses. Besides that, I tried GNOME, Cinnamon, Xfce4 and MATE. Some of them I did not even get to boot without configuring\nNow I can enjoy ghost free gaming. It\u0026rsquo;s too late for Halloween anyways.\n","date":"2025-11-15T10:00:00Z","permalink":"https://revontulet.dev/p/2025-wayland-sway-nvidia-dlss-unreal-engine-and-ghosting/","title":"Wayland, Sway, Nvidia DLSS, Unreal Engine and Ghosting"},{"content":"Here\u0026rsquo;s a short one about a pattern that we can unfortunately too often observe when it comes to mocks. Especially with databases, we see our \u0026ldquo;Repository\u0026rdquo; or \u0026ldquo;Database Connection\u0026rdquo; mocked out in tests. That is, for the sake of unhappy path testing. However, more often than not we also see that these mocks are used for happy path testing.\nThere is a real danger where our mocks are mocking us.\nThe Bad Let\u0026rsquo;s look at some very simple code. We have a service that gets a request and ends up inserting data into a DB, via some repository. Potentially, the code is close to this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // service.go func (s *Service) InsertIntoDb(request Request) (Response, error) { dbQuery, err := buildInsertQuery(request) // if err ! nil ... // The actual insert result, err := s.repository. ExecuteQuery( dbQuery.TableName, dbQuery.Query, ); // if err != nil ... return result, nil } 1 2 3 4 5 6 7 8 9 // repository.go func (r *Repository) ExecuteQuery( query string, tableName string ) (Response, error) { // For the sake of the argument // We take 2 strings as input // ... don\u0026#39;t do that at home! } The mistake may be obvious to you now as a reader, as the code is very small and colocated. In reality, it is hard to spot that the first parameter is expected to be the query string, and the second parameter to be the table name. Especially, if the reviewer of this code does not look at the signature of ExecuteQuery (and ask yourself, how often do you open a library and check that the call you see in a PR is correct?).\nThe Ugly Consider a test like below.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func Test_InsertIntoDb(t *testing.T) { mockRepo := new(MockRepository) service := \u0026amp;Service{repository: mockRepo} req := Request{TableName: \u0026#34;users\u0026#34;, Data: \u0026#34;foo\u0026#34;} expectedResp := Response{OK: true} mockRepo. On(\u0026#34;ExecuteQuery\u0026#34;, \u0026#34;users\u0026#34;, \u0026#34;INSERT {...}\u0026#34;). Return(expectedResp, nil). Once() resp, err := service.InsertIntoDb(req) // NoError, Resp is OK mockRepo.AssertExpectations(t) } While the test looks right on the surface, are we actually testing that our insert works? Nope. At best, we verify that some string was passed as the first parameter, and some other string was passed as the second one, supposedly the table name and the insert query respectively. But looking at the method\u0026rsquo;s signature, the order would be to first pass the query and then pass the table name.\nOh no, anyway!, with this test (and all other mock tests), we hit 100% Code Coverage! Great, our system is fully tested, straight to production!\nThe Good If we want to verify our happy path with integrated technologies, we can oftentimes use the real deal in integration tests (and if we can\u0026rsquo;t, we should really work towards being able to). Pretty much any major technology, from cloud provider over databases to message brokers, is offered as a testcontainer. Using Testcontainers for Integration Tests, or connecting to a real instance and creating ephemeral data, can greatly increase our confidence in the unit under test.\nOn a side note: With Cursor or Claude Code, we\u0026rsquo;re seeing a lot of seemingly correct mock tests. AI code generators love Mocks, and they often enough end up creating tests that claim to test something way beyond capability. Be sure to pay extra attention there.\n","date":"2025-09-12T10:00:00Z","permalink":"https://revontulet.dev/p/2025-dont-let-your-mocks-mock-you/","title":"Don't Let Your Mocks Mock You!"},{"content":"This post has been lying around as an idea for a couple of months now. A list of bullet points on my computer is from somewhere mid-June, and I originally intended to title it \u0026ldquo;The Underappreciation of Fast Feedback Loops\u0026rdquo;. However, calling out a cost sounds catchier and is more accurate, because we do not only underappreciate them, but they cost us. First and foremost, nerves (because who likes to wait for results?), but moreover do slow feedback loops actively reduce delivery speed of teams.\nIn Software Engineering, we easily set targets around metrics and processes which have a constant behavior, regardless of whether we have 50, 300 or 50,000 lines of code. At the same time, we do not pay close attention to those metrics that change as complexity grows, and only realize their impact when the pain is obvious.\nWhy do we accelerate cars to high speed before we turn on the engine? We don\u0026rsquo;t do this. Reading this headline, you might be wondering \u0026ldquo;What on earth are you talking about?\u0026rdquo; Worry not, the post is still about software, we\u0026rsquo;re getting there.\nA time consuming design It\u0026rsquo;s a sunny day in a utopian dystopian world, and you bring your car in for inspection. At the end of the inspection, you see it being towed out of the workshop, straight onto the highway. The towing car accelerates to a speed of 100 km/h (62 mp/h), and then the key in your car is turned and the ignition kicks in. From that point on, the engine is running. Great, they\u0026rsquo;ve verified that the engine works!\nWhy would you do this?! Coming from our real world, you naturally ask \u0026ldquo;Why do you need to accelerate to a high speed, if you want to know whether the engine turns on?\u0026rdquo; Fair question. \u0026ldquo;Can\u0026rsquo;t you just turn the key with the car standing still?\u0026rdquo; Absolutely. \u0026ldquo;Why does it need to be on the highway and at a high speed to verify that the engine starts?\u0026rdquo; Right, why?\nWe keep building our own limitations You might get an idea now where I am heading to. What if, in this fictional world, the car was built in such a way that it must accelerate to a high speed after an inspection, before the engine can be turned on. It is very obvious that this is a an additional cost and a waste of time, as inspections take much longer.\nLet me ask you this: How often have you heard the sentence \u0026ldquo;We need to deploy this to {somewhere} to verify whether it works\u0026rdquo;? Do you think that deploying your application somewhere is really necessary, to see if your change behaves as expected? Now is the time to ask yourself: Why don\u0026rsquo;t we test this locally? And I am quite sure, you have heard all kinds of excuses around complexity and dependencies. You won\u0026rsquo;t need a highway and a car being towed to high speed to turn on the engine, and your application should not need a fully-fledged environment to verify its behavior.\nThe Development Feedback Loop During the development of a feature, a simplified feedback loop until done looks as follows:\nChange Something Verify - If it does not yet behave as expected, back to 1 Done. Once the Verify step actually yields your expected results, be that automated or manual, you move on and consider it done.\nTheoretical Ideal It is important to understand the implication of where the feedback can be gathered, and how much time each approach takes. In an ideal world, it should look like above.\nNormally, being able to gather feedback locally is the fastest. An engineer only needs to rebuild their feature, maybe have a set of dependencies running on the side, and thus is entirely independent on other systems to verify correctness. If the feedback turns out to not be what they expect, a rebuild happens seemingly fast.\nSecond in line is a staging (also testing) environment. Usually, within this environment all dependencies are present. However, it is not that simple as just recompiling the feature and testing it, given this is not on our machine anymore. To test changes in the staging environment, that change must somehow get there. The actual steps involved depend on the environment, yet due to its nature of needing more steps, the time until feedback can be collected increases.\nLast but not least, what if it can only be verified in the actual production environment that hits real traffic? Regardless of whether it\u0026rsquo;s dangerous to test there or not, it tends to take the most time to get the change out there. Often, one also has to be precautious and apply additional measures, for example the use Feature Flags, to make sure not to impair existing flows.\nIf you\u0026rsquo;re manufacturing an engine, after its assembly, you do not want (nor need) to build a car around it to verify that the engine works.\nFeedback Loops in Practice From working in several teams across various companies, and talking with peers in the field, the reality actually more often than not looks like this.\nIn the first graph, the time to gather feedback on the local machine is infinite. This should depict the absence of a local environment, so there is no way to verify changes locally. The second graph indicates that gathering feedback locally is more time consuming than gathering it in staging.\nTeams tend to either not have a setup to have a (reliable) local feedback loop, or if they do, it is slower (maybe more fragile) than deploying their application to a different environment. This is often excused with having too many dependencies to do this. Whilst there are certainly dependencies that we may not easily replicate locally, we need to keep in mind that at the beginning of every software project, the amount of dependencies is zero. Our projects do not get in a hard-to-locally-verify state out of nowhere, and the fact of \u0026ldquo;too many dependencies to resolve\u0026rdquo; comes from not having resolved one-by-one at the time of their introduction.\nA Self-Inflicted Problem One may wonder now, if testing locally involves the least hoops to jump, how do projects end up in a state where testing in a different environment ends up being faster? This is the software equivalent of the Boiling Frog.\nThe boiling frog is an apologue describing a frog being slowly boiled alive. The premise is that if a frog is put suddenly into boiling water, it will jump out, but if the frog is put in tepid water which is then brought to a boil slowly, it will not perceive the danger and will be cooked to death.\nâ€• Wikipedia When we join a new team, we immediately notice the things that do not work very well. Especially if we come from teams where we took those things for granted. Right here, that is the degradation of the feedback loop. With every dependency pulled in, and every feature that needs to get shipped faster, it is very likely that the feedback time deteriorates a bit. Since we are part of the process, we do not notice it. Much like your relatives told you when growing up \u0026ldquo;You have grown quite a lot since last time we saw you!\u0026rdquo;, although you don\u0026rsquo;t actively notice any difference day-to-day. Most often, teams do not put the effort in keeping local verification easy enough when they add new features. This debt accumulates, and will be paid at some point, but only noticed once it becomes too painful and too much of a drag to keep shipping fast.\nWhere are the targets? If you have seen \u0026ldquo;The Office\u0026rdquo;, Michael Scott screaming \u0026ldquo;Where are the turtles?!\u0026rdquo; might be a scene stuck to your head. Given the boiling frog situation of the feedback loop, I come to wonder: Where are the targets?\nWe tend to track many things in software projects, probably a metric all of us are familiar with is Code Coverage. That is, how many statements of your code are executed during an automated test run. On GitHub, you most likely have encountered at least one repository throughout your life that used CodeCov. For code coverage, we don\u0026rsquo;t lightheartedly accept changes that will lead to a decrease below a certain threshold.\nWhen it comes to delivery speed, Velocity (Wikipedia) and Lead Time (Wikipedia) are terms that pop up. Velocity describes how much time tasks of similar complexity take, whilst lead time denotes the total time taken from requesting a feature to its eventual delivery. At some point, when the feedback loop is so unbearably slow, the team\u0026rsquo;s velocity decreases. And given that the lead time is directly depending on velocity, so will the team end up consuming more time on delivering results.\nWhilst it is accepted for teams to have a target on Code Coverage, and not accept changes that breach a certain threshold, we do not really set up a target of \u0026ldquo;How much time something should take to verify\u0026rdquo;. If we had that target, it would be expected that engineers spend time during their feature development on keeping the feedback loop short and healthy. Sure, this would have a constant impact on the team\u0026rsquo;s velocity for any task, however, it avoids teams ever getting into a situation that is so bad that it is hard to recover. Think about a code base with 100,000 LoC, without any tests. It is unlikely that anyone will ever sufficiently retrofit any sort of automated testing into that code base, without bringing the team\u0026rsquo;s feature delivery to a grinding halt \u0026hellip; for months to come. Strangely enough, it is widely accepted across the industry to take slightly more time by adding a set of automated tests for each task.\nThe Hidden Cost of Slow Feedback Loops Delivering with n-1 Engineers To support my point and underline the headline, let\u0026rsquo;s look at numbers. Whilst I have claimed that slower feedback loops are counterproductive, it still remains to show what the hidden cost is. Note that this is a constructed scenario, and on a one-team level this may not reflect reality well enough. However, taken a large enough organization, the results converge towards the outlined example.\nAssuming we are having a team of 8 engineers and a feedback loop where locally testing is the fastest, that is, the ideal situation. Over the course of the year, the team adds new features but neglects to keep local verifiability fast. Therfore, they start deploying their changes to a staging environment for any verification. Compared to a year ago, an engineer now needs to spend 10 extra minutes per verification attempt. Say each engineer needs to do that 6 times per day. Let\u0026rsquo;s do the math:\n$$ \\begin{aligned} \u0026= 8 \\,\\text{Engineers} \\times 10 \\,\\frac{\\text{Minutes}}{\\text{Run}} \\times 6 \\,\\frac{\\text{Runs}}{\\text{Engineer/Day}} \\\\ \u0026= 480 \\,\\frac{\\text{Minutes}}{\\text{Day}} \\\\ \u0026= 8 \\,\\frac{\\text{Hours}}{\\text{Day}} \\\\ \u0026= 5 \\,\\frac{\\text{Days}}{\\text{Week}} \\end{aligned} $$The absence of a fast feedback loop results in the team losing the capacity of a full time engineer. That team manages to get the work of 7 done with the capacity of 8. Usually not the \u0026ldquo;Buy 2 get 3\u0026rdquo; kind of deal you would like to have.\nCumulative Debt One may make an argument against that, that if we were to constantly add time for keeping feedback loops fast, we end up with the same velocity. On the surface, that looks to be a zero-sum game.\nIn practice, there is a turning point in which the constant effort put into keeping them fast will be overtaken by the slowdown of never addressing them in the first place. With new projects, this trend takes a long time before it sets in. However, once slow enough, it overtakes the linear growth, and from there on will be net-negative until resolved. Given time usually being accounted for feature delivery already, the time where this gets addressed might be far in the future.\nBottom Line To finish this article, I\u0026rsquo;d like to suggest teams to invest more time into fast feedback loops. As an anecdote, I once joined a product where colleagues needed to deploy software to a staging environment, just to verify that their new GET endpoint, reading a record from a database and returning it, works as expected. After questioning this and being told \u0026ldquo;But it\u0026rsquo;s too hard, there are too many dependencies to do this locally\u0026rdquo;, I spent some time to build the local feedback loop, which was eventually adopted by the whole team. For the sake of numbers, the actual time it took to verify was ~20-30 minutes per attempt before the change made, whilst the local setup took precisely the compile time and issuing a request, so less than a minute.\nIf you find yourself in the situation where seemingly simple tasks like this vary a deployment, stop adding yet another feature and address local verifiability. It\u0026rsquo;s worth it.\n","date":"2025-08-17T10:00:00Z","permalink":"https://revontulet.dev/p/2025-hidden-cost-slow-feedback-loops/","title":"The Hidden Cost of Slow Feedback Loops"},{"content":" The Matrix AI is everywhere. It is all around us. Even now, in this very room. You can see it when you look out your window something up on the internet or when you turn on your television washing machine. You can feel it when you go to work do anything nowadays\u0026hellip; when you go to church a search engine\u0026hellip; when you pay fill out your taxes. It is the world that has been pulled over your eyes to blind you from the truth meaningful content.\nâ€• The Matrix, Almost The Matrix Today\u0026rsquo;s AI world reminds me a lot of The Matrix. For about two years, we have been living in a world where AI has become the new Nocode, the new Bitcoin, the new Web3, the new Metaverse, basically everything. It is hardly possible to not encounter AI on, in, over, under, everywhere around us.\nIf I would like to, I could connect my washing machine to the internet. There is even an AI stamp! Whyever that needs AI, my laundry was clean 5 years ago, and my washing machine is still offline, and shall always be.\nWhy this post? Whilst being on a bus ride across Germany, I was thinking about the latest news in tech, my work, and to no one\u0026rsquo;s surprise: AI. Over the course of the past months, we all have been reading more (or less) on AI. Sales pitches have been made that it is the past, the present, the future, drawing a resemblance to the Matrix: \u0026ldquo;It is all around us.\u0026rdquo; Predicting the future is impossible (even with AI, sorry to break it like this to you), but if you just predict every possible outcome, by definition one scenario turns out to be true. We see wild claims and heavy doubts, and have the full spectrum of predictions.\n\u0026ldquo;AI-first\u0026rdquo; movements are popping up everywhere, but let\u0026rsquo;s just have a look on where we are some of it is, and where some of us are now.\nSuccess Stories? Whilst nobody can tell what AI-first exactly is, it seems to be AI doing work that humans will not be needed for anymore. Sometimes for better, sometimes for worse. Two examples where the \u0026ldquo;all-in\u0026rdquo; resulted in at least \u0026ldquo;some-out\u0026rdquo;\nKlarna:\n2024 - Klarna\u0026rsquo;s AI assistant is doing the job of 700 workers (archive.is) 2025 - Feb - 2/3 of customer service is AI assistants at Klarna (archive.is) 2025 - May - Klarna rehires humans to ensure customers can always talk to humans (archive.is) Duolingo:\n2025 - April - Duolingo will replace contract workers with AI (archive.is) 2025 - May - Duolingo Faces Backlash Over AI Strategy, Pivots to Retract Its Statement (archive.is) At the same time, we hear great success stories from the really big tech companies.\n2024 - Amazon - Amazon CEO Andy Jassy Says Company\u0026rsquo;s AI Assistant Has Saved $260M And 4.5K Developer-Years Of Work (archive.is) 2025 - Google - AI Powers 25% of Googleâ€™s Code (archive.is) Amazon and Google are boasting about their success stories, without providing much data. But luckily, we can find that data ourselves, for instance, on Open Source projects\u0026hellip; or can we? The blog \u0026ldquo;Pivot to AI\u0026rdquo; published the article If AI is so good at coding â€¦ where are the open source contributions?, and the result should make all of us wonder\u0026hellip;\nOftentimes, companies get away making these bold claims without ever proving them. We don\u0026rsquo;t see the methodology Amazon used to come up with 4,500 developer years. Was that even work that was necessary in the first place? Say you have a staple of dirty dishes and a trashbin having only an empty milk carton. You could empty the bin, but that\u0026rsquo;s clearly not needed and the dishes won\u0026rsquo;t be done either. What kind of Code is Google referring to, and how much longer would it take a human to produce it? It should be easy to reproduce their results on FOSS.\nAs with any new technology, it is just sufficient to make a bold claim. Those asking about evidence are met with \u0026ldquo;It\u0026rsquo;s true, but I will not make the effort to prove it to you because you won\u0026rsquo;t believe me anyway.\u0026rdquo; Consequently, no need to give proof, right?\nSide-Note: I am aware that there are LLM benchmarks. Coming from \u0026ldquo;It solved a well-defined undergrad test\u0026rdquo; to AGI or above claims is imho too big of a gap, that has yet to be filled. An article about one of GitHub\u0026rsquo;s actual benchmarks is Does GitHub Copilot Improve Code Quality? Here\u0026rsquo;s How We Lie With Statistics (archive.is) from Jadarma\u0026rsquo;s blog\u0026hellip; which draws a grim picture on the methodology.\nTo our luck, there\u0026rsquo;s been a public experiment. Recently (May 2025), .NET Developers tried out Copilot on their GitHub repository, see PR github/dotnet/runtime#115762. Whilst there are a lot of snarky comments, I think we should respect the attempt. It is important to see where we stand, and see where we can improve, or whether we aren\u0026rsquo;t there (yet?). Have a look at it yourself, it\u0026rsquo;s worth it. In this experiment, AI could unfortunately not live up to the standard of what CEOs are telling shareholders. Instead, the closest resemblance would be a Fiebertraum (German for feverish dreams, basically a nightmare).\nWe are only six months away from AGI! This statement can probably not be attributed to just one single person, as it is mostly used in a mocking way. Ever since ChatGPT, we heard how all kinds of jobs are going to be cut, engineers are put out of work, and prompt-engineering is the future.\nWithout doubt, AI has made it into our toolbelt. Similar to other tools engineers use. For example, opening a documentation to see how to use a technology. Reading a book for a deep dive into inner workings. Optimizing memory layout for making use of Cache Lanes. Opening Google Search to crawl Stackoverflow.\nThe list of tools, technologies and techniques engineers can use are near endless. And AI is yet another one that helps us.\nHowever, looking back at history\nCompilers didn\u0026rsquo;t replace the need for engineers Google didn\u0026rsquo;t make books obsolete No-Code anyone? \u0026hellip; The questionable speedup of AI(-first) Talking with peers in my field, which is distributed systems and backend engineering, we see our CEOs and CTOs moving to \u0026ldquo;AI-first\u0026rdquo; codebases. With AI code editors like Cursor and Windsurf (the latter soon to be Open AI (archive.is)), engineers are able to not just prompt for snippets, but use their whole codebase as the underlying dataset.\nWhen you try Cursor, ChatGPT,\u0026hellip; for the first time, it is very impressive how quickly it achieves a prototype of what you imagine. If you ever attempted to build a GUI by just drawing a rectangle and reacting to the mouse events, extracting coordinates etc., you probably know how much code that needs. Compare that to e.g. HTML, where creating a button in a few lines of code is the way, and executing an action on mouse click is as easy passing a function to a method named like the action itself. Coming from the lower level of things, this will speed up your GUI building 10x, without a doubt. You are now able to achieve even more in a shorter time - but what kind of work? This brings me to the Pareto Principle, which most of us should be familiar with.\nThe Pareto principle (also known as the 80/20 rule, the law of the vital few and the principle of factor sparsity) states that for many outcomes, roughly 80% of consequences come from 20% of causes (the \u0026ldquo;vital few\u0026rdquo;).\nâ€• Joseph M. Juran, Wikipedia Simply put, you spend 20% of your time to achieve 80% of your goals, but the remaining 20% is where you actually spend the vast majority. What has always been hard were the last 20%. To actually bring it over the finish line requires a lot of effort after the \u0026ldquo;quick and dirty\u0026rdquo; part is done - again, this is where we spend most of our time. Frameworks, higher level languages, compilers\u0026hellip; They all have in common to reduce the first 80% of our work, but do not eliminate 80% of our time spent. With LLMs, we seem to witness the same speedup on that relative negligible 20%.\n\u0026ldquo;I am just one prompt away from getting the right results!\u0026rdquo; About two weeks ago, I tried out Cursor myself on a Golang Template file. Said file is already hard for us to maintain and comprehend, so it seemed a fair baseline. Whilst the first suggestion worked (it took Cursor about 30 minutes to think), one small requirement change made it impossible for Cursor to produce a correct result. For a good 12 hours, over the course of 1 1/2 days, I tried to prompt it such that it yields what we needed. Eventually, I noticed that my prompts converged more and more to be almost the code I wanted. After still not getting a working result, I ended up implementing it myself in less than 30 minutes.\nThe more niche a topic is, the worse seem to be the LLM\u0026rsquo;s results. For about 3 months now I am a Neovim user, before I never used neither Vim nor Nvim (adjusting git config to use nano was one of my first steps on a new OS). As other Nvim users know, you end up configuring a lot. Some of these configurations come from plugins, others are native to Vim. Overall, I made the great experience to get myself into \u0026ldquo;I am just one prompt away from getting the right configuration\u0026rdquo;\u0026hellip; eventually, I look at the clock, see that I spent 30 minutes on what seems to be a 2 line configuration, and end up using Google, GitHub and Reddit to find the right answer within seconds. This experience is shared among peers, where AI traps you into the thinking \u0026ldquo;I am only one prompt away\u0026rdquo;, whilst cleary it just does not know the answer. You can try it yourself the next time you are trapped. Prompt the solution back to your favorite coding tool and ask it if this works, or why the solution does/not work. Prepare yourself a tea beforehand, to enjoy the storytelling.\nAnother example stems from Go. Since January I am using Golang, and it seems that Golang adds features that are adopted almost as soon as they are released. One example that found adoption near instant are range loops with integers (gobyexample). We\u0026rsquo;re 9 months past Golang\u0026rsquo;s 1.23 Release, whilst ChatGPT still seem to be unaware of it. Whenever its creating a for loop, you will see the old for i := 0; i \u0026lt; n; i++ syntax. In a PR last week, an AI review bot told me that Go version 1.24 does not exist, although it\u0026rsquo;s been released since February 2025.\nAt work I got to witness a conversation less than two weeks ago, where people were discussing that Cursor on a large-ish codebase does not work well with their rules. It is natural that the bigger the code base the more rules you want to have, to avoid just one more edge-case and live hands-off. Think of it like your code, the more cases you support, the more \u0026ldquo;if\u0026quot;s you end up with, eventually. An engineer pointed out that some rules seem to be ignored as of recent. To their conclusion, the prompt in Cursor is appended to the rules, i.e. the full content of the rules file + the prompt are fed into Cursor when making a request. Every. single. request. Thus, a too large ruleset won\u0026rsquo;t fit into the context window, and your code ends up not looking as expected, given the large amount of rules.\nNo forward-looking statements A forward-looking statement predicts, projects, or uses future events as expectations or possibilities\nâ€• Wikipedia If I recall correctly, the first time I encountered that statement was in the context of shareholders, acquisitions and such. This might be the most accurate statement for the future of A(G)I for coding. It\u0026rsquo;s just perfectly aligned with \u0026ldquo;We can\u0026rsquo;t promise anything\u0026rdquo;, and one should think people won\u0026rsquo;t then, but at the same time any possible kind of claim has been made - excluding those that LLMs make themselves.\nWhilst LLMs and AI these days seem to be truly helpful tools, we should come back to a more nuanced reality. Companies have yet to prove that AI as a mostly autonomous assistant can live up to its promises, but all we see is that it depends. Right now, it depends a lot more towards the side of overpromises, rather than meeting the big expectations.\nMany engineers, myself included, are probably using AI tools regularly in their day-to-day life. Much so like using Google Search, opening a book, reading a documentation or eating breakfast.\nBottom line: We\u0026rsquo;re having quite exciting times ahead of us. It would be great if AI could prove what is proclaimed these days - But I have my doubts. I wonder if we\u0026rsquo;re going to see job roles with descriptions of \u0026ldquo;We need engineers to help us untangle the AI mess we created\u0026rdquo;. If that happens to be the case, I hope that Google Search will be back from the dead.\nWe\u0026rsquo;re just 6 months away from AGI.\nâ€• CEOs these days, whatever date you\u0026#39;re reading this ","date":"2025-05-31T10:00:00Z","permalink":"https://revontulet.dev/p/2025-ai-first/","title":"AI-first - We're just 6 months away from AGI ;-)"},{"content":"This post covers how to tether your iPhone via USB to your RaspberryPi, and share the iPhone\u0026rsquo;s network on eth0. Eventually, the RaspberryPi will be connected to a router via ethernet, making the router use the iPhone\u0026rsquo;s internet connection. A RaspberryPi 4B is used, but this guide should work on other models too.\nOne may wonder why this setup is needed. Routing at least DNS traffic through the Pi with the Pi-hole is a common use-case, but all traffic less so.\nMy connection to the world is through the iPhone\u0026rsquo;s hotspot. Said hotspot turns invisible after a period of time. It is not the classic hidden SSID invisible, but connecting to it once invisible is impossible, only connected devices stay.\nStill, why use a router, when the RaspberryPi comes with a WLAN chip? About a month ago, I attempted a similar setup and had the Pi directly as a WiFi hotspot. Unfortuantely, the throughput was only about 40 Mbit, defeating the purpose of the iPhone\u0026rsquo;s 300 Mbit 5G connection.\nGiven the fact that my RaspberyPi was collecting dust, it was perfectly suited. Moreover does my router at hand not support a USB connection, hence the need.\nInstalling Ubuntu Server To install Ubuntu Server, you may use the RaspberryPi Imager. It can be downloaded from raspberrypi.com/software.\nDuring the installation, it will ask you to provide a custom configuration. Choose yes and provide a username and a password to be able to SSH into it during the next steps. Then install it.\nOnce done, start your RaspberryPi and connect it via ethernet to your router.\nRaspberryPi Internet Setup Find the RaspberryPi\u0026rsquo;s IP Address from your router. Usually, it is listed under connected devices. Then ssh into it, e.g.\n1 ssh name-you-chose@192.168.X.Y SSH probably asks you to trust the fingerprint, and you may select yes. Note: In case you need to factory reset your Pi, and it gets assigned the same IP, you will see an error from ssh. That is due to not matching fingerprints. Remove the entries of that IP from ~/.ssh/known_hosts.\nConnect to the iPhone Install the packages\nusbmuxd libimobiledevice-utils ipeth-utils (this may be optional, I didn\u0026rsquo;t try it without) 1 2 3 4 sudo apt install -y \\ usbmuxd \\ ipheth-utils \\ libimobiledevice-utils Now it is time to connect our iPhone. With Mobile Hotspot enabled, it will show a dialog to Trust/Not trust the RaspberryPi. Trust it, and you may be required to enter your password.\nWe can see that it\u0026rsquo;s connected on the RaspberryPi running\n1 networkctl 1 2 3 4 5 IDX LINK TYPE OPERATIONAL SETUP 1 lo loopback carrier unmanaged 2 eth0 ether routable configured 3 wlan0 wlan off unmanaged 4 enxfae5ce082148 ether off unmanaged Important: Your iPhone will have a different ID, and in the steps from here on you need to replace it in the provided snippets.\nThe iPhone is enx..., and the next step is to have it managed, so it gets an IP assigned and we can route traffic through that interface. At this point, the guide deviates slightly from Arch Wiki\u0026rsquo;s setup using systemd-networkd. Although one may set it up that way, Ubuntu Server ships with netplan, which we take advantage of. That way, we only need to configure netplan.\nThere should already be an entry for eth0, so you may only add the part for the iPhone.\n1 sudo vim /etc/netplan/50-cloud-init.yaml 1 2 3 4 5 6 7 network: version: 2 renderer: networkd ethernets: enxfae5ce082148: // Add this dhcp4: true // Add this // ... keep eth0 unchanged for now Apply the new network config.\n1 netplan apply We can confirm it using the ip command.\n1 ip a 1 2 3 4 5 6 7 8 4: enxfae5ce082148: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether aa:bb:cc:dd:ff:gg brd ff:ff:ff:ff:ff:ff inet 123.12.10.4/28 metric 100 brd 123.12.10.15 scope global dynamic enxfae5ce082148 valid_lft 86357sec preferred_lft 86357sec inet6 1111:1222:333:4444:5555:6666:7777:8888/64 scope global mngtmpaddr noprefixroute valid_lft forever preferred_lft forever inet6 1111::2222:3333:4444:2148/64 scope link proto kernel_ll valid_lft forever preferred_lft forever To test the connection, curl provides the option to set an interface through which to connect.\n1 2 3 curl --interface enxfae5ce082148 \\ https://example.com \\ -w \u0026#39;%{speed_download}\u0026#39; -o /dev/null Great, now you could use the iPhone\u0026rsquo;s USB tethered network to connect to the internet.\nRoute traffic through the iPhone The second part is to tell iptables to route traffic through our iPhone\u0026rsquo;s interface. It is done with adding this entry using the iptables command.\n1 iptables -t nat -A POSTROUTING -o enxfae5ce082148 -j MASQUERADE Note that the newly created entry is not persisted on reboot. To persist it, one solution is to use iptables-persistent. Whilst installing, it asks you to back up existing configurations. Answer \u0026ldquo;Yes\u0026rdquo;.\n1 sudo apt install -y iptables-persistent You will be able to confirm a MASQUERADE entry running\n1 cat /etc/iptables/rules.v4 In case the entry is not present, rerun the steps and the persistence\n1 sudo iptables-save \u0026gt; /etc/iptables/rules.v4 We still need to enable IP forwarding between interfaces. This is achieved adding the following configuration.\n1 sudo vim /etc/sysctl.d/30-ipforward.conf 1 net.ipv4.ip_forward = 1 Last but not least, we must be able to connect on eth0. I did not get into the steps of setting up DHCP on it, so you may do that yourself. After this step, it will be assigned 192.168.42.1.\n1 sudo vim /etc/netplan/50-cloud-init.yaml 1 2 3 4 5 6 7 8 9 10 network: version: 2 renderer: networkd ethernets: enxfae5ce082148: dhcp4: true eth0: dhcp4: false addresses: - 192.168.42.1/24 From here on, for connecting to your RaspberryPi on eth0, you will need to assign the external device (e.g. a Laptop) a correct IP address, the right net mask and the right gateway. Note that the IP address suffix must be a value between 2 and 254.\nGiven our setup, the configuration is\nType Value IP Address 192.168.42.X Gateway 192.168.42.1 Netmask 24 or 255.255.255.0 Reboot the RaspberryPi, and the changes take effect. Keep in mind that you must assign the IP on your external device like above table, otherwise you will not see the RPi.\nThe Pi can be accessed at ssh your-name@192.168.42.1 thereafter.\nUsing the RaspberryPi with a Router The final step of this setup is to use the RaspberryPi as the internet provider for a router. Across most routers, these steps are almost identical, and you may consult your router\u0026rsquo;s documentation on where to find the right setuppage.\nFirst, you need to connect your RaspberryPi to the port of the router that says \u0026ldquo;WAN\u0026rdquo;.\nThen you need to find out where you can set the Conenction Mode/Internet Connection type of your router. Provide the values as below, and your router should be able to connect to the RaspberryPi, and use its network.\nType Value Connection Mode Static IP IP address 192.168.42.77 Subnet Mask 255.255.255.0 Default Gateway 192.168.42.1 Primary DNS server 9.9.9.9 Secondary DNS server 8.8.8.8 The resulting speed with this setup is ~190 Mbit downloading, and 22 Mbit uploading. With a different USB cable, I only achieved 120 Mbit, so the remaining 110 Mbit may be due to the USB cable.\nConnecting to the iPhone directly gives me almost 300 Mbit, so there is definitely a loss of performance somewhere.\n","date":"2025-05-11T08:00:00Z","permalink":"https://revontulet.dev/p/2025-iphone-ethernet-bridge-rpi-4-ubuntu/","title":"iPhone Ethernet bridge on RaspberryPi with Ubuntu Server"},{"content":"Welcome to part one of the three-part series \u0026ldquo;Hello, Sway!\u0026rdquo; The series is split into\nArch \u0026amp; Sway setup using archinstall Sway Configuration Linux QoL Tools Motivation For a while already, there has been no real reason for me to keep Windows. The doubt started about two years ago, in January 2023, when I got myself a Steam Deck. Although the purchase was mostly to emulate console and handheld games, of course the question was \u0026ldquo;But Can It Run Crysis?\u0026rdquo; To my surprise, the games I tried performed incredibly well, almost all of them out of the box. Thanks to ProtonDB, if there was a title that needed some extra configuration, it is usually there.\nBesides gaming, my work is either on a Mac or (used to be) on a Linux machine. When using Linux at work, the default for me was to set up Arch and i3wm (I use Arch btw). Moreover, after being a nano user for quite a while, it was time to move to vim. It\u0026rsquo;s a nice coincidence that Sway uses vim keybinds for moving windows, although one could easily configure that in i3wm.\nGiven the circumstances, let\u0026rsquo;s move to Linux!\nArch \u0026amp; Sway setup using archinstall archinstall is a guided installer to setup Arch. There are many posts on how to create a bootable USB stick with Arch, and how to run archinstall. Thus, instead of providing all the steps here, you may want to follow e.g. this one on debugpoint.com: Installing Arch Linux Using archinstall Automated Script up to the point of installing the Desktop Environment.\nBefore looking into what to select, first comes two common issues, one often faced on laptops and the other may faced when running on an Nvidia card. Scroll past that to Sway, Ly Greeter and Graphics Driver in case you don\u0026rsquo;t encounter any issues.\nIssues During Archinstall Archinstall boots into a Blackscreen on Nvidia It may happen that with Nvidia Graphics Cards, once you select Arch Linux Install from the bootmedium, you only see a blank black screen. In that case, try to add nomodeset.\nSelect the entry on the USB stick you want to boot from, then press e. Navigate your cursor to the end of the line, and append nomodeset. This should resolve the issue. If you\u0026rsquo;re curious what exactly this does, feel free to read this post on ubuntuforums.com: How to set NOMODESET ant other kernel boot options in grub2\nCannot connect to Wifi This issue is common on Laptops. To connect to the WiFi, one may use iwctl (as suggested). Trying to turn on the WiFi could result in operation failed.\nTo fix this, run\n1 rfkill unblock all and then retry.\nFor further reading on why and how, see rfkill caveat (ArchWiki/NetworkConfiguration).\nSway, Graphics Driver and Ly On the desktop environment installation, select the following\nDesktop - Sway This one goes without saying.\nGraphics Driver For the Graphics Driver, it depends on your Graphics Card. As my laptop has an RTX 3070, which is newer than Turing, I chose the \u0026ldquo;Nvidia (open kernel \u0026hellip;)\u0026rdquo;. To find which Nvidia driver you need, please consult ArchWiki/NVIDIA.\nGreeter - Ly My setup uses Ly as the greeter. This greeter takes you to the next level. Check it out on github.com/fairyglade/ly.\nNow, finish up the installation, and boot into your system.\nStart Sway - Nvidia Unsupported GPU As mentioned in the Arch Wiki, to start Sway with Nvidia drivers, one must explicitly state that this is an unspported GPU (ref). Edit the Sway Desktop file\n1 vim /usr/share/wayland-sessions/sway.desktop and change the Exec line to\n1 Exec=sway --unsupported-gpu Logout, select Sway, and start it \u0026hellip; aaand you may see a black screen. At least that happened when I did this, and I don\u0026rsquo;t have an explanation for this, as in, why it is fixed after a reboot. In case that doesn\u0026rsquo;t happen: Great. If it happens though, reboot, and then - at least on my machineâ„¢ - it worked.\nCongratulations. You most likely have an Arch \u0026amp; Sway setup with a fancy login screen now. In the next post, we\u0026rsquo;ll look into some tools to get simple Sway configuration with most important things sorted.\n","date":"2025-03-26T18:00:00Z","permalink":"https://revontulet.dev/p/2025-arch-and-i3-hello-sway-install-arch-and-sway/","title":"Arch and i3 - Hello, Sway! - Install Arch and Sway"},{"content":"Welcome to revontulet.dev! So \u0026hellip; what is this about?\nIntroduction My name is Matthias DÃ¶pmann. I\u0026rsquo;m a Software Engineer, originally from Germany, nowadays located at the Arctic Circle in Finland. There is no specific content for this blog. At the time of writing, I host a couple of articles on MisterDerpie.com, that I intend to migrate over to this blog.\nThere were several reasons I stopped writing on the old blog. The first is, that whilst living in Dublin and working for Amazon/Amazon Web Services, I was fairly occupied by work and rarely found motivation to follow up on these topics besides work. Second, in 2022, the world at Amazon was still remote, so on top of that I spent a lot of time travelling. Moreover, instead of reading computer science/software engineering books, I spent a good chunk studying languages (actual languages, not Programming Languages). These days I\u0026rsquo;m taking courses in Finnish, so I am not sure how much time is left to actually dump content here, but let\u0026rsquo;s see! Ultimately, the non-integration of the old blog with GitHub Actions or the like set the bar higher to publish contentent. Although I dockerized the process of rendering the blog, uploading it to an FTP after the render was still too annoying (\u0026hellip; and yes, whilst I could have automated that, I didn\u0026rsquo;t). Checking out a GitHub repository, write down some thought, and then just hit git commmit and git push is just a lot more convenient.\nGiven the content of my last blog, my work has shifted a bit. Whilst I was working in Java \u0026amp; Kotlin mostly, since 2025, my main language at work is Go (and YAML, loads of YAML).\nAbout Revontulet The reason to call this site \u0026ldquo;Revontulet\u0026rdquo; is that Revontulet (Wikipedia) is the Finnish word for Aurora (Wikipedia). I found that a unique enough name for a website, as well as coupling it close to where I am situated these days.\nThe cover photo is taken in Northern Lapland, Finland, in June 2024. You can find the exact location in the link section at the bottom.\n","date":"2025-03-16T00:00:00Z","image":"https://revontulet.dev/p/2025-hello-world/cover-min_hu_cb48e90546cee70f.png","permalink":"https://revontulet.dev/p/2025-hello-world/","title":"Hello World"},{"content":"Using i3 and Ubuntu native is probably best, but you may not get the choice to do so, in case your working machine is running Windows. I looked through several guides how to set up Ubuntu with i3 on WSL2, but it took me several hours to get the setup I looked for because things did not work as expected. Therefore I\u0026rsquo;m writing this guide.\nInstalling Ubuntu on WSL2 There are plenty of tutorials and videos to do so. I will refer to the very one that I used to set up the bare, GUIless minimum, WSL2 Ubuntu GUI by David Bombai. Reproduce the steps until Install Ubuntu GUI (7:25) to get the headless Ubuntu on WSL2 up and running.\nInstalling i3 and VcXsrv Once you set up your Ubuntu, created a user and are prompted with the shell, update the package index.\n1 sudo apt update Next, install lxdm and i3. We also need to provide the information where Xserver sends its signals to. This is done by specifying the DISPLAY environment variable in .bashrc. The echo command adds this.\n1 2 sudo apt install -y lxdm i3 echo \u0026#34;export DISPLAY=\\$(cat /etc/resolv.conf | grep nameserver | awk \u0026#39;{print \\$2; exit;}\u0026#39;):0.0\u0026#34; \u0026gt;\u0026gt; ~/.bashrc After you setup all this, restart Ubuntu (e.g. run exit and then start it again).\nNext, install VcXsrv. Once installed, in your installation directory (default: C:\\Program Files\\VcXsrv) start xlaunch.exe. Launch it with the parameters provided in the screenshots.\nSelect Fullscreen, then click Next.\nSelect the option Start no client and click Next.\nIn Extra Settings, select Disable access control and click Next.\nYou will be presented with the final setup screen and don\u0026rsquo;t need to do anything here. Click Finish and your screen should turn black, because VcXsrv opened a fullscreen window with no clients attached. Press Alt-Tab to tab out the window, and go back to your Ubuntu terminal.\nIn your Ubuntu terminal, run i3 (i.e. type i3 and press enter) and you should start seeing some log (maybe including error messages). When you now tab back into the black fullscreen, after waiting a short time you should be able to select your i3 config. It doesn\u0026rsquo;t matter whether you run the config setup or not, for some reason it won\u0026rsquo;t be properly persisted. Thus, we need to \u0026ldquo;restore\u0026rdquo; the original config. Exit VcXsrv and go back to the Ubuntu console. If i3 hasn\u0026rsquo;t terminated, press CTRL+C to close it.\nRun the contents of this MisterDerpie/i3-default-config-alt-key gist in your Ubuntu shell. This you can do by copying the content and right-click into the Ubuntu window. It will create the proper i3 configuration needed for your next connection.\nRestart Ubuntu and restart VcXsrv (with the previous parameters). When you now run i3 again, go to VcXsrv you should see everything working.\nExtra: Oh My Zsh As I\u0026rsquo;m a very happy user of Oh My Zsh, and you may use it too, installing and making it the default shell would cause the DISPLAY variable to stop being defined. This is because it\u0026rsquo;s part of .bashrc and not .zshrc.\nAssuming you don\u0026rsquo;t have Oh My Zsh installed yet, below script will do the job.\n1 2 sudo apt install -y git zsh sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; Once installed, issue below command to reenable the proper definition of the DISPLAY environment variable.\n1 echo \u0026#34;export DISPLAY=\\$(cat /etc/resolv.conf | grep nameserver | awk \u0026#39;{print $2; exit;}\u0026#39;):0.0\u0026#34; \u0026gt;\u0026gt; ~/.zshrc ","date":"2021-12-04T19:00:00Z","permalink":"https://revontulet.dev/p/2021-ubuntu-gui-wsl2/","title":"Ubuntu with WSL2 and i3 Window Manager GUI"},{"content":"During my summer vacation, I got myself a Lenovo Legion 5 Pro, powered by an AMD Ryzen 7 5800H and an Nvidia RTX3070. Though this post is not about the legion, I have to say it is an awesome laptop. What this post is about is how to make Ubuntu with i3wm run on the Lenovo Legion.\nInstalling Ubuntu Create Bootable USB Media You need a USB stick as well as a Ubuntu ISO image. Plenty of tutorials exist on the web to create a bootable Ubuntu USB stick. Ubuntu.com provides official documentation, coming from a device running Windows, Ubuntu or Mac OS X.\nBoot USB Media Once you created your boot device, boot up the Legion and press F12 to get into the boot menu. From there, select the USB stick.\nNext you are presented with the GRUB interface, where you have to select Ubuntu (safe graphics) and then press RETURN. After a while (length of the while depends on the speed of your USB stick) you should see a window asking whether you want to Try or Install Ubuntu. Select Install Ubuntu.\nThere is only one part in the installer you need to select a certain option to not run into issues. At some point you should be asked whether you want to have a Normal Installation or Minimal Installation. See this image in the official guide. In the bottom of the window, below Other options, you have to select Install third-party software. This is necessary to install the proprietary Nvidia driver. If you don\u0026rsquo;t, you will most likely be presented a black screen after booting into Ubuntu.\nFinish the installation and reboot into Ubuntu.\ni3 Install i3 Once you booted to your freshly installed Ubuntu, it is very straightforward to install i3. Run this command in your Terminal.\n1 sudo apt install i3 That\u0026rsquo;s already it.\nReplace gdm The next step is to replace GNOME Display Manager, abbreviated gdm. We will replace it with LightDM. It is as easy as installing i3.\n1 sudo apt install lightdm At some point during the installation, a window pops up in your terminal, asking you to select your display manager. In the list, use the arrow keys to navigate to lightdm and hit RETURN to confirm.\nIf you ever want to go back to gdm, you can do so by running below command.\n1 sudo dpkg-reconfigure gdm3 Why replace gdm? This is the part where I ran into a problem that may only be with my setup. When having multiple screens connected (1 over HDMI, 1 over Lenovo Dock), I could login to i3 but then everything was frozen. But it was not Linux that froze, as I could tell from the clock continuing to run. Logging into Gnome worked though. Also, logging into Gnome before logging into i3 made it work. But neither do I want to use Gnome nor do I want to take extra steps every time I boot Linux. So I tried back and forth, and the only way I got this to work is to use lightdm over gdm.\nThe fact that I wasted quite some time on this motivated me sharing a short blog post. Using a different display manager might not suit everyone, but probably for most users this does the job.\n","date":"2021-08-22T15:00:00Z","permalink":"https://revontulet.dev/p/2021-lenovo-legion-ubuntu/","title":"Ubuntu and i3 on Lenovo Legion 5 Pro"},{"content":"In my previous post I stated that I am currently reading Kotlin in Action (2017, Manning). In chapter 9, the authors introduce generics in Kotlin. My overall experience with generics in Java is good, but apparently that book taught me something completely new. The concept of Covariance and Contravariance was something unknown to me. This post, though categorized Kotlin, focuses more on the concepts in general, yet will demonstrate them in Kotlin. I assume the reader has a basic understanding of generics and inheritance.\nInheritance for normal Classes Before we get into generics, let us look into inheritance for normal classes. For that, consider the following class hierarchy.\n1 2 3 abstract class ClothingItem(val price: Float) class Shoe(price: Float, val size: Float) : ClothingItem(price) class Jacket(price: Float, val hasZipper: Boolean) : ClothingItem(price) Between Shoe and Jacket there is no relationship in terms of \u0026ldquo;Shoe is-a Jacket\u0026rdquo; or \u0026ldquo;Jacket is-a Shoe\u0026rdquo;. Thus, whenever we expect a shoe, we cannot treat it as a jacket and vice versa. But between ClothingItem and Shoe/Jacket, there is in fact an is-a relationship. Shoes and jackets are both clothing items. Hence we can always use a Shoe or Jacket in code when we expect a ClothingItem.\n1 2 3 fun printPrice(item: ClothingItem) { println(\u0026#34;The item costs ${item.price}.\u0026#34;) } We can call that with either shoe or jacket.\n1 2 3 4 5 // The item costs 5. printPrice(Shoe(5f, 33.5f)) // The item costs 20. printPrice(Jacket(20f, false)) Covariance To explain how inheritance in generics works, and the word \u0026ldquo;covariance\u0026rdquo;, I want to do that by an example.\nDomain Model We create a Basket where we can put exactly 2 of any clothing item. Also we define a function to print the total price of the items in the basket.\n1 2 3 4 5 6 7 class Basket\u0026lt;T : ClothingItem\u0026gt;(val firstItem: T, val secondItem: T) fun printBasketPrice(basket: Basket\u0026lt;ClothingItem\u0026gt;) { val firstPrice = basket.firstItem.price val secondPrice = basket.secondItem.price val total = basket.firstItem.price + basket.secondItem.price println(\u0026#34;The total price of $firstPrice + $secondPrice = $total.\u0026#34;) } Now, assume we only want to buy shoes, so we create a basket only containing shoes. This saves us from accidently putting a jacket into the basket. We then want to print its price, using the previously defined function.\n1 2 val shoeBasket = Basket\u0026lt;Shoe\u0026gt;(Shoe(5.5f, 37.5f)) printPrice(shoeBasket) When we try to run this, it won\u0026rsquo;t compile, because the types are not matching.\nBasket\u0026lt;Shoe\u0026gt; not is-a Basket\u0026lt;ClothingItem\u0026gt; This is strange. We know that Shoe is-a ClothingItem, and wherever we expect the latter, we could actually provide the former. It turns out that Basket\u0026lt;Shoe\u0026gt; is not a Basket\u0026lt;ClothingItem\u0026gt;.\nEven though the inner types are in an is-a relationship with each other, in the generic world this does not apply. Why is that?\nIt is best shown by changing the basket to be able to replace items, i.e. change val to var. Now we add a function that is able to replace the first clothing item by one that is provided. This won\u0026rsquo;t compile, but if it would, the program would crash at the print.\n1 2 3 4 5 6 7 8 9 10 11 class Basket\u0026lt;T : ClothingItem\u0026gt;(var firstItem: T, var secondItem: T) fun replaceFirstItem(basket: Basket\u0026lt;ClothingItem\u0026gt;, item: ClothingItem) { basket.firstItem = item } //... called from some other place val basket = Basket\u0026lt;Shoe\u0026gt;(Shoe(1f, 2f), Shoe(3f, 4f)) replaceFirstItem(basket, Jacket(5f, false)) // The print causes the program to crash // Basket\u0026lt;Shoe\u0026gt; now holds a Jacket, and Jacket has no size property println(\u0026#34;The shoe has size ${basket.firstItem.size}.\u0026#34;) And that is the reason why the is-a relationship for generics is not safe to treat the same way as for normal classes. It is unsafe for a function that expects a more generic type to pass a more specific one.\nBut if we could not do that, this would limit us by a lot. You\u0026rsquo;ve seen the function to print the price of the basket.\n1 2 3 fun printBasketPrice(basket: Basket\u0026lt;ClothingItem\u0026gt;) { // ... } If we would not be able to do that, we would have to create the same function for each type of clothing. That may be possible, but neither keeps your code DRY nor would it be feasible if you had hundreds of items.\nDefinition of Covariance The previous can be achieved with covariance. Formally defined:\n1 2 Given a class with a generic parameter: SomeClass\u0026lt;T\u0026gt;. We say SomeClass\u0026lt;A\u0026gt; is-a SomeClass\u0026lt;B\u0026gt; if A is-a B, and that SomeClass\u0026lt;T\u0026gt; is covariant on T. Informally speaking, the \u0026ldquo;normal object inheritance\u0026rdquo; holds. Thus the covariant basket inheritance diagram looks like the following.\nCovariance in Code Covariance exactly solves our problem. But how do we achieve that in code? And, if we enable that, would we run into the same problem that we could maybe override values with the wrong type?\nIn Kotlin, you define covariance on T using the keyword out.\n1 2 3 4 5 6 // Note, we use val again class Basket\u0026lt;out T : ClothingItem\u0026gt;(val firstItem: T, val secondItem: T) // ... call it val basket = Basket\u0026lt;Shoe\u0026gt;(Shoe(1f, 2f), Shoe(3f, 4f)) println(\u0026#34;The shoe has size ${basket.firstItem.size}.\u0026#34;) The keyword is not coincidantelly named out, it actually has a specific reason. When you mark the generic parameter as covariant, you can only use an instance of T in out positions. This does probably introduce another questionmark, because what are out positions? Informally said, an out position is everything that is returning, but never receiving. An instance of T could be acquired by getting from the basket, because the getter would return the instance. An operation like setting a variable to an instance would not be allowed, because the setter would receive the instance.\nWith that said, with a covariant basket, we could never run into overriding an item with a wrong instance. How should we? The operation to write is a receiving operation, thus not an out operation. This is also the reason we have to define the items as val instead of var. As var provides setters, it wouldn\u0026rsquo;t compile.\nContravariance Covariance is a concept that is quite natural, because it follows the \u0026ldquo;normal flow\u0026rdquo; of inheritance. What is way less natural is the concept of contravariance, because it allows you to walk \u0026ldquo;upwards\u0026rdquo; in the inheritance tree - kind of.\nAdding an Action Object Let\u0026rsquo;s create an abstract base class that supports a single function, provided two objects, and returns nothing.\n1 2 3 abstract class Action\u0026lt;T\u0026gt; { abstract fun performAction(firstItem: T, secondItem: T): Unit } For contravariance we want to work with a basket full of jackets. So we create an action specific to jackets.\n1 2 3 4 5 6 7 8 9 class JacketAction : Action\u0026lt;Jacket\u0026gt;() { override fun performAction(firstItem: Jacket, secondItem: Jacket) { if (firstItem.hasZipper \u0026amp;\u0026amp; secondItem.hasZipper) { println(\u0026#34;Both jackets have zippers!!!\u0026#34;) } else { println(\u0026#34;You have at least one non-zipper jacket.\u0026#34;) } } } Call Action on printing Let\u0026rsquo;s create a method that takes a basket full of jackets and an action for jackets. Also we create a jacket basket and call the method with it and the previously defined jacket action.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 fun printComplimentAndPerformAction(basket: Basket\u0026lt;Jacket\u0026gt;, action: Action\u0026lt;Jacket\u0026gt;) { println(\u0026#34;Nice jackets!\u0026#34;) action.performAction(basket.firstItem, basket.secondItem) } // ... in the caller val jacketBasket: Basket\u0026lt;Jacket\u0026gt; = Basket(Jacket(1f, false), Jacket(2f, false)) val jacketAction = JacketAction() printComplimentAndPerformAction(jacketBasket, jacketAction) /* Output Nice jackets! You have at least one non-zipper jacket. */ More general action Say we want to have an action we can perform on any clothing item. That is a common use-case. You have a less specific type (in our example: Clothing Item) and want to perform an action based on methods already provided by that type. Such action could, for example, be to print the more expensive of the two items. If we had to create that for every single item type, we would repeat code all over the place and this would not scale at all. Not to mention again that there are many clothing items.\n1 2 3 4 5 6 7 8 9 class ClothingActionHighestPrice : Action\u0026lt;ClothingItem\u0026gt;() { override fun performAction(firstItem: ClothingItem, secondItem: ClothingItem) { if (firstItem.price \u0026gt; secondItem.price) { println(\u0026#34;First item is more expensive.\u0026#34;) } else { println(\u0026#34;Second item is more expensive\u0026#34;) } } } We would like to call the printComplimentAndPerformAction method with the ClothingActionHighestPrice. But this will not compile.\n1 2 3 val clothingActionHighestPrice = ClothingActionHighestPrice() // This call won\u0026#39;t compile printComplimentAndPerformAction(jacketBasket, clothingActionHighestPrice) At first glance, this seems to be intuitive. We know that Jacket is-a ClothingItem, so we cannot pass a ClothingItem when we want to have a Jacket. So what we want to do is shown in below diagram.\nDefinition of Contravariance It is possible to achieve previous diagram by contravariance.\n1 2 Given a class with a generic parameter: SomeClass\u0026lt;T\u0026gt;. We say SomeClass\u0026lt;A\u0026gt; is-a SomeClass\u0026lt;B\u0026gt; if B is-a A, and that SomeClass\u0026lt;T\u0026gt; is contravariant on T. Informally speaking, we say that our class with generic parameter A is a child of another class with generic parameter B if B supersedes A. The diagram shows that we want to do exactly that. ClothingItem supersedes Jacket, yet we want the Action\u0026lt;ClothingItem\u0026gt; to be a Action\u0026lt;Jacket\u0026gt;.\nWhen I read about contravariance the first time, I was confused. My confusion came from that I thought we are actually allowing the Action\u0026lt;ClothingItem\u0026gt; to perform methods on Jacket. This is obviously not the case, the clothing item action only sees a clothing item.\nContravariance in Code For covariance there is out, and contravariance is more or less the opposite of covariance. It\u0026rsquo;s not a big surprise that the keyword for contravariance is in. On the abstract base class, the Action, we make T covariant.\n1 2 3 abstract class Action\u0026lt;in T\u0026gt; { abstract fun performAction(firstItem: T, secondItem: T): Unit } As with the out keyword, the in keyword also specifies where the type is allowed to be placed at. You can only place T on receiving, not returning operations. Passing the jackets into the performAction function is receiving, so we can safely do that. Yet we cannot receive T from performAction, because the base type is unknown to that method. It could be anything more specific.\nAssume for contravariance you would allow to also return the type. When the printComplimentAndPerformAction method, defined on jacket, would receive some clothing item, it could also be a shoe. This would not be safe anymore.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 abstract class Action\u0026lt;in T\u0026gt; { // Having T in the return is not allowed abstract fun performAction(firstItem: T, secondItem: T): T } // ... class NotAllowedReturn : Action\u0026lt;ClothingItem\u0026gt;() { override fun performAction(firstItem: ClothingItem, secondItem: ClothingItem): ClothingItem { return Shoe(1f, 1f) } } // ... fun printComplimentAndPerformAction(basket: Basket\u0026lt;Jacket\u0026gt;, action: Action\u0026lt;Jacket\u0026gt;) { println(\u0026#34;Nice jackets!\u0026#34;) val receivedItem = action.performAction(basket.firstItem, basket.secondItem) // If firstItem were var, we would put a shoe in here // Which violates the contract that there are only jackets basket.firstItem = receivedItem } Invariance Now that covariance and contravariance are covered, the last piece is the default mode: Invariance.\n1 2 Given a class with a generic parameter: SomeClass\u0026lt;T\u0026gt;. We say SomeClass\u0026lt;A\u0026gt; is invariant to SomeClass\u0026lt;B\u0026gt; if neither SomeClass\u0026lt;A\u0026gt; is-a SomeClass\u0026lt;B\u0026gt; nor SomeClass\u0026lt;B\u0026gt; is-a SomeClass\u0026lt;A\u0026gt;. In simple words: Whenever an instance of SomeClass\u0026lt;A\u0026gt; is expected, you must provide SomeClass\u0026lt;A\u0026gt;, and cannot provide a SomeClass\u0026lt;T\u0026gt; with a T being higher/lower in the class hierarchy. This is the default mode for any generic class in Kotlin. That is why the first example didn\u0026rsquo;t work and we had to make it covariant.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 abstract class ClothingItem(val price: Float) class Shoe(price: Float, val size: Float) : ClothingItem(price) class Jacket(price: Float, val hasZipper: Boolean) : ClothingItem(price) // by default, T is invariant class Basket\u0026lt;T : ClothingItem\u0026gt;(val firstItem: T, val secondItem: T) // and thus this method only takes a basket of type ClothingItem fun printBasketPrice(basket: Basket\u0026lt;ClothingItem\u0026gt;) { val firstPrice = basket.firstItem.price val secondPrice = basket.secondItem.price val total = basket.firstItem.price + basket.secondItem.price println(\u0026#34;The total price of $firstPrice + $secondPrice = $total.\u0026#34;) } fun main(args: Array\u0026lt;String\u0026gt;) { val shoeBasket = Basket\u0026lt;Shoe\u0026gt;(Shoe(5.5f, 37.5f)) // so this will not compile printBasketPrice(shoeBasket) } Bottom Line Covariance and contravariance enable us to provide compile time guarantee for runtime safety. The most prominent example for covariance is the Collection interface. Though contravariance has fewer use cases, you probably have used that one probably already. The Comparator interface is the most prominent candidate where contravariance is used.\n","date":"2021-07-04T22:00:00Z","permalink":"https://revontulet.dev/p/2021-generics-invariance-covariance-contravariance/","title":"Generics - Invariance, Covariance and Contravariance"},{"content":"The repository with the Kotlin code can be found on github/MisterDerpie/kotlin-data-stubs.\nCurrently I\u0026rsquo;m having the great luck that our team started using Kotlin at work. For that reason, I decided to learn Kotlin properly and started my journey with Kotlin in Action (2017, Manning). This is a bit outdated, considering that Kotlin 1.5 is the current release, but the vast majority of concepts are the same as for 1.0.\nOne concept that fascinates me and I really see a lot of value in is Extensions. Especially for tests I find that concept very useful, because it enables us to provide test related logic to our classes without actually adding this in the real production code. Let\u0026rsquo;s have a look at how we can utilize extensions to create stubs of business classes.\nProblem statement in Java Test Utils to Stub Classes A problem that often arises in Java projects is that you need to provide stubs for using e.g. the same customer in multiple tests. Suppose you have a class Customer.java that looks like this\n1 2 3 4 5 6 7 8 9 @AllArgsConstructor /* Generates constructor with all parameters */ @Getter /* Generates getters for all properties */ public class Customer { private final UUID id; private final String name; private final Integer age; } and you would like to stub the same customer for your tests. How would you do that? Probably you create some class called TestUtils.java, that provides a static method stubCustomer. In larger projects, you most likely have many different stubs in it, from Address over Customer to Shopping Cart. Thus, you start adding all these stubs in the TestUtils class.\n1 2 3 4 5 6 7 8 9 10 11 12 public class TestUtils { public static Customer stubCustomer() { ... } public static Address stubAddress() { ... } ... } I have seen such classes having hundreds of lines of code, with many different, totally unrelated objects. A first attempt to improve that would be to create TestUtilsCustomer.java, TestUtilsAddress.java, \u0026hellip; That would leave us with a better, yet still not the best solution.\nTest Code meets Production Code Wouldn\u0026rsquo;t it be nice to provide stubCustomer directly in the Customer itself? With Java, you would have to include such method in your production code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 @AllArgsConstructor /* Generates constructor with all parameters */ @Getter /* Generates getters for all properties */ public class Customer { private final UUID id; private final String name; private final Integer age; public static Customer stubFullAgeCustomer() { return new Customer(UUID.randomUUID(), \u0026#34;JavaName\u0026#34;, 18) } } Testcode should never, never reside in production code. With Kotlin, we can actually do that, but without production code ever knowing about this stubFullAgeCustomer method.\nSolution in Kotlin Before continue reading, think about why it would be nice to have a preconfigured Customer provided by the Customer object itself. When implementing the Builder Pattern, you configure each property by a callchain of functions with the properties\u0026rsquo; names. It could look like this Customer.name(\u0026quot;Builder\u0026quot;).age(23).uuid(UUID.randomUUID()).build(). So why not getting a fully preset instance as part of this object, too?\nThis can be achieved in Kotlin.\nSetup We define a Customer data class and an AgeCheckService (the latter is purely to have something to test, so that I can show how our extension works).\nCustomer.kt\n1 2 3 4 5 6 7 8 9 10 package dto import java.util.UUID data class Customer( val id: UUID, val name: String, val age: Int) { companion object {} } What enables us to extend the Customer with new \u0026ldquo;static\u0026rdquo; methods for our test is the companion object. As this is a pure data class, we don\u0026rsquo;t add anything else.\nAgeCheckService.kt\n1 2 3 4 5 6 7 8 9 10 11 package service import dto.Customer class AgeCheckService { companion object { fun isCustomerOfFullAge(customer: Customer): Boolean { return customer.age \u0026gt;= 18 } } } This services only functionality is to return true when the customer is of full age, and false when they\u0026rsquo;re minor. As initially stated, this is for the sole purpose of having something to test.\nExtend Customer In our test, we would like to call the following:\n1 2 Customer.stubFullAgeCustomer() Customer.stubMinorAgeCustomer() to retrieve preconfigured customers with some age greater or equal than 18 or less than 18, respectively. This is as easy as doing companion object extensions.\nTo do so, simply place a file in your testpath, and define the functions on Customer.Companion.\n1 2 3 4 5 6 7 8 9 10 11 package dto import java.util.UUID fun Customer.Companion.stubFullAgeCustomer(): Customer { return Customer(UUID.nameUUIDFromBytes(\u0026#34;This is some string for UUID\u0026#34;.toByteArray()), \u0026#34;MisterDerpie\u0026#34;, 24) } fun Customer.Companion.stubMinorAgeCustomer(): Customer { return Customer(UUID.nameUUIDFromBytes(\u0026#34;This is another UUID string\u0026#34;.toByteArray()), \u0026#34;MinorPerson\u0026#34;, 17) } Note that when you do not define a name for the companion object in the class, it will be accessible by Companion. Congratulations, you just added extensions which you can use in your tests.\nTest AgeCheckService using Extension Stub The last step is to import your Extensions into your test. They are not automatically applied globally to your Customer, which is the reason why you explicitly have to import them.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package service import dto.Customer import dto.stubFullAgeCustomer import dto.stubMinorAgeCustomer import org.junit.jupiter.api.Test import org.assertj.core.api.Assertions.assertThat class AgeCheckServiceTest { @Test fun `should return true when customer is of full age`() { val customer = Customer.stubFullAgeCustomer() assertThat(AgeCheckService.isCustomerOfFullAge(customer)).isEqualTo(true) } @Test fun `should return false when customer is of minor age`() { val customer = Customer.stubMinorAgeCustomer() assertThat(AgeCheckService.isCustomerOfFullAge(customer)).isEqualTo(false) } } See the imports at the top. I placed the Customer in a package called dto. The stubFullAgeCustomer and stubMinorAgeCustomer are also placed in a package called dto (but the name doesn\u0026rsquo;t matter, it\u0026rsquo;s just due to the same folder structure in the tests). As stated initially, we need to import them to actually apply the extension to the Customer.\nThe tests are then straightforward: Get the customer and assert that the age service in fact returns true when they\u0026rsquo;re of full age and false when they\u0026rsquo;re of minor age.\n","date":"2021-06-25T22:00:00Z","permalink":"https://revontulet.dev/p/2021-teststubs-with-kotlin-extensions/","title":"Teststubs with Kotlin Extensions"},{"content":"The sourcecode of this post is available on github/MisterDerpie/spring-boot-with-mongodb.\nForeword For a small application to store receipts I wanted to use Spring Boot and NoSQL database MongoDB. As with many basic topics in the spring world, there is a Getting Started guide on spring.io, with the specific title \u0026ldquo;Accessing Data with MongoDB\u0026rdquo;.\nThough this guide may suffice for a really straightforward start, it actually misses out two, from my point of view, essential questions.\nHow to connect to a MongoDB instance? How to integration test MongoDB? Therefore I wrote this post. As I want to use JavaMoney, I also cover Mongo Converters. The list explains what is covered in this post.\nUse MongoDB as a data store in Spring Connect to a MongoDB instance with your provided credentials Integration test with an embedded MongoDB Use JavaMoney with MongoDB Setup MongoDB First we need an instance of MongoDB. I will - as usual - use Docker. Therefore I assume you have docker installed, if not, there are plenty of guides how to do that available online. We use the Docker-Mongo image. To start a MongoDB instance that is running in the background, run below code from the shell.\n1 2 3 4 5 6 docker container run -d \\ --name mongodb \\ -e MONGO_INITDB_ROOT_USERNAME=admin \\ -e MONGO_INITDB_ROOT_PASSWORD=admin \\ -p 27017-27019:27017-27019 \\ mongo:latest Setup Spring Boot Project Create Spring Project This section is about creating the Spring project. After you created the project, don\u0026rsquo;t forget to add the additional dependencies (next section).\nCreate project with Spring Initializr The easiest way to create the project is using Spring Initializr. I will use Gradle as the build automation tool, but it doesn\u0026rsquo;t matter and you can well select Maven.\nWe will include the 2 starters\nLombok Spring Data MongoDB To do so, click on Add Dependencies (top right corner) and search for Spring Data Mongo DB (don\u0026rsquo;t use the reactive one) and Lombok.\nSelect that you want to have a Gradle Project, use Java, fill out the Project Metadata, select Packaging Jar and Java 11. Then click Generate and you should get a .zip containing your project. Unzip this anywhere and open it in the IDE of your choice. I prefer IntelliJ.\nCreate project with Gradle In case you want to create your Gradle project from scratch, you can follow docs.gradle.org - Building Java Applications Sample. Add below dependencies to your dependencies block in the build.gradle file.\n1 2 3 4 5 implementation \u0026#39;org.springframework.boot:spring-boot-starter-data-mongodb\u0026#39; implementation \u0026#39;org.springframework.boot:spring-boot-starter-web\u0026#39; compileOnly \u0026#39;org.projectlombok:lombok\u0026#39; annotationProcessor \u0026#39;org.projectlombok:lombok\u0026#39; testImplementation \u0026#39;org.springframework.boot:spring-boot-starter-test\u0026#39; Additional Dependencies To show how to integration test and how to use JavaMoney with MongoDB, we add two dependencies, namely Flapdoodle Embedded MongoDB and JavaMoney. Add\n1 2 3 testImplementation \u0026#39;org.springframework.boot:spring-boot-starter-test\u0026#39; testImplementation \u0026#39;de.flapdoodle.embed:de.flapdoodle.embed.mongo:3.0.0\u0026#39; implementation \u0026#39;org.javamoney.moneta:moneta-core:1.4.2\u0026#39; to your dependencies in the build.gradle file. It should then look similar to this.\n1 2 3 4 5 6 7 8 dependencies { implementation \u0026#39;org.springframework.boot:spring-boot-starter-data-mongodb\u0026#39; compileOnly \u0026#39;org.projectlombok:lombok\u0026#39; annotationProcessor \u0026#39;org.projectlombok:lombok\u0026#39; testImplementation \u0026#39;org.springframework.boot:spring-boot-starter-test\u0026#39; testImplementation \u0026#39;de.flapdoodle.embed:de.flapdoodle.embed.mongo:3.0.0\u0026#39; implementation \u0026#39;org.javamoney.moneta:moneta-core:1.4.2\u0026#39; } Connect Application to MongoDB This part only covers how to configure your application to be able to connect with MongoDB. I assume you have a MongoDB instance running configured the same way as I did with the docker image. That is, listening on port 27017, with user admin and password admin. Note that you should not use those credentials in a real application of course, but for only running this on your local machine and getting started this is perfectly fine.\nIn the resources folder (where the application.properties resides), create a new properties file called e.g. application-production.properties and put the following content in it.\n1 2 3 4 5 6 spring.data.mongodb.authentication-database=admin spring.data.mongodb.username=admin spring.data.mongodb.password=admin spring.data.mongodb.database=testdatabase spring.data.mongodb.port=27017 spring.data.mongodb.host=localhost This enables Spring\u0026rsquo;s MongoDB repository to\nconnect to a MongoDB instance running on localhost with port 27017 and username admin and password admin and selected database testdatabase and authentication database admin (default in the docker container) Why not the default application.properties? You may wonder why we don\u0026rsquo;t use the default properties file. The reason is that the embedded MongoDB for some reason does not override these values. In case you run your integration tests without providing a profile and/or not providing these values, it would fall back to the default properties file. Thus the tests would fail, as you could not connect to the database.\nFor the sake of completion, before your tests even start you would see this error.\n1 org.springframework.beans.factory.BeanCreationException: Error creating bean with name \u0026#39;embeddedMongoServer\u0026#39; Create Mongo Document in Application The next part is to create a MongoDB document representation class. Simply put, this is the entity you are going to persist in the database. We keep it simple and want to store an Item we purchased. Its values are an id, a name and a price. We will use the id as the primary key.\nCreate a class called Item with the following contents.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import lombok.AllArgsConstructor; import lombok.Data; import org.javamoney.moneta.FastMoney; import org.springframework.data.annotation.Id; import org.springframework.data.mongodb.core.mapping.Document; import java.util.UUID; @Document @AllArgsConstructor @Data public class Item { @Id public UUID id; public FastMoney price; public String name; } Let\u0026rsquo;s explain a bit what\u0026rsquo;s going on here.\n@Document indicates that we want this class to be persistable in MongoDB. In case you are familiar with JPA, this is the equivalent to @Entity.\n@AllArgsConstructor and @Data are Lombok annotations. The former one creates what it states, a constructor with all parameters. With the latter we automatically create getters and setters.\n@Id marks this field as the primary key to use in the database.\nUse MongoRepository to access Document The next step is that we want to store our document in the database. Spring provides a very easy to use interface for that. Just create a class called ItemRepository that extends MongoRepository\u0026lt;T, ID\u0026gt;.\n1 2 3 4 5 6 7 import org.springframework.data.mongodb.repository.MongoRepository; import java.util.UUID; public interface ItemRepository extends MongoRepository\u0026lt;Item, UUID\u0026gt; { } The first generic parameter of MongoRepository indicates what entity we want to persist. As our Item is the entity, we put this there. To identify items, we set the primary key of type UUID. Therefore we put the ID type as the second generic parameter.\nIntegration Test MongoDB We are done, almost. Let us test our MongoRepository whether it works as expected (hint, it doesn\u0026rsquo;t, but we will get to that).\nCreate the test Create a test class ItemRepositoryTest with the following content\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import org.javamoney.moneta.FastMoney; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.autoconfigure.data.mongo.DataMongoTest; import java.util.UUID; import static org.assertj.core.api.Assertions.assertThat; @DataMongoTest public class ItemRepositoryTest { @Autowired ItemRepository itemRepository; @Test public void shouldStoreItem() { Item item = new Item(UUID.randomUUID(), FastMoney.of(1, \u0026#34;EUR\u0026#34;), \u0026#34;Test Item\u0026#34;); itemRepository.save(item); Item storedItem = itemRepository.findById(item.getId()).orElseThrow(); assertThat(storedItem.getId()).isEqualTo(item.getId()); assertThat(storedItem.getPrice()).isEqualTo(item.getPrice()); assertThat(storedItem.getName()).isEqualTo(item.getName()); } } Let us look at what\u0026rsquo;s done here, before we run the test.\n@DataMongoTest starts the Spring Boot Test context. In addition to that it also creates an embedded MongoDB instance.\n@Autowired \u0026ldquo;injects the dependency\u0026rdquo; of ItemRepository into the test class. Simply put, Spring creates an instance of ItemRepository and puts the reference to that instance in itemRepository. So itemRepository is not null, and the creation of it is done by Spring\u0026rsquo;s Dependency Injection container. If you want to to read more about it click on the links for @Autowired and Spring Dependency Injection.\nThe test itself then is pretty simple. We create an Item with a random UUID, value of 1 euro and the name \u0026ldquo;Test Item\u0026rdquo;. Then we store this in the database and try to query the database for the item ID, asserting that the retrieved item is equal to the initial item.\nRun the test When we try to run the test, it will even fail before the first assertion is reached. Looking at the logs, we can see the following error.\n1 2 3 Failed to instantiate org.javamoney.moneta.FastMoney using constructor NO_CONSTRUCTOR with arguments org.springframework.data.mapping.model.MappingInstantiationException: Failed to instantiate org.javamoney.moneta.FastMoney using constructor NO_CONSTRUCTOR with arguments ... The problem is the internal representation of FastMoney. When serialized into a JSON object, it actually does not just look like\n1 { \u0026#34;number\u0026#34;: 1, \u0026#34;currency\u0026#34;: \u0026#34;EUR\u0026#34; } but like\n1 {\u0026#34;currency\u0026#34;:{\u0026#34;context\u0026#34;:{\u0026#34;providerName\u0026#34;:\u0026#34;java.util.Currency\u0026#34;,\u0026#34;empty\u0026#34;:false},\u0026#34;numericCode\u0026#34;:978,\u0026#34;defaultFractionDigits\u0026#34;:2,\u0026#34;currencyCode\u0026#34;:\u0026#34;EUR\u0026#34;},\u0026#34;number\u0026#34;:1.00000,\u0026#34;precision\u0026#34;:1,\u0026#34;factory\u0026#34;:{\u0026#34;defaultMonetaryContext\u0026#34;:{\u0026#34;fixedScale\u0026#34;:true,\u0026#34;maxScale\u0026#34;:5,\u0026#34;amountType\u0026#34;:\u0026#34;org.javamoney.moneta.FastMoney\u0026#34;,\u0026#34;precision\u0026#34;:19,\u0026#34;providerName\u0026#34;:null,\u0026#34;empty\u0026#34;:false},\u0026#34;amountType\u0026#34;:\u0026#34;org.javamoney.moneta.FastMoney\u0026#34;,\u0026#34;maxNumber\u0026#34;:92233720368547.75807,\u0026#34;minNumber\u0026#34;:-92233720368547.75808,\u0026#34;maximalMonetaryContext\u0026#34;:{\u0026#34;fixedScale\u0026#34;:true,\u0026#34;maxScale\u0026#34;:5,\u0026#34;amountType\u0026#34;:\u0026#34;org.javamoney.moneta.FastMoney\u0026#34;,\u0026#34;precision\u0026#34;:19,\u0026#34;providerName\u0026#34;:null,\u0026#34;empty\u0026#34;:false}},\u0026#34;context\u0026#34;:{\u0026#34;fixedScale\u0026#34;:true,\u0026#34;maxScale\u0026#34;:5,\u0026#34;amountType\u0026#34;:\u0026#34;org.javamoney.moneta.FastMoney\u0026#34;,\u0026#34;precision\u0026#34;:19,\u0026#34;providerName\u0026#34;:null,\u0026#34;empty\u0026#34;:false},\u0026#34;zero\u0026#34;:false,\u0026#34;positive\u0026#34;:true,\u0026#34;negative\u0026#34;:false,\u0026#34;negativeOrZero\u0026#34;:false,\u0026#34;positiveOrZero\u0026#34;:true,\u0026#34;scale\u0026#34;:5} But FastMoney does only have a constructor that accepts these very two parameters. The solution is to provide a custom converter, that would store our money representation exactly as what the constructor expects.\nMongoDB Converter Create Converters To successfully save and load our Item from the database, we need to convert the price. This is done by creating a ReadingConverter and a WritingConverter. Create two classes, namely FastMoneyReadConverter and FastMoneyWriteConverter with the following contents.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import org.javamoney.moneta.FastMoney; import org.springframework.core.convert.converter.Converter; import org.springframework.data.convert.ReadingConverter; import java.math.BigDecimal; @ReadingConverter public class FastMoneyReadConverter implements Converter\u0026lt;String, FastMoney\u0026gt; { public FastMoney convert(String input) { String[] storedValue = input.split(\u0026#34;###\u0026#34;); return FastMoney.of(new BigDecimal(storedValue[0]), storedValue[1]); } } 1 2 3 4 5 6 7 8 9 10 11 12 import org.javamoney.moneta.FastMoney; import org.springframework.core.convert.converter.Converter; import org.springframework.data.convert.WritingConverter; @WritingConverter public class FastMoneyWriteConverter implements Converter\u0026lt;FastMoney, String\u0026gt; { public String convert(FastMoney input) { return input.getNumber() + \u0026#34;###\u0026#34; + input.getCurrency(); } } Our converters implement the Converter\u0026lt;Input, Output\u0026gt; interface. The naming states exactly what we want to achieve with them. With a ReadingConverter, we want to convert when reading from the database into the Java Object. A WritingConverter is used when we write into the database from the Java Object.\nWhat the converters do is very straightforward. We take the values from the Money, concatenate them with a placeholder of three hashtags and then save this entire string to the database. On reading from the database, we revert that operation by splitting them at the three hashtags and creating a FastMoney instance of the two obtained values.\nCreate Converter Config We created our converters but need to make the MongoRepository aware of using them when storing/loading documents from MongoDB. For that, we simply create a configuration and provide a list of MongoCustomConversions.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.data.mongodb.core.convert.MongoCustomConversions; import java.util.List; @Configuration public class ConverterConfig { @Bean public MongoCustomConversions mongoCustomConversions() { return new MongoCustomConversions(List.of(new FastMoneyReadConverter(), new FastMoneyWriteConverter())); } } @Configuration loads this class in the Spring context.\n@Bean tells Spring to get an instance of the return type (here: MongoCustomConversions) by calling this method. In the mongoCustomConversion method we pass a list of our converters to the constructor of MongoCustomConversions and return that.\nIntegration Tests Revisited Let\u0026rsquo;s try running our integration tests again. Now that we have the converters in place, we should be good to go, right? No. Unfortunately our tests will fail for the very same reason again.\nWhy is that?\nWhen running with @DataMongoTest, it does not scan for the ConverterConfig. That is why we need to include this in the Spring context in this test. This is easily done by adding @Import(ConverterConfig.class).\nOur test should finally look like this and pass\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import org.javamoney.moneta.FastMoney; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.autoconfigure.data.mongo.DataMongoTest; import org.springframework.context.annotation.Import; import java.util.UUID; import static org.assertj.core.api.Assertions.assertThat; @DataMongoTest @Import(ConverterConfig.class) public class ItemRepositoryTest { @Autowired ItemRepository itemRepository; @Test public void shouldStoreItem() { Item item = new Item(UUID.randomUUID(), FastMoney.of(1, \u0026#34;EUR\u0026#34;), \u0026#34;Test Item\u0026#34;); itemRepository.save(item); assertThat(storedItem.getId()).isEqualTo(item.getId()); assertThat(storedItem.getPrice()).isEqualTo(item.getPrice()); assertThat(storedItem.getName()).isEqualTo(item.getName()); } } Congratulations, you successfully wired Spring with MongoDB.\n","date":"2021-05-26T22:00:00Z","permalink":"https://revontulet.dev/p/2021-spring-boot-with-mongodb/","title":"Spring Boot with MongoDB"},{"content":"Foreword Since many years I am using JDownloader as the solution to download things on all my computers. The problem with it is though that the files are not synchronized between them. Thus I cannot access what I downloaded from any machine different from the original downloader. This problem worsens even with the fact that once I am not at home, I cannot access the harddrives the multimedia is on.\nWhat bugs me about JDownloader though is that there is no CLI application nor a WebUI, unless you want to rely on their external MyJDownloader service. But I wanted to find a way that enables me to run JDownloader on a headless RaspberryPi. The JDownloader should be accessible via browser, yet selfhosted.\nPrerequisites I only tested this on a RaspberryPi 4 with 8 GB of RAM running Ubuntu Server 21.04. It could be that the Raspbery Pi 3 is too weak for this, but feel free to give it a try. Therefore I assume you are running on a Debian based distribution. All not-containerized programs we need to build the multimedia station is Docker and ZeroTier.\nInstallation of Prerequisites To build this setup, we are using\nDocker ZeroTier MisterDerpie/jdownloader-docker Docker-Apache/httpd. We need to install Docker and ZeroTier (the latter needs to be set up) before starting, the other two components will be touched later on.\nDocker Installing Docker on the RaspberryPi is straightforward.\n1 curl https://get.docker.com/ | sh After the installation, you need to be add your user to the docker group.\n1 sudo usermod -aG docker $USER Once this is done, reboot or logout and login. Verify that the installation was successful by running hello-world.\n1 docker container run --rm hello-world ZeroTier This section describes how to setup ZeroTier on the RaspberryPi. This is solely for accessing it from anywhere. In case you don\u0026rsquo;t want to access the Pi from outside the same network which it is running in, you can skip this section. The IP address will then be your Pi\u0026rsquo;s local IP address.\nAs straightforward as it is to install Docker, the same it is for ZeroTier. Yet we need more steps to do it.\nCreate ZeroTier Account Go to ZeroTier.com and click on Sign Up to create an account. It is free and you don\u0026rsquo;t need any premium plan.\nCreate a ZeroTier Network On the same page, go to Log In. After logging in, you should be redirected to my.zerotier.com. Click on Create Network.\nYou should see a table with the new created network.\nClick anywhere on the row to go to the network configuration. The network can be named as you want. Important: Ensure that it is marked as private.\nThis will prevent any other user who may guess your network ID from being able to access it directly. Scroll down to the Advanced section and select any of the auto-assign IP addresses from the range in the lowest row. I selected 192.168.195.*, so this tutorial will continue assuming you\u0026rsquo;re in the same range.\nIf you scroll further to the Members section, you should see that no devices have joined this network yet.\nJoin ZeroTier Network with Raspberry Pi To join a network, we need the network ID. This can be found in the yellow box stating No devices have joined in a way of join \u0026lt;NetworkId\u0026gt;. You can join via zerotier-cli command. My networkid is a84ac5c10a0a436f, so I run the following.\n1 sudo zerotier-cli join a84ac5c10a0a436f When the command ran successful, the console output should be 200 join OK. After some seconds you should be able to see the first member in the Members box.\nTo get it assigned an IP address in the virtual ZeroTier network, we need to check the Auth? box. Unless you check this, the device will not be part of this network and cannot communicate with others. I also recommend you to give it a name, for example RaspberryPi.\nWe are done. ZeroTier is set up and installed on our RaspberryPi. As the yellow box warns us, there should be at least two devices. Of course, the other devices should be the hostmachines we want to access the RPi from.\nJoin ZeroTier Network from another machine Linux In case your host machine is a Debian based Linux machine, you can install ZeroTier the very same as done for the RaspberryPi. The only step you need to do after installation is joining the previously created network.\nWindows On Windows, download the ZeroTier One.msi installer and install it. Start ZeroTier from the program menu. Then you should see the icon in the tray (where the speaker symbol is in the taskbar).\nIn case you don\u0026rsquo;t see it, click on the arrow next to the speaker. Then click on the icon and click Join Network ....\nEnter the network\u0026rsquo;s id and click Join.\nAfter a short time Windows should ask you whether you want your computer to be discoverable. Select Yes.\nFinal Step Once you have joined the network from your host machine, don\u0026rsquo;t forget to go back to the webinterface and authorize your device. Again, I recommend you to also name it. See the screenshots from Join ZeroTier Network with RaspberryPi how to authorize and name it.\nJDownloader-Docker The JDownloader-Docker image enables you to run JDownloader and access the UI via any browser. This image has minor JDownloader preconfiguration. If you are interested in the details, check the repository. To do so, we first need to clone the repo and then build the image.\n1 git clone https://github.com/MisterDerpie/jdownloader-docker Go into the root of the directory. From there, build the image named jd2-base.\n1 docker build -t jd2-base . This process will take some time. Once it\u0026rsquo;s done, congratulations: You are now having a JDownloader base image. We will not extend the image but use it to run our container.\nCreate Download Directory Create a folder in your home directory that is called jdownloader-root.\n1 mkdir ~/jdownloader-root/ Go in that directory and create 2 directories, Downloads and Extracted.\n1 2 mkdir Downloads mkdir Extracted Start JDownloader The webinterface to JDownloader is accessible on port 8080. We can link it to any port on the Pi, yet we will use the same port. Make sure you are in the jdownloader-root directory and the directories Downloads and Extracted exist. Then, let\u0026rsquo;s start our JDownloader container.\n1 2 3 4 5 6 docker container run -d \\ -p 8080:8080 \\ -v \u0026#34;$(pwd)/Downloads\u0026#34;:/Downloads \\ -v \u0026#34;$(pwd)/Extracted\u0026#34;:/Extracted \\ --name jdownloader \\ jd2-base This may also take some time. Once it\u0026rsquo;s done you should see the ID of the container being printed.\nWhat does this do? We start the JDownloader container with the name jdownloader and link it\u0026rsquo;s Downloads and Extracted directory to the ones in ~/jdownloader-root/. As I mentioned, the image is preconfigured, so everything that is downloaded will land in Downloads. In case it\u0026rsquo;s an archive, it will be extracted to Extracted and the archives will be deleted.\nNote: This container is not configured to startup on boot again. You\u0026rsquo;d have to run docker container start jdownloader after a reboot.\nConnect to JDownloader This section only works if you set up ZeroTier. Go to the ZeroTier Webinterface and find out the IP Address of the RaspberryPi.\nAs you can see, the IP of my RaspberryPi is 192.168.195.230. To connect to JDownloader, open \u0026lt;yourip\u0026gt;:8080/vnc.html in your browser. So in my case, I go to 192.168.195.230:8080/vnc.html. Click Connect, and there it is. We see JDownloader running and open in the browser.\nApache Webserver for File Access We are almost good to go. Now JDownloader downloads and extracts files for us. We can also access them by accessing the Pi, but how can we access them from e.g. VLC?\nI present you a very basic and rudimentary way of providing them via Docker-Apache/httpd. You will be able to access the files via browser by simply putting in the Pi\u0026rsquo;s IP address.\nNote: This only works when you store all files in either Downloads or Extracted directories. In case you followed this guide, this applies.\nCreate index.html To be presented with a link to your JDownloader WebUI, as well as the Downloads and Extracted folder selection on the landing page, go to ~/jdownloader-root/. There create a file called index.html and add the following content.\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Server\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;a href=\u0026#34;\u0026#34; id=\u0026#34;jdownloader\u0026#34;\u0026gt;JDownloader\u0026lt;/a\u0026gt;\u0026lt;br/\u0026gt; \u0026lt;a href=\u0026#34;./Downloads/\u0026#34;\u0026gt;Downloads\u0026lt;/a\u0026gt;\u0026lt;br/\u0026gt; \u0026lt;a href=\u0026#34;./Extracted/\u0026#34;\u0026gt;Extracted\u0026lt;/a\u0026gt; \u0026lt;script\u0026gt; document.getElementById(\u0026#39;jdownloader\u0026#39;).href = window.location.protocol + \u0026#34;//\u0026#34; + window.location.hostname + \u0026#34;:8080/vnc.html\u0026#34;; \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; Start the Webserver Go into the ~/jdownloader-root/ directory. From there, start the webserver.\n1 2 3 docker container run -d --name webserver \\ -p 80:80 \\ -v \u0026#34;$(pwd)\u0026#34;:/usr/local/apache2/htdocs/ httpd It starts on port 80 (the default port) containing the entire directory. When you go to your browser and navigate to your Pi\u0026rsquo;s IP address (in my case 192.168.195.230) you will be presented the landing page. There you can select whether you want to browse the Downloads or Extracted directory. Moreover can you also go to JDownloader directly from the landing page.\nClicking on e.g. Downloads gives you an index of the files/folders contained in it.\nBottom Line That\u0026rsquo;s already it ðŸŽ‰. You\u0026rsquo;re now running a fully remote accessible Multimedia Station on your Raspberry Pi. As mentioned, all the containers are not configured to start on the Pi\u0026rsquo;s startup. So once you reboot your RaspberryPi, you would have to run the following.\n1 2 docker container start jdownloader docker container start webserver There are several guides available online on how to make a container start on boot. This can for example be achieved by executing docker run with the --restart=always parameter or making it a service in systemd.\nAdditional Content We have JDownloader downloading and extracting, can access it from anywhere and also access the files via browser. Yet two topics should be visited to make it fully worth. The first is how we get containers \u0026amp; links into JDownloader. Second topic is about how to stream downloaded content in VLC.\nAdd DLCs or Links to JDownloader To add links, you can use the clipboard from the sidebar. Click on the sidebar icon.\nNext, open the cliboard.\nThen paste your links in the clipboard. Make a right-click somewhere in the LinkGrabber field. Select Paste Links and the linkgrabber should start gathering them.\nTo get DLCs to JDownloader, you can simply put them in the Downloads or Extracted directory. Do so by e.g. transfer them to the Pi first (e.g. via ssh) and then open them via File -\u0026gt; Load Linkcontainer. The docker image additionally offers the volume /containers where you can place them in, but I didn\u0026rsquo;t use it in this post. This could be used in addition to running for instance a FileZilla container for that directory.\nStream to VLC Media Player As many people use JDownloader to download multimedia files, it would be good having acces to them in VLC player. Open the webui. From there navigate to the file you want to stream. Make a right click on the file and copy it\u0026rsquo;s link address.\nThen go to VLC and click on File -\u0026gt; Open Network Stream ....\nPaste the link there and click on Play.\nVLC should now start playing your selected video.\n","date":"2021-05-02T17:00:00Z","permalink":"https://revontulet.dev/p/2021-raspberry-pi-four-remote-multimedia-station/","title":"RaspberryPi 4 as a remote Multimedia Station"},{"content":"Foreword This blog is created using Jekyll.\nThe striked through sentence above used to be true for the old version of this blog, hosted on [misterderpie.com]. However, the version at [revontulet.dev] uses GitHub pages and Hugo.\nOriginal Post I really like Jekyll, as I don\u0026rsquo;t like frontend development. One problem is though, that I currently only have Jekyll installed on one single machine. So when I\u0026rsquo;m not at home but want to add a blog post, I would have to install Jekyll on the machine I\u0026rsquo;m travelling with. Luckily there is Docker and the envygeeks/jekyll-docker Jekyll Image. As I\u0026rsquo;m adding blogposts very rarely, I do not need to have a full build and delivery pipeline for it (despite the fact that at the time of writing I wouldn\u0026rsquo;t even be able to create such). A simple container to build the latest version of the site suits my needs.\nThe reason I made a small script for that is the following.\nObviously I\u0026rsquo;m lazy, and want to have an out-of-the-box command for every machine. Creating the container with docker run --rm will always create a new container installing all dependencies. This process can take a while. Persisting the container after first creation enables to rerun it after adding a new post. No need to install dependencies again. The script is nothing special, but I thought it\u0026rsquo;s been a while since I added a post. Maybe you can find some use in it.\nSimply create an executable script in your GitHub Repo and put the following content in it (I named it build.sh):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/bin/bash BUILDER_NAME=jekyll_builder JEKYLL_VERSION=3.8 if [ ! \u0026#34;$(docker ps -a -q -f name=$BUILDER_NAME)\u0026#34; ]; then docker container run \\ --name $BUILDER_NAME \\ --env JEKYLL_ENV=production \\ --volume=\u0026#34;$(pwd)\u0026#34;:/srv/jekyll \\ --entrypoint \u0026#34;/bin/bash\u0026#34; \\ jekyll/jekyll:$JEKYLL_VERSION \\ jekyll build else docker container start -i $BUILDER_NAME fi When then running ./build.sh on first startup it will create a container jekyll_builder. Whenever you run this again after adding a blog post (or removing one), this will build the latest version of your site. The output is in _site/ directory (relative to your repositories root), so the same as without using Docker at all.\nOne downside to this approach is that as soon as you move the repository on your machine, this would not work as expected anymore. To make it work again you could simply remove the container docker container rm jekyll_builder.\n","date":"2021-04-24T20:00:00Z","permalink":"https://revontulet.dev/p/2021-create-blog-using-jekyll-and-docker/","title":"Create a blog using Jekyll and Docker"},{"content":"In my previous post, I showed how to enable the Raspberry Pi 4 on Ubuntu Server to read out DHT11/DHT22 sensor data. But this is a very hacky solution. Moreover is the Raspberry Pi cluttered with some stuff, that we may need to configure differently for another setup or remove as a whole. It\u0026rsquo;s a nice coincidence that I\u0026rsquo;m currently reading Docker in Action, Second Edition (Manning, 2019), so I wanted to build a docker image to read out the data. There are some images available for this already, but I still wanted to build my own light-weight image.\nEnough foreword, let\u0026rsquo;s jump right in.\nPrerequesites Raspberry Pi 4 with Docker installed (curl https://get.docker.com/ | sh) DHT11/22 connected with your RapsberryPi (find any guide online for wiring) Build Base Image Interactively in a Container This section is to interactively build the base docker image. If you want to get a Dockerfile right away, see the next section.\nStart Container As base we use python:3.7-alpine, as it is a comparatively very small image with only 41 MB size. We start the alpine shell /bin/ash and to attach us to the container we use the -it parameter. The --name dht parameter is to find the container by a predefined name.\n1 docker container run -it --name dht python:3.7-alpine /bin/ash The shell should now look like below. We\u0026rsquo;re now running as a root user in the container.\n1 / # Install Build Dependencies Build Tools To build all dependencies we need to be able to build the python lib RPi.GPIO.\nRun\n1 2 apk update \u0026amp;\u0026amp; apk upgrade apk add g++ python3-dev to install the build tools.\nRPi.GPIO It cost me some headache and searching around how I get this to build, as I always got errors that gcc exited with status code 1. Luckily I stumbled upon a post in https://archlinuxarm.org/forum/viewtopic.php?p=64598#p64598 by user peiyangxie who gave a one liner how to do this. So let\u0026rsquo;s do it, run\n1 CFLAGS=\u0026#34;-fcommon\u0026#34; pip3 install RPi.GPIO in the container. This will install RPi.GPIO.\nadafruit-circuitpython-dht Last but not least we need to install the library adafruit-circuitpython-dht. Do this by running\n1 pip3 install adafruit-circuitpython-dht Clean up As we do not need g++ and python3-dev anymore, we should remove them to shrink container size.\n1 apk del g++ python3-dev Then exit the container and return to the host shell.\nBuild Image from Container It remains to build the image from the container. This is easily done by\n1 docker container commit dht dht-image and then remove the container\n1 docker container rm dht Via Dockerfile To build dht-image automatically, just create a file called Dockerfile and put below input in it.\n1 2 3 4 5 6 7 8 9 FROM python:3.7-alpine RUN apk update \u0026amp;\u0026amp; apk upgrade RUN apk add g++ python3-dev ## See https://archlinuxarm.org/forum/viewtopic.php?p=64598#p64598 RUN CFLAGS=\u0026#34;-fcommon\u0026#34; pip3 install RPi.GPIO RUN pip3 install adafruit-circuitpython-dht RUN apk del g++ python3-dev ENTRYPOINT [\u0026#34;/bin/ash\u0026#34;] Navigate to the place the Dockerfile is located and run\n1 docker image build -t dht-image . to build the image.\nRead DHT with dht-image If you completed the aforementioned section you should have an image called dht-image. Let\u0026rsquo;s verify this.\n1 docker images It should print out a list of images, which contains dht-image (most likely at the top).\nIn the previous post I shared a script dht.py which we will first download into the container and then change the pin/sensor according to our needs. Finally we run the script and should get results from our sensor.\nNote\nDocker is not able to access the GPIO pins unless we give it acces. There is a really good, short and comprehensive way about the best options we have on stackoverflow.com/a/48234752. We will use the second option and add /dev/gpiomem as a device to our container, as it is the easiest and most sensible option for our needs.\nStart Container We start the container with /dev/gpiomem as a device and run /bin/ash in it.\n1 docker container run --device /dev/gpiomem -it dht-image /bin/ash Download dht script Download the dht.py script via\n1 wget https://revontulet.dev/p/2021-raspberry-pi-four-dht/dht.py Adjust dht script to wiring The line we need to change is line 8. If you are familiar with Vi you could easily do that, but in case you are not this is also achievable with a single sed command (Source: stackoverflow.com). Run the following command with\n{X} - 11 or 22 (DHT version) {Y} - Your GPIO Pin 1 sed -i \u0026#39;8s/.*/dhtDevice = adafruit_dht.DHT{X}(board.D{Y})/\u0026#39; dht.py so for instance, I have a DHT11 and it is connected to Pin 4, so I would run\n1 sed -i \u0026#39;8s/.*/dhtDevice = adafruit_dht.DHT11(board.D4)/\u0026#39; dht.py Run dht script This is the easiest step. Just execute it via\n1 python3 dht.py and you should start seeing output.\nI do not know why, but this is failing a lot more often than running directly on the Pi. So you most likely see something like this\n1 2 3 4 5 6 7 8 9 / # python3 dht.py Checksum did not validate. Try again. Checksum did not validate. Try again. Temp: 69.8 F / 21.0 C Humidity: 20% Checksum did not validate. Try again. A full buffer was not returned. Try again. Temp: 69.8 F / 21.0 C Humidity: 20% Checksum did not validate. Try again. Checksum did not validate. Try again. Anyway, here we are, having a working docker image in place that returns (most of the times) the results from the sensor.\n","date":"2021-03-31T22:00:00Z","permalink":"https://revontulet.dev/p/2021-raspberry-pi-four-dht-docker/","title":"Read Temperature \u0026 Humidity with RaspberryPi 4 - Docker Setup"},{"content":"I recently bought a RaspberryPi 4 and installed Ubuntu Server 20.04 64 Bit on it. There are many tutorials available how to read the temperature/humidity via Python, unfortunately none of them worked out of the box. It seems to be that they are\neither for Raspbian OS or for 32 Bit OSes. After some searching I found a way from several sources to make it run. This is a horrible hacky workaround and I\u0026rsquo;d be very happy to see a better way and/or out-of-the-box solution.\nPrerequisites Ubuntu Server 20.04.2 LTS Python 3 (preinstalled in aforementioned Ubuntu image) DHT11/DHT22 with 4 Pins connected to the Pi (follow this guide to see the wiring) dht.py (Source code from learn.adafruit.com) Installation 1. Install Python 3 Pip We will use adafruit-circuitpython-dht to read out the sensor and install it via pip. Run below command from the shell.\n1 sudo apt install python3-pip 2. Install RPi.GPIO via apt-get Usually this is a dependency to adafruit-circuitpython-dht and thus could be installed with it. Unfortunately it does not succeed installing the dependency, so we need to install it ourself. For more information see this thread on askubuntu.com. Run below command from the shell.\n1 sudo apt-get install RPi.GPIO 3. Install adafruit-circuitpython-dht 1 sudo pip3 install adafruit-circuitpython-dht 4. Install libgiod This step is a bit ugly. Basically the default installation did not succeed for me, and I ended up finding that this solution on GitHub worked yet slightly modified.\n4.1. Install and build libgpiod 1 2 3 4 sudo apt install libgpiod-dev git build-essential git clone https://github.com/adafruit/libgpiod_pulsein.git cd libgpiod_pulsein/src make You should see a new folder named libgpiod_pulsein. Run the script dht.py (linked in the prerequisites) and you should see some similar error:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 Traceback (most recent call last): File \u0026#34;run.py\u0026#34;, line 11, in \u0026lt;module\u0026gt; dhtDevice = adafruit_dht.DHT11(board.D16) File \u0026#34;/usr/local/lib/python3.8/dist-packages/adafruit_dht.py\u0026#34;, line 265, in __init__ super().__init__(True, pin, 18000, use_pulseio) File \u0026#34;/usr/local/lib/python3.8/dist-packages/adafruit_dht.py\u0026#34;, line 56, in __init__ self.pulse_in = PulseIn(self._pin, 81, True) File \u0026#34;/usr/local/lib/python3.8/dist-packages/adafruit_blinka/microcontroller/bcm283x/pulseio/PulseIn.py\u0026#34;, line 67, in __init__ self._process = subprocess.Popen(cmd) File \u0026#34;/usr/lib/python3.8/subprocess.py\u0026#34;, line 854, in __init__ self._execute_child(args, executable, preexec_fn, close_fds, File \u0026#34;/usr/lib/python3.8/subprocess.py\u0026#34;, line 1702, in _execute_child raise child_exception_type(errno_num, err_msg, err_filename) FileNotFoundError: [Errno 2] No such file or directory: \u0026#39;/usr/local/lib/python3.8/dist-packages/adafruit_blinka/microcontroller/bcm283x/pulseio/libgpiod_pulsein\u0026#39; Of particular interest for us is the last line. The file exists, but Python cannot see/access it. Unfortunately granting Python any rights does not solve this problem either. So we need to replace it by the previous build and then grant python correct rights.\nThe path might be different on your computer, so don\u0026rsquo;t blindly copy \u0026amp; paste the next steps but in case it differs use path from your output.\n4.2. Replace libgpiod with build Go into the directory libgpiod_pulsein/src/ from step 4.1. and run the following command from there (important: Replace the path if it differs).\n1 sudo cp libgpiod_pulsein /usr/local/lib/python3.8/dist-packages/adafruit_blinka/microcontroller/bcm283x/pulseio/libgpiod_pulsein 4.3. Set suid for Python If you run dht.py again, it will show you the same error as before. We need to allow any user to run python with the same rights as the owner, which is root. It is done via setuid. This solution was posted on armbian.com. Please note: It is very dangerous to grant these rights to Python. So you should rather copy your Python Binary and change only the copy. See the linked solution for doing so.\nAs I have Python 3.8 on my machine, my Python Binary is found in /usr/bin/python3.8. To enable setuid, run the following command:\n1 sudo chmod 4775 /usr/bin/python3.8 Read Sensor Data Everything set up, so we are ready to read the data. You need to alter the script dht.py in line 8, depending on your GPIO Pin and Sensor:\n1 2 3 4 ## VE - DHT version, e.g. 11 or 22 ## PIN - GPIO Pin the board is connected to, e.g. D4 = GPIO Pin 4 ## VE PIN dhtDevice = adafruit_dht.DHT22(board.D4) It should now start without any errors and print output similar (different values) to that.\n1 2 3 4 5 âžœ ~ python dht.py Temp: 66.2 F / 19.0 C Humidity: 32% Temp: 68.0 F / 20.0 C Humidity: 33% Temp: 68.0 F / 20.0 C Humidity: 33% Temp: 68.0 F / 20.0 C Humidity: 33% ","date":"2021-03-21T11:00:00Z","permalink":"https://revontulet.dev/p/2021-raspberry-pi-four-dht/","title":"Read Temperature \u0026 Humidity with RaspberryPi 4"}]